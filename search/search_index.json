{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenSARLab What is OpenSARLab? OpenSARLab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARLab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARLab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARLab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARLab deployment. Sign in or create an account to try OpenSARLab How will OpenSARLab benefit my work as a SAR scientist? OpenSARLab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis How will OpenSARLab benefit the class or training I am planning? OpenSARLab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers. How is OpenSARLab different from Binder? Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity) How to Access OpenSARLab As a Paid Service Managed by Alaska Satellite Facility Enterprise Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARLab deployment to suit your needs. Deploy OpenSARLab to Your Own AWS Account (Coming Soon) Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS. Contact Us Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Home"},{"location":"#welcome-to-opensarlab","text":"","title":"Welcome to OpenSARLab"},{"location":"#what-is-opensarlab","text":"OpenSARLab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARLab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARLab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARLab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARLab deployment. Sign in or create an account to try OpenSARLab","title":"What is OpenSARLab?"},{"location":"#how-will-opensarlab-benefit-my-work-as-a-sar-scientist","text":"OpenSARLab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis","title":"How will OpenSARLab benefit my work as a SAR scientist?"},{"location":"#how-will-opensarlab-benefit-the-class-or-training-i-am-planning","text":"OpenSARLab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers.","title":"How will OpenSARLab benefit the class or training I am planning?"},{"location":"#how-is-opensarlab-different-from-binder","text":"Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity)","title":"How is OpenSARLab different from Binder?"},{"location":"#how-to-access-opensarlab","text":"","title":"How to Access OpenSARLab"},{"location":"#as-a-paid-service-managed-by-alaska-satellite-facility-enterprise","text":"Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARLab deployment to suit your needs.","title":"As a Paid Service Managed by Alaska Satellite Facility Enterprise"},{"location":"#deploy-opensarlab-to-your-own-aws-account-coming-soon","text":"Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS.","title":"Deploy OpenSARLab to Your Own AWS Account (Coming Soon)"},{"location":"#contact-us","text":"Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Contact Us"},{"location":"dev/","text":"OpenScienceLab Build and Deploy the Portal Build and Deploy OpenSARLab Image Build and Deploy OpenSARLab Cluster System Diagram Conda Environment Options OpenSARLab Notifications Troubleshooting Custom Mintpy Conda Build Instructions","title":"Dev"},{"location":"release_notes/","text":"June 2021 October 2021 February 2022 February 2023 February 2024 December 2024","title":"Release notes"},{"location":"user/","text":"Welcome to the OpenSARLab User Guide Configuring Multi-Factor Authentication Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARLab Account Details Git in OpenSARLab OpenSARLab Terminal OpenSARLab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARLab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARLab User Guide"},{"location":"user/#welcome-to-the-opensarlab-user-guide","text":"Configuring Multi-Factor Authentication Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARLab Account Details Git in OpenSARLab OpenSARLab Terminal OpenSARLab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARLab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARLab User Guide"},{"location":"dev-guides/about_opensciencelab/","text":"About OpenScienceLab OpenScienceLab is about Open Science. Brought to you by... the Alaska Satellite Facility: making remote sensing accessible. And... the OpenScienceLab team. And... by developers like you. Thank you.","title":"OpenScienceLab"},{"location":"dev-guides/about_opensciencelab/#about-opensciencelab","text":"OpenScienceLab is about Open Science. Brought to you by... the Alaska Satellite Facility: making remote sensing accessible. And... the OpenScienceLab team. And... by developers like you. Thank you.","title":"About OpenScienceLab"},{"location":"dev-guides/destroy_deployment/","text":"Return to Developer Guide Destroy Deployments It is essential to destroy a deployment at the end of its life cycle so that no resources are left in place. With a proper destruction procedure, one can mitigate the accrued cost of AWS. WARNING: Before deleting the deployments, developers will need to account for the following: When deleting the CloudFormation stack, the deletion order matters. Delete some of the CloudFormation stacks before deleting ECR. The name of the items you're deleting may differ depending on the deployment you are taking down. For example, your deployment's CloudFormation stack may not have a region name. Do NOT take down the CloudWatch logs. These are used for statistical analysis later on. Removing CloudFormation Stacks When you first go to CloudFormation, it should look something like the following: Because the order of removing deployment is essential, you will need to take down the CloudFormation stacks first. Deleting the following CloudFormation stacks will kill the deployment and remove its resources: <deployment_name>-container <deployment_name>-cluster deployment_name NB : Do not manually delete any S3 buckets after emptying them in the steps below. If you delete the buckets after emptying them, the CloudFormation stack deletions associated with those buckets will fail. To fix the issues regarding deleted buckets, you will have to recreate the empty buckets to proceed. Therefore, let CloudFormation delete the S3 buckets for you once you empty them. Follow the below instructions to properly delete these stacks. Steps to Delete CloudFormation Stacks 1. Prepare to delete the <deployment_name>-container CloudFormation stack NB: The <deployment_name>-container is independent of other stacks, i.e., the deletion order does not matter. Empty the codepipeline-<region>-<deployment_name>-container S3 bucket Navigate to the AWS S3 console Click the codepipeline-<region>-<deployment_name>-container S3 bucket option. Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button 2. Delete ECR repos Navigate to the AWS Elastic Container Registry Before deleting the ECR, you will need to empty them first. Click the repository name first and go to the individual ECR repository. Select all items by clicking the first box. Select delete and confirm the deletion. Go back to the ECR Registry and click the option next to the empty <deployment_name>/<profile_namespace> repository Click the Delete button Confirm the deletion by typing delete in the provided field Click the Delete button Repeat for each profiles 3. Delete the <deployment_name>-container CloudFormation stack Navigate to the AWS CloudFormation console Click the <deployment_name>-container stack option Click the Delete button Click the Delete Stack button Click the <deployment_name>-container stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically NB: Stack deletion process will be similar to above instructions for other stacks. Deleting the rest of the CloudFormation Stacks Unlike the container stack, the deletion order matters for the rest. On the bright side, the deletion process should be similar to the container stacks. Specifically, you will need to delete your stacks in the following order: <deployment_name>-jupyterhub stack <deployment_name>-cluster stack * <deployment_name>-cluster-pipeline stack NB:*These stacks have additional steps. In above order, follow these steps (except for stack 3): Delete the <deployment_name>-<stack_name> CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the <deployment_name>-<stack_name> stack Click the Delete button 1. Click the Delete stack button Click the <deployment_name>-<stack_name> stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically If you are deleting stack 3 ( <deployment_name>-cluster-pipeline ), you will need to follow these steps first before deleting the stack: Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Delete <deployment_name>/hub and <deployment_name>/notifications repository. NB: Refer to the Delete ECR Repos section for how to delete ECR repos. Delete EBS Snapshots and Volumes To mitigate the cost associated with storage space, it is crucial to deallocate unused resources. The below steps will guide you on how to do so. First, navigate to the AWS EC2 console - this step should be identical for both EBS snapshots and EBS volumes. Delete EBS snapshots Click the Snapshots link in the sidebar menu Filter by <cost allocation tag> : <deployment_name> Double check that you filtered for the correct deployment! Select all snapshots Select Delete from the Actions menu Confirm by clicking the Yes, delete button Delete EBS volumes Navigate to the AWS EC2 console Click the Volumes link in the sidebar menu Filter by osl-stackname: <deployment_name> Double check that you filtered for the correct deployment! Select all volumes Select Delete volumes from the Actions menu Confirm by clicking the Yes, delete button (Optional) Confirm that all resources have been deleted Once you've taken down the deployment, you may want to verify the resource usage. Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the Tag Editor link in the sidebar menu and fill in the following: Regions: <current-region> Tags: Key : Cost allocation tag Value : <deployment_name> Click the Search resources button Identify and delete any remaining resources (Optional) Delete Calendar Now that you are done with taking down the deployment, you will need to delete the calendar notifications. Go to your Google Calendar Choose a deployment you wish to remove Open Settings and Sharing Click Delete under Remove calendar","title":"Destroy deployment"},{"location":"dev-guides/destroy_deployment/#destroy-deployments","text":"It is essential to destroy a deployment at the end of its life cycle so that no resources are left in place. With a proper destruction procedure, one can mitigate the accrued cost of AWS.","title":"Destroy Deployments"},{"location":"dev-guides/destroy_deployment/#warning-before-deleting-the-deployments-developers-will-need-to-account-for-the-following","text":"When deleting the CloudFormation stack, the deletion order matters. Delete some of the CloudFormation stacks before deleting ECR. The name of the items you're deleting may differ depending on the deployment you are taking down. For example, your deployment's CloudFormation stack may not have a region name. Do NOT take down the CloudWatch logs. These are used for statistical analysis later on.","title":"WARNING: Before deleting the deployments, developers will need to account for the following:"},{"location":"dev-guides/destroy_deployment/#removing-cloudformation-stacks","text":"When you first go to CloudFormation, it should look something like the following: Because the order of removing deployment is essential, you will need to take down the CloudFormation stacks first. Deleting the following CloudFormation stacks will kill the deployment and remove its resources: <deployment_name>-container <deployment_name>-cluster deployment_name NB : Do not manually delete any S3 buckets after emptying them in the steps below. If you delete the buckets after emptying them, the CloudFormation stack deletions associated with those buckets will fail. To fix the issues regarding deleted buckets, you will have to recreate the empty buckets to proceed. Therefore, let CloudFormation delete the S3 buckets for you once you empty them. Follow the below instructions to properly delete these stacks.","title":"Removing CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#steps-to-delete-cloudformation-stacks","text":"","title":"Steps to Delete CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#1-prepare-to-delete-the-deployment_name-container-cloudformation-stack","text":"NB: The <deployment_name>-container is independent of other stacks, i.e., the deletion order does not matter. Empty the codepipeline-<region>-<deployment_name>-container S3 bucket Navigate to the AWS S3 console Click the codepipeline-<region>-<deployment_name>-container S3 bucket option. Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button","title":"1. Prepare to delete the &lt;deployment_name&gt;-container CloudFormation stack"},{"location":"dev-guides/destroy_deployment/#2-delete-ecr-repos","text":"Navigate to the AWS Elastic Container Registry Before deleting the ECR, you will need to empty them first. Click the repository name first and go to the individual ECR repository. Select all items by clicking the first box. Select delete and confirm the deletion. Go back to the ECR Registry and click the option next to the empty <deployment_name>/<profile_namespace> repository Click the Delete button Confirm the deletion by typing delete in the provided field Click the Delete button Repeat for each profiles","title":"2. Delete ECR repos"},{"location":"dev-guides/destroy_deployment/#3-delete-the-deployment_name-container-cloudformation-stack","text":"Navigate to the AWS CloudFormation console Click the <deployment_name>-container stack option Click the Delete button Click the Delete Stack button Click the <deployment_name>-container stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically NB: Stack deletion process will be similar to above instructions for other stacks.","title":"3. Delete the &lt;deployment_name&gt;-container CloudFormation stack"},{"location":"dev-guides/destroy_deployment/#deleting-the-rest-of-the-cloudformation-stacks","text":"Unlike the container stack, the deletion order matters for the rest. On the bright side, the deletion process should be similar to the container stacks. Specifically, you will need to delete your stacks in the following order: <deployment_name>-jupyterhub stack <deployment_name>-cluster stack * <deployment_name>-cluster-pipeline stack NB:*These stacks have additional steps. In above order, follow these steps (except for stack 3): Delete the <deployment_name>-<stack_name> CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the <deployment_name>-<stack_name> stack Click the Delete button 1. Click the Delete stack button Click the <deployment_name>-<stack_name> stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically If you are deleting stack 3 ( <deployment_name>-cluster-pipeline ), you will need to follow these steps first before deleting the stack: Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Delete <deployment_name>/hub and <deployment_name>/notifications repository. NB: Refer to the Delete ECR Repos section for how to delete ECR repos.","title":"Deleting the rest of the CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#delete-ebs-snapshots-and-volumes","text":"To mitigate the cost associated with storage space, it is crucial to deallocate unused resources. The below steps will guide you on how to do so. First, navigate to the AWS EC2 console - this step should be identical for both EBS snapshots and EBS volumes.","title":"Delete EBS Snapshots and Volumes"},{"location":"dev-guides/destroy_deployment/#delete-ebs-snapshots","text":"Click the Snapshots link in the sidebar menu Filter by <cost allocation tag> : <deployment_name> Double check that you filtered for the correct deployment! Select all snapshots Select Delete from the Actions menu Confirm by clicking the Yes, delete button","title":"Delete EBS snapshots"},{"location":"dev-guides/destroy_deployment/#delete-ebs-volumes","text":"Navigate to the AWS EC2 console Click the Volumes link in the sidebar menu Filter by osl-stackname: <deployment_name> Double check that you filtered for the correct deployment! Select all volumes Select Delete volumes from the Actions menu Confirm by clicking the Yes, delete button","title":"Delete EBS volumes"},{"location":"dev-guides/destroy_deployment/#optional-confirm-that-all-resources-have-been-deleted","text":"Once you've taken down the deployment, you may want to verify the resource usage. Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the Tag Editor link in the sidebar menu and fill in the following: Regions: <current-region> Tags: Key : Cost allocation tag Value : <deployment_name> Click the Search resources button Identify and delete any remaining resources","title":"(Optional) Confirm that all resources have been deleted"},{"location":"dev-guides/destroy_deployment/#optional-delete-calendar","text":"Now that you are done with taking down the deployment, you will need to delete the calendar notifications. Go to your Google Calendar Choose a deployment you wish to remove Open Settings and Sharing Click Delete under Remove calendar","title":"(Optional) Delete Calendar"},{"location":"dev-guides/system_diagram/","text":"Return to Table of Contents OpenSARLab System Diagram","title":"System diagram"},{"location":"dev-guides/system_diagram/#opensarlab-system-diagram","text":"","title":"OpenSARLab System Diagram"},{"location":"dev-guides/troubleshooting/","text":"Return to Developer Guide A. Users 1. OSL Servers Time Out Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"Troubleshooting"},{"location":"dev-guides/troubleshooting/#a-users","text":"","title":"A. Users"},{"location":"dev-guides/troubleshooting/#1-osl-servers-time-out","text":"Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"1. OSL Servers Time Out"},{"location":"dev-guides/cluster/build_and_deploy_opensarlab_cluster/","text":"Build and Deploy OpenSARLab Cluster Build the docker images first based off opensarlab-container . Deploy the following in the same AWS account and region as the previous container images. Create new GitHub repo To organize repos, use the naming convention: deployment-{location/owner}-{maturity?}-cluster Copy canonical opensarlab-cluster and commit. Either copy/paste or use git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Make sure any hidden files (like .gitignore, .yamllint, etc.) are properly copied. Within AWS add GitHub Connections. If done before, the app should show your GitHub app name. https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html Make sure you are in the right region of your AWS account. Once Connections is setup, save the Connection arn for later. Remember to add the current GitHub repo to the Connection app GitHub > Settings > GitHub Apps > AWS Connector for GitHub > Repository Access Add GitHub repo Add a SSL certificate to AWS Certification Manager. You will need the ARN of the certificate. Update opensciencelab.yaml within the code. See explaination of the various parts here . Deploy the CloudFormation template found at pipeline/cf-setup-pipeline.yaml . Use the following parameters: Parameter Description Stack name The CloudFormation stack name. For readablity, append -pipeline to the end. CodeStarConnectionArn The ARN of the Connection made eariler. CostTagKey Useful if using billing allocation tags. CostTagValue USeful if using billing allocation tags. Note that many resources will have this in their name for uniqueness. It needs to be short in length. GitHubBranchName The branch name of the GitHub repo where the code resides. GitHubFullRepo The GitHub repo name. Needs to be in the format {GitHub organization}/{GitHub repo} from https://github.com/OrgName/RepoName . The pipeline will take a few seconds to form. If the cloudformation stack fails to fully form it will need to be fully deleted and the template will need to be re-uploaded. The pipeline will start to build automatically in CodePipeline. A successful run will take about 12 minutes. If it takes signitifcantly less time then the build might have failed even if CodePipeline says successful. Sometimes the final buld stage will error with something like \"build role not found\". In this case, just retry the stage. There is sometimes a race condtion for AWS role creations. During the course of the build, other CloudFormation stacks will be created. One of these is for the cluster. Within Outputs will be the Load Balancer url which can be used within external DNS. Add the Portal SSO Token to Secrets Manager. Update sso-token/{region}-{cluster name} . Add deployment to Portal Update labs.{maturity}.yaml and re-build Portal. Within the Portal Access page, create lab sheet with the lab_short_name found in opensciencelab.yaml . Within the Portal Access page, add usernames and profiles as needed. Add CloudShell access From the AWS console, start CloudShell (preferably in it's own browser tab) CloudShell copy and paste are not shifted like a normal terminal. They are your normal keyboard operations. If needed, update default editor: Append to ~/.bashrc the command export EDITOR=vim Setup access to the K8s cluster From the AWS EKS page, get the cluster name for below. From the AWS IAM page, get the ARN of the role {region namwe}-{cluster name}-user-full-access On the CloudShell terminal, run aws eks update-kubeconfig --name {EKS clsuter name} --role-arn {role ARN} Run kubectl get pods -A . You should see any user and hub pods. Bump the AutoScaling Groups For reasons unknown, fresh brand-new ASGs need to be \"primed\" by setting the desired number to one. JupyterHub's autoscaler will scale the groups back down to zero if there is no use. This normally has to only be done once. Start a JupyterLab server to make sure one works Within CloudShell, check the PVC and PV of the user volume. Make sure the K8s annotation pv.kubernetes.io/provisioned-by: ebs.csi.aws.com is present. If not, then the JupyterHub volume managment will fail and volumes will become orpaned upon lifecycle deletion. Destroy OpenSARLab Cluster To take down, consult destroy deployment docs","title":"Build and Deploy OpenSARLab Cluster"},{"location":"dev-guides/cluster/build_and_deploy_opensarlab_cluster/#build-and-deploy-opensarlab-cluster","text":"Build the docker images first based off opensarlab-container . Deploy the following in the same AWS account and region as the previous container images. Create new GitHub repo To organize repos, use the naming convention: deployment-{location/owner}-{maturity?}-cluster Copy canonical opensarlab-cluster and commit. Either copy/paste or use git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Make sure any hidden files (like .gitignore, .yamllint, etc.) are properly copied. Within AWS add GitHub Connections. If done before, the app should show your GitHub app name. https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html Make sure you are in the right region of your AWS account. Once Connections is setup, save the Connection arn for later. Remember to add the current GitHub repo to the Connection app GitHub > Settings > GitHub Apps > AWS Connector for GitHub > Repository Access Add GitHub repo Add a SSL certificate to AWS Certification Manager. You will need the ARN of the certificate. Update opensciencelab.yaml within the code. See explaination of the various parts here . Deploy the CloudFormation template found at pipeline/cf-setup-pipeline.yaml . Use the following parameters: Parameter Description Stack name The CloudFormation stack name. For readablity, append -pipeline to the end. CodeStarConnectionArn The ARN of the Connection made eariler. CostTagKey Useful if using billing allocation tags. CostTagValue USeful if using billing allocation tags. Note that many resources will have this in their name for uniqueness. It needs to be short in length. GitHubBranchName The branch name of the GitHub repo where the code resides. GitHubFullRepo The GitHub repo name. Needs to be in the format {GitHub organization}/{GitHub repo} from https://github.com/OrgName/RepoName . The pipeline will take a few seconds to form. If the cloudformation stack fails to fully form it will need to be fully deleted and the template will need to be re-uploaded. The pipeline will start to build automatically in CodePipeline. A successful run will take about 12 minutes. If it takes signitifcantly less time then the build might have failed even if CodePipeline says successful. Sometimes the final buld stage will error with something like \"build role not found\". In this case, just retry the stage. There is sometimes a race condtion for AWS role creations. During the course of the build, other CloudFormation stacks will be created. One of these is for the cluster. Within Outputs will be the Load Balancer url which can be used within external DNS. Add the Portal SSO Token to Secrets Manager. Update sso-token/{region}-{cluster name} . Add deployment to Portal Update labs.{maturity}.yaml and re-build Portal. Within the Portal Access page, create lab sheet with the lab_short_name found in opensciencelab.yaml . Within the Portal Access page, add usernames and profiles as needed. Add CloudShell access From the AWS console, start CloudShell (preferably in it's own browser tab) CloudShell copy and paste are not shifted like a normal terminal. They are your normal keyboard operations. If needed, update default editor: Append to ~/.bashrc the command export EDITOR=vim Setup access to the K8s cluster From the AWS EKS page, get the cluster name for below. From the AWS IAM page, get the ARN of the role {region namwe}-{cluster name}-user-full-access On the CloudShell terminal, run aws eks update-kubeconfig --name {EKS clsuter name} --role-arn {role ARN} Run kubectl get pods -A . You should see any user and hub pods. Bump the AutoScaling Groups For reasons unknown, fresh brand-new ASGs need to be \"primed\" by setting the desired number to one. JupyterHub's autoscaler will scale the groups back down to zero if there is no use. This normally has to only be done once. Start a JupyterLab server to make sure one works Within CloudShell, check the PVC and PV of the user volume. Make sure the K8s annotation pv.kubernetes.io/provisioned-by: ebs.csi.aws.com is present. If not, then the JupyterHub volume managment will fail and volumes will become orpaned upon lifecycle deletion.","title":"Build and Deploy OpenSARLab Cluster"},{"location":"dev-guides/cluster/build_and_deploy_opensarlab_cluster/#destroy-opensarlab-cluster","text":"To take down, consult destroy deployment docs","title":"Destroy OpenSARLab Cluster"},{"location":"dev-guides/cluster/egress_config/","text":"Egress Configuration If enabled, the Istio service mesh can apply rules for rate limiting and domain blocking. To facilitate usability, a custom configuration is employed with custom rules. These rules for a particular configuration will only apply to the user or dask pod assigned to the corresponsing egress profile. The configurations need to be found in the {root}/egress_configs directory/useretc. Schema In general, any parameter starting with @ is global, % is sequential, and + is one-time. Wildcards * not allowed. Comment lines start with # and are ignored. Other line entries: Parameter Value Type description @profile str Required. Egress profile name that will be assigned to the lab profile. There can only be one @profile per egress config file. Other @profile references will be ignored. Because the profile name is part of the naming structure of some k8s resources, it must be fqdn compatible. @rate int Required. Rate limit (per 10 seconds) applied to the assigned pod. Value is the max value of requests per second. Any subsequent @rate is ignored. To turn off rate limit, set value to None . @list white or black Required. Either the config is a whitelist or a blacklist. Any subsequent @list is ignored. @include str Optional. Any named .conf file within a sibling includes folder will be copied/inserted at the point of the @include . Having @rate , @include , or @profile within the \"included\" configs will throw an error. Other rules for ordering still apply. %port int,int Required. Port value for the host. Must have a value between 1 and 65535. Ports can be consolidated by comma seperation. Ports seperated by => will be treated like a redirect ( this is currently not working. The ports will be treated as seperated by a comma ). %timeout str Optional. Timeout for a valid timeout for any subsequent host. The vlaue must end in s for seconds, m for minutes, etc. +ip num Optional. Any valid fqdn ip address. ^ str Optional. Globally negate the hostname value. Useful for disabling included hosts. Lines not prepended with @ , % , + , ^ , or # will be treated as a hostname. Examples Blacklist with rate limiting # Included blacklist %timeout 10s %port 80=>443 example.com # This conf is required!! # This will be used by profiles that don't have any explicit whitelist and are not None @profile default @rate 30 @list black @include blacklist # Note that the explicit redirect is not working properly and should not be used # Both port 80 and port 443 will be allowed, though %port 80=>443 %timeout 1s blackhole.webpagetest.org Whitelist with rate limiting @profile m6a-large-whitelist @rate 30 @list white @include asf @include aws @include earthdata @include github @include local @include mappings @include mintpy @include others @include packaging @include ubuntu","title":"Egress Configuration"},{"location":"dev-guides/cluster/egress_config/#egress-configuration","text":"If enabled, the Istio service mesh can apply rules for rate limiting and domain blocking. To facilitate usability, a custom configuration is employed with custom rules. These rules for a particular configuration will only apply to the user or dask pod assigned to the corresponsing egress profile. The configurations need to be found in the {root}/egress_configs directory/useretc.","title":"Egress Configuration"},{"location":"dev-guides/cluster/egress_config/#schema","text":"In general, any parameter starting with @ is global, % is sequential, and + is one-time. Wildcards * not allowed. Comment lines start with # and are ignored. Other line entries: Parameter Value Type description @profile str Required. Egress profile name that will be assigned to the lab profile. There can only be one @profile per egress config file. Other @profile references will be ignored. Because the profile name is part of the naming structure of some k8s resources, it must be fqdn compatible. @rate int Required. Rate limit (per 10 seconds) applied to the assigned pod. Value is the max value of requests per second. Any subsequent @rate is ignored. To turn off rate limit, set value to None . @list white or black Required. Either the config is a whitelist or a blacklist. Any subsequent @list is ignored. @include str Optional. Any named .conf file within a sibling includes folder will be copied/inserted at the point of the @include . Having @rate , @include , or @profile within the \"included\" configs will throw an error. Other rules for ordering still apply. %port int,int Required. Port value for the host. Must have a value between 1 and 65535. Ports can be consolidated by comma seperation. Ports seperated by => will be treated like a redirect ( this is currently not working. The ports will be treated as seperated by a comma ). %timeout str Optional. Timeout for a valid timeout for any subsequent host. The vlaue must end in s for seconds, m for minutes, etc. +ip num Optional. Any valid fqdn ip address. ^ str Optional. Globally negate the hostname value. Useful for disabling included hosts. Lines not prepended with @ , % , + , ^ , or # will be treated as a hostname.","title":"Schema"},{"location":"dev-guides/cluster/egress_config/#examples","text":"Blacklist with rate limiting # Included blacklist %timeout 10s %port 80=>443 example.com # This conf is required!! # This will be used by profiles that don't have any explicit whitelist and are not None @profile default @rate 30 @list black @include blacklist # Note that the explicit redirect is not working properly and should not be used # Both port 80 and port 443 will be allowed, though %port 80=>443 %timeout 1s blackhole.webpagetest.org Whitelist with rate limiting @profile m6a-large-whitelist @rate 30 @list white @include asf @include aws @include earthdata @include github @include local @include mappings @include mintpy @include others @include packaging @include ubuntu","title":"Examples"},{"location":"dev-guides/cluster/opensciencelab_yaml/","text":"Contents of opensciencelab.yaml Schema for the egress config can be found ../egress_config.md . --- parameters: lab_short_name: The url-friendly short name of the lab deployment. cost_tag_key: Name of the cost allocation tag. cost_tag_value: Value of the cost allocation tag. Also used by cloudformation during setup for naming. admin_user_name: Username of initial JupyterHub admin certificate_arn: AWS arn of the SSL certificate held in Certificate Manager container_namespace: A namespaced path within AWS ECR containing custom images lab_domain: Domain of JupyterHub deployment. Use `load balancer` if not known. portal_domain: Domain of the OSL Portal. Used to communicate with email services, etc. # Volume and snapshot lifecycle managament days_till_volume_deletion: The number of integer days after last server use when the user's volume is deleted. To never delete volume, use value 365000. days_after_server_stop_till_warning_email: Comma seperated list of integer days after last server use when user gets warning email. Must have minimum one value. To never send emails, use value 365000 days_till_snapshot_deletion: The number of integer days after last server use when the user's snapshot is deleted. To never delete snapshot, use value 365000. days_after_server_stop_till_deletion_email: Number of integer days after last server use when user gets email notifiying about permanent deletion of data. Must have minimum one value. To never send emails, use value 365000 utc_hour_of_day_snapshot_cron_runs : Integer hour (UTC) when the daily snapshot cron runs. utc_hour_of_day_volume_cron_runs: Integer hour (UTC) when the daily snapshot cron runs. # Versions of sofware installed eks_version: '1.31' # https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html kubectl_version: '1.31.0/2024-09-12' # https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html aws_ebs_csi_driver_version: '2.36.0' # https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases jupyterhub_helm_version: '3.3.7' # https://jupyterhub.github.io/helm-chart/ jupyterhub_hub_image_version: '4.1.5' # Match App Version of JupyterHub Helm aws_k8s_cni_version: 'v1.18.5' # https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html cluster_autoscaler_helm_version: '9.43.1' # https://github.com/kubernetes/autoscaler/releases > cluster-autoscaler-chart istio_version: '1.23.2' # https://github.com/istio/istio/releases; set to None if disabling Istio dask_helm_version: '2024.1.0' # https://helm.dask.org/ > dask-gateway-{version}; Set to None if disabling Dask nodes: - name: hub # Required instance: The EC2 instance for the hub node. Type t3a.medium is preferred. min_number: 1 # Required max_number: 1 # Required node_policy: hub # Required is_hub: True # Required - name: daskcontroller # Required instance: t3a.medium, t3.medium min_number: 1 # Required max_number: 1 # Required node_policy: dask_controller # Required is_dask_controller: True # Required is_spot: True - name: Name of node type. Must be alphanumeric (no special characters, whitespace, etc.) instance: The EC2 instance for the hub node. Fallback types seperated by commas. (m6a.xlarge, m5a.xlarge) min_number: Minimum number of running node of this type in the cluster (0) max_number: Maximum number of running node of this type in the cluster (25) node_policy: Node permission policy (user) root_volume_size: Size of the root volume of the EC2 (GiB) (Optional, range 1 - 16,384) is_dask_worker: The EC2 is a dask worker (Optional, True). is_spot: The EC2 is part of a spot fleet (Optional, True). # Service accounts allow a built-in way to interact with AWS resources from within a server. # However, the default AWS profile is overwritten and may have inintended consequences. service_accounts: - name: service_account_name namespace: namespace of k8s resource (jupyter) permissions: - Effect: \"Allow\" Action: - \"AWS Resource Action\" Resource: \"AWS Resource ARN\" dask_profiles: - name: Name of dask profile that the user can select (Example 1) short_name: example_1 description: \"Basic worker used by example notebook\" image_url: FQDN with docker tags (233535791844.dkr.ecr.us-west-2.amazonaws.com/smce-test-opensarlab/daskworker:180a826). If not public, the domain must be in the same AWS account as the cluster. node_name: Node must be defined as a dask worker. egress_profile: Name of the egress config to use. Do not include `.conf` suffix (Optional) lab_profiles: - name: Name of profile that users can select (SAR 1) description: Description of profile image_url: FQDN of JupyterLab single user image with docker tags ( 233535791844.dkr.ecr.us-west-2.amazonaws.com/smce-test-opensarlab/sar:ea3e147). If not public, the domain must be in the same AWS account as the cluster. hook_script: Name of the script ran on user server startup (sar.sh) (optional) memory_guarantee: RAM usage guaranteed per user (6G) (Optional. Defaults to 0% RAM.) memory_limit: RAM usage guaranteed per user (16G) (Optional. Defaults to 100% RAM of server.) cpu_guarantee: CPU usage guaranteed per user (15) (Optional. Defaults to 0% CPU. Memory limits are preferable.) cpu_limit: CPU usage limit per user (30) (Optional. Defaults to 100% CPU of server. Memory limits are preferable.) storage_capacity: Size of each user's home directory (500Gi). Cannot be reduced after allocation. node_name: Node name as given in above section (sar1) delete_user_volumes: If True, deletes user volumes upon server stopping (Optional. Defaults to False.) desktop: If True, use Virtual Desktop by default (Optional. Defaults to False) The desktop enviromnent must be installed on image. default: If True, the specific profile is selected by default (Optional. False if not explicity set.) service_account: Name of previously defined service account to apply to profile (Optional) egress_profile: Name of the egress config to use. Do not include `.conf` suffix (Optional)","title":"Contents of opensciencelab.yaml"},{"location":"dev-guides/cluster/opensciencelab_yaml/#contents-of-opensciencelabyaml","text":"Schema for the egress config can be found ../egress_config.md . --- parameters: lab_short_name: The url-friendly short name of the lab deployment. cost_tag_key: Name of the cost allocation tag. cost_tag_value: Value of the cost allocation tag. Also used by cloudformation during setup for naming. admin_user_name: Username of initial JupyterHub admin certificate_arn: AWS arn of the SSL certificate held in Certificate Manager container_namespace: A namespaced path within AWS ECR containing custom images lab_domain: Domain of JupyterHub deployment. Use `load balancer` if not known. portal_domain: Domain of the OSL Portal. Used to communicate with email services, etc. # Volume and snapshot lifecycle managament days_till_volume_deletion: The number of integer days after last server use when the user's volume is deleted. To never delete volume, use value 365000. days_after_server_stop_till_warning_email: Comma seperated list of integer days after last server use when user gets warning email. Must have minimum one value. To never send emails, use value 365000 days_till_snapshot_deletion: The number of integer days after last server use when the user's snapshot is deleted. To never delete snapshot, use value 365000. days_after_server_stop_till_deletion_email: Number of integer days after last server use when user gets email notifiying about permanent deletion of data. Must have minimum one value. To never send emails, use value 365000 utc_hour_of_day_snapshot_cron_runs : Integer hour (UTC) when the daily snapshot cron runs. utc_hour_of_day_volume_cron_runs: Integer hour (UTC) when the daily snapshot cron runs. # Versions of sofware installed eks_version: '1.31' # https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html kubectl_version: '1.31.0/2024-09-12' # https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html aws_ebs_csi_driver_version: '2.36.0' # https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases jupyterhub_helm_version: '3.3.7' # https://jupyterhub.github.io/helm-chart/ jupyterhub_hub_image_version: '4.1.5' # Match App Version of JupyterHub Helm aws_k8s_cni_version: 'v1.18.5' # https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html cluster_autoscaler_helm_version: '9.43.1' # https://github.com/kubernetes/autoscaler/releases > cluster-autoscaler-chart istio_version: '1.23.2' # https://github.com/istio/istio/releases; set to None if disabling Istio dask_helm_version: '2024.1.0' # https://helm.dask.org/ > dask-gateway-{version}; Set to None if disabling Dask nodes: - name: hub # Required instance: The EC2 instance for the hub node. Type t3a.medium is preferred. min_number: 1 # Required max_number: 1 # Required node_policy: hub # Required is_hub: True # Required - name: daskcontroller # Required instance: t3a.medium, t3.medium min_number: 1 # Required max_number: 1 # Required node_policy: dask_controller # Required is_dask_controller: True # Required is_spot: True - name: Name of node type. Must be alphanumeric (no special characters, whitespace, etc.) instance: The EC2 instance for the hub node. Fallback types seperated by commas. (m6a.xlarge, m5a.xlarge) min_number: Minimum number of running node of this type in the cluster (0) max_number: Maximum number of running node of this type in the cluster (25) node_policy: Node permission policy (user) root_volume_size: Size of the root volume of the EC2 (GiB) (Optional, range 1 - 16,384) is_dask_worker: The EC2 is a dask worker (Optional, True). is_spot: The EC2 is part of a spot fleet (Optional, True). # Service accounts allow a built-in way to interact with AWS resources from within a server. # However, the default AWS profile is overwritten and may have inintended consequences. service_accounts: - name: service_account_name namespace: namespace of k8s resource (jupyter) permissions: - Effect: \"Allow\" Action: - \"AWS Resource Action\" Resource: \"AWS Resource ARN\" dask_profiles: - name: Name of dask profile that the user can select (Example 1) short_name: example_1 description: \"Basic worker used by example notebook\" image_url: FQDN with docker tags (233535791844.dkr.ecr.us-west-2.amazonaws.com/smce-test-opensarlab/daskworker:180a826). If not public, the domain must be in the same AWS account as the cluster. node_name: Node must be defined as a dask worker. egress_profile: Name of the egress config to use. Do not include `.conf` suffix (Optional) lab_profiles: - name: Name of profile that users can select (SAR 1) description: Description of profile image_url: FQDN of JupyterLab single user image with docker tags ( 233535791844.dkr.ecr.us-west-2.amazonaws.com/smce-test-opensarlab/sar:ea3e147). If not public, the domain must be in the same AWS account as the cluster. hook_script: Name of the script ran on user server startup (sar.sh) (optional) memory_guarantee: RAM usage guaranteed per user (6G) (Optional. Defaults to 0% RAM.) memory_limit: RAM usage guaranteed per user (16G) (Optional. Defaults to 100% RAM of server.) cpu_guarantee: CPU usage guaranteed per user (15) (Optional. Defaults to 0% CPU. Memory limits are preferable.) cpu_limit: CPU usage limit per user (30) (Optional. Defaults to 100% CPU of server. Memory limits are preferable.) storage_capacity: Size of each user's home directory (500Gi). Cannot be reduced after allocation. node_name: Node name as given in above section (sar1) delete_user_volumes: If True, deletes user volumes upon server stopping (Optional. Defaults to False.) desktop: If True, use Virtual Desktop by default (Optional. Defaults to False) The desktop enviromnent must be installed on image. default: If True, the specific profile is selected by default (Optional. False if not explicity set.) service_account: Name of previously defined service account to apply to profile (Optional) egress_profile: Name of the egress config to use. Do not include `.conf` suffix (Optional)","title":"Contents of opensciencelab.yaml"},{"location":"dev-guides/container/build_and_deploy_opensarlab_image/","text":"Build and Deploy OpenSARLab Image Container Setup Container Build in AWS Create AWS account if needed Gain GitHUb access if needed Create new GitHub repo To organize repos, use the naming convention: deployment-{location/owner}-{maturity?}-container Copy canonical opensarlab-container and commit Either copy/paste or use git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Within AWS add GitHub Connections. If done before, the app should show your GitHub app name. https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html Make sure you are in the right region of your AWS account. Once Connections is setup, save the Connection arn for later. Remember to add the current GitHub repo to the Connection app GitHub > Settings > GitHub Apps > AWS Connector for GitHub > Repository Access Add GitHub repo Within AWS CloudFormation, upload the template file cf-container.yaml and build. When prompted, use the Parameters: Parameter Description Stack name The CloudFormation stack name. For readablity, append -pipeline to the end. CodeStarConnectionArn The ARN of the Connection made eariler. ContainerNamespace The ECR prefix acting as a namespace for the images. This will be needed for the cluster's opensarlab.yaml . CostTagKey Useful if using billing allocation tags. CostTagValue USeful if using billing allocation tags. Note that many resources will have this in their name for uniqueness. It needs to be short in length. GitHubBranchName The branch name of the GitHub repo where the code resides. GitHubFullRepo The GitHub repo name. Needs to be in the format {GitHub organization}/{GitHub repo} from https://github.com/OrgName/RepoName . The pipeline will take a few seconds to form. If the cloudformation stack fails to fully form it will need to be fully deleted and the template will need to be re-uploaded. The pipeline will start to build automatically in CodePipeline. A successful run will take about 20 minutes. If it takes signitifcantly less time then the build might have failed even if CodePipeline says successful. Destroy OpenSARLab Image Container To take down, consult destroy deployment docs","title":"Build and Deploy OpenSARLab Image"},{"location":"dev-guides/container/build_and_deploy_opensarlab_image/#build-and-deploy-opensarlab-image-container","text":"","title":"Build and Deploy OpenSARLab Image Container"},{"location":"dev-guides/container/build_and_deploy_opensarlab_image/#setup-container-build-in-aws","text":"Create AWS account if needed Gain GitHUb access if needed Create new GitHub repo To organize repos, use the naming convention: deployment-{location/owner}-{maturity?}-container Copy canonical opensarlab-container and commit Either copy/paste or use git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Within AWS add GitHub Connections. If done before, the app should show your GitHub app name. https://docs.aws.amazon.com/dtconsole/latest/userguide/connections-create-github.html Make sure you are in the right region of your AWS account. Once Connections is setup, save the Connection arn for later. Remember to add the current GitHub repo to the Connection app GitHub > Settings > GitHub Apps > AWS Connector for GitHub > Repository Access Add GitHub repo Within AWS CloudFormation, upload the template file cf-container.yaml and build. When prompted, use the Parameters: Parameter Description Stack name The CloudFormation stack name. For readablity, append -pipeline to the end. CodeStarConnectionArn The ARN of the Connection made eariler. ContainerNamespace The ECR prefix acting as a namespace for the images. This will be needed for the cluster's opensarlab.yaml . CostTagKey Useful if using billing allocation tags. CostTagValue USeful if using billing allocation tags. Note that many resources will have this in their name for uniqueness. It needs to be short in length. GitHubBranchName The branch name of the GitHub repo where the code resides. GitHubFullRepo The GitHub repo name. Needs to be in the format {GitHub organization}/{GitHub repo} from https://github.com/OrgName/RepoName . The pipeline will take a few seconds to form. If the cloudformation stack fails to fully form it will need to be fully deleted and the template will need to be re-uploaded. The pipeline will start to build automatically in CodePipeline. A successful run will take about 20 minutes. If it takes signitifcantly less time then the build might have failed even if CodePipeline says successful.","title":"Setup Container Build in AWS"},{"location":"dev-guides/container/build_and_deploy_opensarlab_image/#destroy-opensarlab-image-container","text":"To take down, consult destroy deployment docs","title":"Destroy OpenSARLab Image Container"},{"location":"dev-guides/container/conda_environments/","text":"Return to Developer Guide There are a few options for creating conda environments in OpenSARLab. Each option come with benefits and drawbacks. Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks Drawbacks Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local , Register Their Kernels, and Run Any Setup Scripts Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts Drawbacks Increases the time it takes to start an OpenSARLab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage. Leave Conda Environment Creation up to the Users Benefits Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts. Drawbacks Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Conda Environment Options"},{"location":"dev-guides/container/conda_environments/#there-are-a-few-options-for-creating-conda-environments-in-opensarlab","text":"Each option come with benefits and drawbacks.","title":"There are a few options for creating conda environments in OpenSARLab."},{"location":"dev-guides/container/conda_environments/#create-conda-environments-register-their-kernels-and-run-any-setup-scripts-in-the-docker-images","text":"","title":"Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s"},{"location":"dev-guides/container/conda_environments/#benefits","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks","title":"Benefits"},{"location":"dev-guides/container/conda_environments/#drawbacks","text":"Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used.","title":"Drawbacks"},{"location":"dev-guides/container/conda_environments/#create-conda-environments-in-the-docker-images-then-in-the-hook-script-sync-them-to-homelocal-register-their-kernels-and-run-any-setup-scripts","text":"","title":"Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local, Register Their Kernels, and Run Any Setup Scripts"},{"location":"dev-guides/container/conda_environments/#benefits_1","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts","title":"Benefits"},{"location":"dev-guides/container/conda_environments/#drawbacks_1","text":"Increases the time it takes to start an OpenSARLab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage.","title":"Drawbacks"},{"location":"dev-guides/container/conda_environments/#leave-conda-environment-creation-up-to-the-users","text":"","title":"Leave Conda Environment Creation up to the Users"},{"location":"dev-guides/container/conda_environments/#benefits_2","text":"Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts.","title":"Benefits"},{"location":"dev-guides/container/conda_environments/#drawbacks_2","text":"Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Drawbacks"},{"location":"dev-guides/container/mintpy_conda/","text":"Build latest mintpy and push to custom OpenSARLab conda channel Sometimes the latest version of MintPy is desired within a notebook but the official release is not current. These instructions will show how to build MintPy and push to a custom conda channel. This assumes that the user has an Anaconda.org account and that this account is attached to a OpenSARLab conda channel. Instructions on how to do this are not provided here. A. Create working directory mkdir conda-mintpy cd conda-mintpy B. Create conda environment conda create -n conda-build python=3.10 anaconda-client conda activate conda-build C. Clone conda feedstock of mintpy. https://github.com/conda-forge/mintpy-feedstock git clone git@github.com:conda-forge/mintpy-feedstock.git D. Clone mintpy code repo. https://github.com/insarlab/mintpy. Only the latest commit is included to speed up download. Drop the --depth 1 to get the whole history. git clone --depth 1 git@github.com:insarlab/MintPy.git E. Assuming mintpy main is used and is also the latest, we won't make any changes to mintpy The feedstock is a framework for building. It does not have the source code to build within it. Copy Mintpy source to feedstock recipes. cp -r MintPy mintpy-feedstock/recipe/MintPy F. Make versioning clear and build MintPy from local. The VERSION here is a working example. The actual version is subject to change. VERSION=1.3.3.dev.COMMIT_SHORT_HASH G. Edit mintpy-feedstock/recipe/meta.yaml -{% set version = \"1.3.2\" %} +{% set version = 1.3.3.dev.COMMIT_SHORT_HASH %} source: - url: https://github.com/insarlab/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz - sha256: 6c1242dee74f13b96aa4b1f8d50c45ec486397796e4bd4bf3f67849f921bfe7f + path: MintPy H. Build This may take a while. cd mintpy-feedstock python3.10 build-locally.py cd .. I. After a successful build, check for artifacts in mintpy-feedstock/build_artifacts/ J. Push changes to OpenSARLab conda channel. # Login to individual account where you are an owner of OpenSARLab org anaconda login # Upload artifacts anaconda upload --user opensarlab mintpy-feedstock/build_artifacts/noarch/mintpy-*.tar.bz2 K. Download and check version mamba install -c opensarlab -c conda-forge mintpy=VERSION conda list | grep -i mintpy L. Cleanup as desired cd ../ ls rm -rf conda-mintpy conda deactivate conda env remove -n conda-build conda env list","title":"Custom Mintpy Conda Build Instructions"},{"location":"dev-guides/container/mintpy_conda/#build-latest-mintpy-and-push-to-custom-opensarlab-conda-channel","text":"Sometimes the latest version of MintPy is desired within a notebook but the official release is not current. These instructions will show how to build MintPy and push to a custom conda channel. This assumes that the user has an Anaconda.org account and that this account is attached to a OpenSARLab conda channel. Instructions on how to do this are not provided here. A. Create working directory mkdir conda-mintpy cd conda-mintpy B. Create conda environment conda create -n conda-build python=3.10 anaconda-client conda activate conda-build C. Clone conda feedstock of mintpy. https://github.com/conda-forge/mintpy-feedstock git clone git@github.com:conda-forge/mintpy-feedstock.git D. Clone mintpy code repo. https://github.com/insarlab/mintpy. Only the latest commit is included to speed up download. Drop the --depth 1 to get the whole history. git clone --depth 1 git@github.com:insarlab/MintPy.git E. Assuming mintpy main is used and is also the latest, we won't make any changes to mintpy The feedstock is a framework for building. It does not have the source code to build within it. Copy Mintpy source to feedstock recipes. cp -r MintPy mintpy-feedstock/recipe/MintPy F. Make versioning clear and build MintPy from local. The VERSION here is a working example. The actual version is subject to change. VERSION=1.3.3.dev.COMMIT_SHORT_HASH G. Edit mintpy-feedstock/recipe/meta.yaml -{% set version = \"1.3.2\" %} +{% set version = 1.3.3.dev.COMMIT_SHORT_HASH %} source: - url: https://github.com/insarlab/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz - sha256: 6c1242dee74f13b96aa4b1f8d50c45ec486397796e4bd4bf3f67849f921bfe7f + path: MintPy H. Build This may take a while. cd mintpy-feedstock python3.10 build-locally.py cd .. I. After a successful build, check for artifacts in mintpy-feedstock/build_artifacts/ J. Push changes to OpenSARLab conda channel. # Login to individual account where you are an owner of OpenSARLab org anaconda login # Upload artifacts anaconda upload --user opensarlab mintpy-feedstock/build_artifacts/noarch/mintpy-*.tar.bz2 K. Download and check version mamba install -c opensarlab -c conda-forge mintpy=VERSION conda list | grep -i mintpy L. Cleanup as desired cd ../ ls rm -rf conda-mintpy conda deactivate conda env remove -n conda-build conda env list","title":"Build latest mintpy and push to custom OpenSARLab conda channel"},{"location":"dev-guides/portal/build_and_deploy_portal/","text":"Build and Deploy the Portal Enable Under Construction page Sometimes the Portal must be taken down for updates. For instance, the EC2 the Portal runs on needs to be respawned for updates. To help facilitate communication with users, an Under Construction page can be enabled. All traffic to the Portal will be redirected to this page. To enable the page, log into the AWS account and go to EC2 console. Go to the Portal load balancer, select the HTTPS:443 listener, and check the Default rule. In the dropdown Actions menu, select Edit Rule. Set the instance target group weight to 0 . Set the lambda target group weight to 1 . At the bottom of the page Save Changes. Changes should take affect almost immediately. To revert changes after updating, repeat the above steps except change the target group weights so that the instance gets 1 and the lambda gets 0 . ---------- The following documentation is older and must be used with caution. Prerequisites AWS SES: Store SES secrets These secrets will be used to communicate with SES to send emails. \"SMTP credentials consist of a username and a password. When you click the Create button below, SMTP credentials will be generated for you.\" The credentials are AWS access keys, like as used in local aws configs. They are valid for the whole region. https://us-west-2.console.aws.amazon.com/ses/home Create a verified email and take out of sandbox. Create SES serets Go to Account Dashboard > Create SMTP credentials . The IAM User Name should be unique and easy to find within IAM. On user creation, SMTP credentials will be created. Store SES secrets Go to https://us-west-2.console.aws.amazon.com/secretsmanager/home Click on Store New Secret > Other type of secret > Plaintext Delete all empty json content. Add username and password as given previously in the following format: USERNAME PASSWORD . Click Next Secret Name: portal/ses-creds , Tags: osl-billing: osl-portal Click Next Click Next Click Store AWS Secrets Manager: Create SSO token This token will be used by the labs to communicate and authenticate with the portal. All labs and the portal share this token. It is imperative that this remains secret. The form of the token is very specific. Use the following process to create the token. Create secret bash pip install cryptography ```python3 from cryptography.fernet import Fernet api_token = Fernet.generate_key() api_token ``` Add to AWS Secret Manager Go to https://us-west-2.console.aws.amazon.com/secretsmanager/home Click on Store New Secret > Other type of secret > Plaintext Delete all empty json content. Add api_token . Click Next Secret Name: $CONTAINER_NAMESPACE/sso-token , Tags: osl-billing: osl-portal Click Next Click Next Click Store Docker registry For local dev development, one can use a local docker registry. docker run -d -p 5000:5000 --restart=always --name registry registry:2 Otherwise, the remote docker images will be stored in AWS ECR, as setup by CloudFormation Docker repo Clone the portal code: git clone git@github.com:ASFOpenSARlab/deployment-opensciencelab-prod-portal.git Setup If production, upload the Cloudformation template cf-portal-setup.yaml and build. Once the cloudformation is done, go to EC2 Connect of the portal EC2, log onto the server and cd /home/ec2-user/code . Then setup prerequisites via make setup-ec2 . Note that you will be warned about reformatting the DB volume. If this is the first time running (as it should be), do so. If locally, go to the root of the docker repo. Then setup prerequisites via make setup-ubuntu . Build cp labs.example.yaml labs.{maturity}.yaml . The name of the config doesn't matter (except it cannot be labs.run.yaml) Update labs.{maturity}.yaml as needed make config=labs.{maturity}.yaml Destroy If production, clear out the registry images, delete the CloudFormation setup, delete db ebs snapshots, and delete logs. If locally, make clean and then stop the localhost registry (if being used). Other less used procedures Logs In production, normally the logs will show up in CloudWatch. For both, docker compose logs -f . Replace Portal DB from snapshot If the Portal DB needs to be replaced by a snapshot backup, do the following. Unless otherwise sated, all of these steps take place within EC2 Connect. Elevated permissions will be needed via sudo or sudo su - . Restore backup snapshot to volume This procedure assumes that the usual DB volume is present and being used. We only want to update the DB file. Within cf-portal-setup.yaml , it is assumed that the AZ of the EC2's subnet is set as us-west-2a. a. From the EC2 console, select the snapshot that will be restored. Get the SNAPSHOT_ID, e.g. snap-0c0dbee2e7c9f0c12 b. From the EC2 console, select the portal EC2. Get the EC2_INSTANCE_ID, e.g. i-0ca96843e97d9bd29 c. First run a dry run to make sure permissions are available for EBS volume creation. aws ec2 create-volume \\ --dry-run \\ --availability-zone us-west-2a \\ --snapshot-id $SNAPSHOT_ID \\ --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=portal-db-backup}]' Then actually create an EBS volume from the backup snapshot. ``` aws ec2 create-volume \\ --availability-zone us-west-2a \\ --snapshot-id $SNAPSHOT_ID \\ --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=portal-db-backup}]' ``` From response output, get VOLUME_ID, e.g. vol-0a0869b5ab9f77090 Attach backup volume to EC2 First run a dry run to make sure permissions are available. aws ec2 attach-volume \\ --dry-run \\ --device /dev/sdm \\ --instance-id $EC2_INSTANCE_ID \\ --volume-id $VOLUME_ID Then actually run it. aws ec2 attach-volume \\ --device /dev/sdm \\ --instance-id $EC2_INSTANCE_ID \\ --volume-id $VOLUME_ID Mount device to filesystem sudo mkdir -p /tmp/portal-db-from-snapshot/ sudo mount /dev/sdm /tmp/portal-db-from-snapshot/ If you get an error message something like /wrong fs type, bad option, bad superblock then you cannot mount the filesystem. AWS's way of handling volumes makes things difficult. https://serverfault.com/questions/948408/mount-wrong-fs-type-bad-option-bad-superblock-on-dev-xvdf1-missing-codepage Since we are working with a temporay mount, run the following instead: sudo mount -t xfs -o nouuid /dev/sdm /tmp/portal-db-from-snapshot/ . Check for mounted directories df Look for something like (not /dev/xvdj) /dev/xvdj 1038336 34224 1004112 4% /srv/jupyterhub /dev/xvdm 1038336 34224 1004112 4% /tmp/portal-db-from-snapshot Create a backup of the old DB file within the filesystem for just in case sudo cp ./srv/portal/jupyterhub/jupyterhub.sqlite ./srv/portal/jupyterhub/jupyterhub.sqlite.$(date +\"%F-%H-%M-%S\") Copy over backup DB file sudo cp /tmp/portal-db-from-snapshot/jupyterhub.sqlite ./srv/portal/jupyterhub/jupyterhub.sqlite Unmount and detach backup volume from EC2 sudo umount /tmp/portal-db-from-snapshot/ sudo aws ec2 detach-volume --volume-id VOLUME_ID Delete backup volume sudo aws ec2 delete-volume --volume-id VOLUME_ID","title":"Build and Deploy the Portal"},{"location":"dev-guides/portal/build_and_deploy_portal/#build-and-deploy-the-portal","text":"","title":"Build and Deploy the Portal"},{"location":"dev-guides/portal/build_and_deploy_portal/#enable-under-construction-page","text":"Sometimes the Portal must be taken down for updates. For instance, the EC2 the Portal runs on needs to be respawned for updates. To help facilitate communication with users, an Under Construction page can be enabled. All traffic to the Portal will be redirected to this page. To enable the page, log into the AWS account and go to EC2 console. Go to the Portal load balancer, select the HTTPS:443 listener, and check the Default rule. In the dropdown Actions menu, select Edit Rule. Set the instance target group weight to 0 . Set the lambda target group weight to 1 . At the bottom of the page Save Changes. Changes should take affect almost immediately. To revert changes after updating, repeat the above steps except change the target group weights so that the instance gets 1 and the lambda gets 0 .","title":"Enable Under Construction page"},{"location":"dev-guides/portal/build_and_deploy_portal/#-","text":"The following documentation is older and must be used with caution.","title":"----------"},{"location":"dev-guides/portal/build_and_deploy_portal/#prerequisites","text":"","title":"Prerequisites"},{"location":"dev-guides/portal/build_and_deploy_portal/#aws-ses-store-ses-secrets","text":"These secrets will be used to communicate with SES to send emails. \"SMTP credentials consist of a username and a password. When you click the Create button below, SMTP credentials will be generated for you.\" The credentials are AWS access keys, like as used in local aws configs. They are valid for the whole region. https://us-west-2.console.aws.amazon.com/ses/home Create a verified email and take out of sandbox. Create SES serets Go to Account Dashboard > Create SMTP credentials . The IAM User Name should be unique and easy to find within IAM. On user creation, SMTP credentials will be created. Store SES secrets Go to https://us-west-2.console.aws.amazon.com/secretsmanager/home Click on Store New Secret > Other type of secret > Plaintext Delete all empty json content. Add username and password as given previously in the following format: USERNAME PASSWORD . Click Next Secret Name: portal/ses-creds , Tags: osl-billing: osl-portal Click Next Click Next Click Store","title":"AWS SES: Store SES secrets"},{"location":"dev-guides/portal/build_and_deploy_portal/#aws-secrets-manager-create-sso-token","text":"This token will be used by the labs to communicate and authenticate with the portal. All labs and the portal share this token. It is imperative that this remains secret. The form of the token is very specific. Use the following process to create the token. Create secret bash pip install cryptography ```python3 from cryptography.fernet import Fernet api_token = Fernet.generate_key() api_token ``` Add to AWS Secret Manager Go to https://us-west-2.console.aws.amazon.com/secretsmanager/home Click on Store New Secret > Other type of secret > Plaintext Delete all empty json content. Add api_token . Click Next Secret Name: $CONTAINER_NAMESPACE/sso-token , Tags: osl-billing: osl-portal Click Next Click Next Click Store","title":"AWS Secrets Manager: Create SSO token"},{"location":"dev-guides/portal/build_and_deploy_portal/#docker-registry","text":"For local dev development, one can use a local docker registry. docker run -d -p 5000:5000 --restart=always --name registry registry:2 Otherwise, the remote docker images will be stored in AWS ECR, as setup by CloudFormation","title":"Docker registry"},{"location":"dev-guides/portal/build_and_deploy_portal/#docker-repo","text":"Clone the portal code: git clone git@github.com:ASFOpenSARlab/deployment-opensciencelab-prod-portal.git","title":"Docker repo"},{"location":"dev-guides/portal/build_and_deploy_portal/#setup","text":"If production, upload the Cloudformation template cf-portal-setup.yaml and build. Once the cloudformation is done, go to EC2 Connect of the portal EC2, log onto the server and cd /home/ec2-user/code . Then setup prerequisites via make setup-ec2 . Note that you will be warned about reformatting the DB volume. If this is the first time running (as it should be), do so. If locally, go to the root of the docker repo. Then setup prerequisites via make setup-ubuntu .","title":"Setup"},{"location":"dev-guides/portal/build_and_deploy_portal/#build","text":"cp labs.example.yaml labs.{maturity}.yaml . The name of the config doesn't matter (except it cannot be labs.run.yaml) Update labs.{maturity}.yaml as needed make config=labs.{maturity}.yaml","title":"Build"},{"location":"dev-guides/portal/build_and_deploy_portal/#destroy","text":"If production, clear out the registry images, delete the CloudFormation setup, delete db ebs snapshots, and delete logs. If locally, make clean and then stop the localhost registry (if being used).","title":"Destroy"},{"location":"dev-guides/portal/build_and_deploy_portal/#other-less-used-procedures","text":"","title":"Other less used procedures"},{"location":"dev-guides/portal/build_and_deploy_portal/#logs","text":"In production, normally the logs will show up in CloudWatch. For both, docker compose logs -f .","title":"Logs"},{"location":"dev-guides/portal/build_and_deploy_portal/#replace-portal-db-from-snapshot","text":"If the Portal DB needs to be replaced by a snapshot backup, do the following. Unless otherwise sated, all of these steps take place within EC2 Connect. Elevated permissions will be needed via sudo or sudo su - . Restore backup snapshot to volume This procedure assumes that the usual DB volume is present and being used. We only want to update the DB file. Within cf-portal-setup.yaml , it is assumed that the AZ of the EC2's subnet is set as us-west-2a. a. From the EC2 console, select the snapshot that will be restored. Get the SNAPSHOT_ID, e.g. snap-0c0dbee2e7c9f0c12 b. From the EC2 console, select the portal EC2. Get the EC2_INSTANCE_ID, e.g. i-0ca96843e97d9bd29 c. First run a dry run to make sure permissions are available for EBS volume creation. aws ec2 create-volume \\ --dry-run \\ --availability-zone us-west-2a \\ --snapshot-id $SNAPSHOT_ID \\ --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=portal-db-backup}]' Then actually create an EBS volume from the backup snapshot. ``` aws ec2 create-volume \\ --availability-zone us-west-2a \\ --snapshot-id $SNAPSHOT_ID \\ --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=portal-db-backup}]' ``` From response output, get VOLUME_ID, e.g. vol-0a0869b5ab9f77090 Attach backup volume to EC2 First run a dry run to make sure permissions are available. aws ec2 attach-volume \\ --dry-run \\ --device /dev/sdm \\ --instance-id $EC2_INSTANCE_ID \\ --volume-id $VOLUME_ID Then actually run it. aws ec2 attach-volume \\ --device /dev/sdm \\ --instance-id $EC2_INSTANCE_ID \\ --volume-id $VOLUME_ID Mount device to filesystem sudo mkdir -p /tmp/portal-db-from-snapshot/ sudo mount /dev/sdm /tmp/portal-db-from-snapshot/ If you get an error message something like /wrong fs type, bad option, bad superblock then you cannot mount the filesystem. AWS's way of handling volumes makes things difficult. https://serverfault.com/questions/948408/mount-wrong-fs-type-bad-option-bad-superblock-on-dev-xvdf1-missing-codepage Since we are working with a temporay mount, run the following instead: sudo mount -t xfs -o nouuid /dev/sdm /tmp/portal-db-from-snapshot/ . Check for mounted directories df Look for something like (not /dev/xvdj) /dev/xvdj 1038336 34224 1004112 4% /srv/jupyterhub /dev/xvdm 1038336 34224 1004112 4% /tmp/portal-db-from-snapshot Create a backup of the old DB file within the filesystem for just in case sudo cp ./srv/portal/jupyterhub/jupyterhub.sqlite ./srv/portal/jupyterhub/jupyterhub.sqlite.$(date +\"%F-%H-%M-%S\") Copy over backup DB file sudo cp /tmp/portal-db-from-snapshot/jupyterhub.sqlite ./srv/portal/jupyterhub/jupyterhub.sqlite Unmount and detach backup volume from EC2 sudo umount /tmp/portal-db-from-snapshot/ sudo aws ec2 detach-volume --volume-id VOLUME_ID Delete backup volume sudo aws ec2 delete-volume --volume-id VOLUME_ID","title":"Replace Portal DB from snapshot"},{"location":"dev-guides/portal/notifications/","text":"Return to Developer Guide Create OpenSARLab Notifications Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"OpenSARLab Notifications"},{"location":"dev-guides/portal/notifications/#create-opensarlab-notifications","text":"Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"Create OpenSARLab Notifications"},{"location":"release-notes/release_02-2022/","text":"Welcome to the February 2022 OpenSARLab Update! Changes: Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update Ubuntu 20.04.3 LTS JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04 JupyterLab There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th. Matplotlib widget matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook. Url-widget The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks. New Memory Monitor Location JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen. Notebook Debugger JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs Mamba The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba. Mamba Gator is installed mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu. Spellchecker The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen. Custom Extensions We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles. Recommended Jupyter Notebook Changes The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"February 2022"},{"location":"release-notes/release_02-2022/#welcome-to-the-february-2022-opensarlab-update","text":"","title":"Welcome to the February 2022 OpenSARLab Update!"},{"location":"release-notes/release_02-2022/#changes","text":"Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update","title":"Changes:"},{"location":"release-notes/release_02-2022/#ubuntu-20043-lts","text":"JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04","title":"Ubuntu 20.04.3 LTS"},{"location":"release-notes/release_02-2022/#jupyterlab","text":"There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th.","title":"JupyterLab"},{"location":"release-notes/release_02-2022/#matplotlib-widget","text":"matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook.","title":"Matplotlib widget"},{"location":"release-notes/release_02-2022/#url-widget","text":"The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks.","title":"Url-widget"},{"location":"release-notes/release_02-2022/#new-memory-monitor-location","text":"JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen.","title":"New Memory Monitor Location"},{"location":"release-notes/release_02-2022/#notebook-debugger","text":"JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs","title":"Notebook Debugger"},{"location":"release-notes/release_02-2022/#mamba","text":"The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba.","title":"Mamba"},{"location":"release-notes/release_02-2022/#mamba-gator-is-installed","text":"mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu.","title":"Mamba Gator is installed"},{"location":"release-notes/release_02-2022/#spellchecker","text":"The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen.","title":"Spellchecker"},{"location":"release-notes/release_02-2022/#custom-extensions","text":"We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles.","title":"Custom Extensions"},{"location":"release-notes/release_02-2022/#recommended-jupyter-notebook-changes","text":"The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"Recommended Jupyter Notebook Changes"},{"location":"release-notes/release_02-2023/","text":"Welcome to the February 2023 OpenScienceLab Update! Changes: OpenScienceLab Single Sign On Automatic Authentication User Access IP Filter User Request Multi Download Kernel Usage Recommended Jupyter Notebook Changes OpenScienceLab OpenSARLab is now a part of OpenScienceLab with significant changes. Users can access different courses from a single page. Single Sign On New single sign-on (SSO) to simplify the logging-in process. SSO eliminates the burden of the sign-on process by listing all accounts on a single web page with additional layers of security. Automatic Authentication Users who sign up to the OpenScienceLab no longer need admins to activate their accounts. User Access Admins can easily give users access to different profiles from a single page. Admins can toggle access lists based on various deployments. IP Filter OpenScienceLab will prevent users in one of NASA\u2019s designated countries list from creating an account so that we are compliant with NASA\u2019s security protocol. User Request Users can send a request to the OpenScienceLab team from the portal. Multi Download Users can download multiple files at once through OpenScienceLab. NB : Downloading multiple directories requires users to compress the directories beforehand. Kernel Usage The latest update of JupyterLab has a built-in kernel monitor that tracks detailed resource usage, such as: Kernel Host Memory consumption and availability CPU usage Recommended Jupyter Notebook Changes The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab: MintPy Update: Due to the recent MintPy update, import syntax for version 1.4.1+ differs from previous versions. As a result, some notebooks may be incompatible with the latest version of MintPy. Below are examples of how to import MintPy depending on which versions you have: ``` python version 1.4.0 and below: import mintpy.view as view import mintpy.tsview as tsview . . . version 1.4.1+ from mintpy.cli import view, tsview, ... ``` For additional backward compatibility changes, please refer to the previous release notes.","title":"February 2023"},{"location":"release-notes/release_02-2023/#welcome-to-the-february-2023-opensciencelab-update","text":"","title":"Welcome to the February 2023 OpenScienceLab Update!"},{"location":"release-notes/release_02-2023/#changes","text":"OpenScienceLab Single Sign On Automatic Authentication User Access IP Filter User Request Multi Download Kernel Usage Recommended Jupyter Notebook Changes","title":"Changes:"},{"location":"release-notes/release_02-2023/#opensciencelab","text":"OpenSARLab is now a part of OpenScienceLab with significant changes. Users can access different courses from a single page.","title":"OpenScienceLab"},{"location":"release-notes/release_02-2023/#single-sign-on","text":"New single sign-on (SSO) to simplify the logging-in process. SSO eliminates the burden of the sign-on process by listing all accounts on a single web page with additional layers of security.","title":"Single Sign On"},{"location":"release-notes/release_02-2023/#automatic-authentication","text":"Users who sign up to the OpenScienceLab no longer need admins to activate their accounts.","title":"Automatic Authentication"},{"location":"release-notes/release_02-2023/#user-access","text":"Admins can easily give users access to different profiles from a single page. Admins can toggle access lists based on various deployments.","title":"User Access"},{"location":"release-notes/release_02-2023/#ip-filter","text":"OpenScienceLab will prevent users in one of NASA\u2019s designated countries list from creating an account so that we are compliant with NASA\u2019s security protocol.","title":"IP Filter"},{"location":"release-notes/release_02-2023/#user-request","text":"Users can send a request to the OpenScienceLab team from the portal.","title":"User Request"},{"location":"release-notes/release_02-2023/#multi-download","text":"Users can download multiple files at once through OpenScienceLab. NB : Downloading multiple directories requires users to compress the directories beforehand.","title":"Multi Download"},{"location":"release-notes/release_02-2023/#kernel-usage","text":"The latest update of JupyterLab has a built-in kernel monitor that tracks detailed resource usage, such as: Kernel Host Memory consumption and availability CPU usage","title":"Kernel Usage"},{"location":"release-notes/release_02-2023/#recommended-jupyter-notebook-changes","text":"The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab: MintPy Update: Due to the recent MintPy update, import syntax for version 1.4.1+ differs from previous versions. As a result, some notebooks may be incompatible with the latest version of MintPy. Below are examples of how to import MintPy depending on which versions you have: ``` python","title":"Recommended Jupyter Notebook Changes"},{"location":"release-notes/release_02-2023/#version-140-and-below","text":"import mintpy.view as view import mintpy.tsview as tsview . . .","title":"version 1.4.0 and below:"},{"location":"release-notes/release_02-2023/#version-141","text":"from mintpy.cli import view, tsview, ... ``` For additional backward compatibility changes, please refer to the previous release notes.","title":"version 1.4.1+"},{"location":"release-notes/release_02-2024/","text":"Welcome to the February 2024 OpenScienceLab Update! Changes: Multi-Factor Authentication Multi-Factor Authentication OpenScienceLab now requires users to set up Multi-Factor Authentication (MFA) to access OpenScienceLab resources. Directions on how to do so, as well as troubleshooting recommendations, can be found on the new MFA documentation page","title":"February 2024"},{"location":"release-notes/release_02-2024/#welcome-to-the-february-2024-opensciencelab-update","text":"","title":"Welcome to the February 2024 OpenScienceLab Update!"},{"location":"release-notes/release_02-2024/#changes","text":"Multi-Factor Authentication","title":"Changes:"},{"location":"release-notes/release_02-2024/#multi-factor-authentication","text":"OpenScienceLab now requires users to set up Multi-Factor Authentication (MFA) to access OpenScienceLab resources. Directions on how to do so, as well as troubleshooting recommendations, can be found on the new MFA documentation page","title":"Multi-Factor Authentication"},{"location":"release-notes/release_06-2021/","text":"Welcome to the June 2021 OpenSARlab Upgrade! Changes: conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier Conda Environments What is conda and what are conda environments? Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases. How did OpenSARlab use conda previously? OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized. What problems did this cause? Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult What has changed? All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like What will happen if I don't create any new environments? You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError). I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now? Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add. nbgitpuller Patch What is nbgitpuller? nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update. What was wrong with nbgitpuller? nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail. How was this issue addressed? We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line Installed jupyter-resource-usage What is jupyter-resource-usage? jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc. Added profile identifier The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"June 2021"},{"location":"release-notes/release_06-2021/#welcome-to-the-june-2021-opensarlab-upgrade","text":"","title":"Welcome to the June 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_06-2021/#changes","text":"conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier","title":"Changes:"},{"location":"release-notes/release_06-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_06-2021/#what-is-conda-and-what-are-conda-environments","text":"Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases.","title":"What is conda and what are conda environments?"},{"location":"release-notes/release_06-2021/#how-did-opensarlab-use-conda-previously","text":"OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized.","title":"How did OpenSARlab use conda previously?"},{"location":"release-notes/release_06-2021/#what-problems-did-this-cause","text":"Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult","title":"What problems did this cause?"},{"location":"release-notes/release_06-2021/#what-has-changed","text":"All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like","title":"What has changed?"},{"location":"release-notes/release_06-2021/#what-will-happen-if-i-dont-create-any-new-environments","text":"You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError).","title":"What will happen if I don't create any new environments?"},{"location":"release-notes/release_06-2021/#i-dont-want-to-wait-for-the-notebooks-to-yell-at-me-and-give-me-environment-warnings-how-can-i-create-the-environments-id-like-right-now","text":"Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add.","title":"I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now?"},{"location":"release-notes/release_06-2021/#nbgitpuller-patch","text":"","title":"nbgitpuller Patch"},{"location":"release-notes/release_06-2021/#what-is-nbgitpuller","text":"nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update.","title":"What is nbgitpuller?"},{"location":"release-notes/release_06-2021/#what-was-wrong-with-nbgitpuller","text":"nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail.","title":"What was wrong with nbgitpuller?"},{"location":"release-notes/release_06-2021/#how-was-this-issue-addressed","text":"We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line","title":"How was this issue addressed?"},{"location":"release-notes/release_06-2021/#installed-jupyter-resource-usage","text":"","title":"Installed jupyter-resource-usage"},{"location":"release-notes/release_06-2021/#what-is-jupyter-resource-usage","text":"jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc.","title":"What is jupyter-resource-usage?"},{"location":"release-notes/release_06-2021/#added-profile-identifier","text":"The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"Added profile identifier"},{"location":"release-notes/release_10-2021/","text":"Welcome to the October 2021 OpenSARlab Upgrade! Changes: Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page Conda Environments Pull in The GEOS657_MRS Repository The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location. OpenSARlab documentation changes OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab. Add current profile name to the top of every page Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"October 2021"},{"location":"release-notes/release_10-2021/#welcome-to-the-october-2021-opensarlab-upgrade","text":"","title":"Welcome to the October 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_10-2021/#changes","text":"Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page","title":"Changes:"},{"location":"release-notes/release_10-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_10-2021/#pull-in-the-geos657_mrs-repository","text":"The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location.","title":"Pull in The GEOS657_MRS Repository"},{"location":"release-notes/release_10-2021/#opensarlab-documentation-changes","text":"OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab.","title":"OpenSARlab documentation changes"},{"location":"release-notes/release_10-2021/#add-current-profile-name-to-the-top-of-every-page","text":"Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"Add current profile name to the top of every page"},{"location":"release-notes/release_12-2024/","text":"Welcome to the February 2024 OpenScienceLab Update! Changes: Documentation updated to reflect current code for container, cluster, and portal.","title":"Welcome to the February 2024 OpenScienceLab Update!"},{"location":"release-notes/release_12-2024/#welcome-to-the-february-2024-opensciencelab-update","text":"","title":"Welcome to the February 2024 OpenScienceLab Update!"},{"location":"release-notes/release_12-2024/#changes","text":"Documentation updated to reflect current code for container, cluster, and portal.","title":"Changes:"},{"location":"user-guides/OpenSARLab_environment/","text":"Return to Table of Contents The OpenScienceLab Environment and the Account Lifecycle Account Lifecycle Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 35 , 40 , 44 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation. NB : While the above is valid for the OpenSARLab deployment, each deployment's lifecycle period may differ. Since the OpenSARLab is the most commonly used deployment, the information listed on this page will mainly focus on the OpenSARLab deployment. OpenSARLab Environment Every OpenSARLab user has access to an Amazon AWS EC2 instance. Depending on demand, the individual user shares their resources with up to 3 users. Operating System Ubuntu 20.04.3 LTS Volume (storage) *500GB of EBS volume storage per user. * Volume size subject to change . OpenSARLab uses Amazon AWS EBS volumes mounted on user servers' home directories for storage. Since volumes are costly to maintain, any inactive accounts will have their volumes destroyed and replaced with the latest snapshot as a backup within a few days. Important Notes about Volume If your volume has been destroyed, the latest snapshot will regenerate a new EBS volume upon the next login. Upon successful login, your account should be identical from your previous session. When regenerating the volume from the snapshot, it can take some time (10+ minutes) to restore all the data. Notebooks may load slower than usual during this period. If users occupy more space than allocated, it will trigger the out-of-storage exception that will prevent users from logging in. It is the users' responsibility to manage their storage. Please contact an OpenScienceLab administrator if you need help logging in. Snapshot (Backup) A snapshot is a backup copy of users' volume. The snapshot of each volume is taken every day at 10:00 UTC. Only the most recent snapshot is retained. The user storage is persistent; you will only lose saved work if your account is inactive for 46 days. Memory (RAM) RAM allocated per user: 6GB - 16GB The amount of memory available to each user depends on the overall instance and may vary from 6 GB to 16 GB. The number of users on the same instance determines the amount of usable memory. Privileges Users do not have root (i.e., sudo ) privileges.","title":"OpenSARLab Account Details"},{"location":"user-guides/OpenSARLab_environment/#the-opensciencelab-environment-and-the-account-lifecycle","text":"","title":"The OpenScienceLab Environment and the Account Lifecycle"},{"location":"user-guides/OpenSARLab_environment/#account-lifecycle","text":"Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 35 , 40 , 44 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation. NB : While the above is valid for the OpenSARLab deployment, each deployment's lifecycle period may differ. Since the OpenSARLab is the most commonly used deployment, the information listed on this page will mainly focus on the OpenSARLab deployment.","title":"Account Lifecycle"},{"location":"user-guides/OpenSARLab_environment/#opensarlab-environment","text":"Every OpenSARLab user has access to an Amazon AWS EC2 instance. Depending on demand, the individual user shares their resources with up to 3 users.","title":"OpenSARLab Environment"},{"location":"user-guides/OpenSARLab_environment/#operating-system","text":"Ubuntu 20.04.3 LTS","title":"Operating System"},{"location":"user-guides/OpenSARLab_environment/#volume-storage","text":"*500GB of EBS volume storage per user. * Volume size subject to change . OpenSARLab uses Amazon AWS EBS volumes mounted on user servers' home directories for storage. Since volumes are costly to maintain, any inactive accounts will have their volumes destroyed and replaced with the latest snapshot as a backup within a few days.","title":"Volume (storage)"},{"location":"user-guides/OpenSARLab_environment/#important-notes-about-volume","text":"If your volume has been destroyed, the latest snapshot will regenerate a new EBS volume upon the next login. Upon successful login, your account should be identical from your previous session. When regenerating the volume from the snapshot, it can take some time (10+ minutes) to restore all the data. Notebooks may load slower than usual during this period. If users occupy more space than allocated, it will trigger the out-of-storage exception that will prevent users from logging in. It is the users' responsibility to manage their storage. Please contact an OpenScienceLab administrator if you need help logging in.","title":"Important Notes about Volume"},{"location":"user-guides/OpenSARLab_environment/#snapshot-backup","text":"A snapshot is a backup copy of users' volume. The snapshot of each volume is taken every day at 10:00 UTC. Only the most recent snapshot is retained. The user storage is persistent; you will only lose saved work if your account is inactive for 46 days.","title":"Snapshot (Backup)"},{"location":"user-guides/OpenSARLab_environment/#memory-ram","text":"RAM allocated per user: 6GB - 16GB The amount of memory available to each user depends on the overall instance and may vary from 6 GB to 16 GB. The number of users on the same instance determines the amount of usable memory.","title":"Memory (RAM)"},{"location":"user-guides/OpenSARLab_environment/#privileges","text":"Users do not have root (i.e., sudo ) privileges.","title":"Privileges"},{"location":"user-guides/OpenSARLab_terminal/","text":"Return to Table of Contents Using the Terminal in OpenSARLab Overview Users sometimes need to use an interactive shell to organize their accounts. With OpenSceinceLab\u2019s built-in terminal, users can use an interactive shell to accomplish their tasks. How to Open a Terminal If there is no Launcher tab in your workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in the Launcher tab. Live Example : How to Use the Terminal Use the command line as you would in any other Unix-like terminal. No Root Privileges Because the jovyan does not have a password, OpenScienceLab users cannot use the sudo command.","title":"OpenSARLab Terminal"},{"location":"user-guides/OpenSARLab_terminal/#using-the-terminal-in-opensarlab","text":"","title":"Using the Terminal in OpenSARLab"},{"location":"user-guides/OpenSARLab_terminal/#overview","text":"Users sometimes need to use an interactive shell to organize their accounts. With OpenSceinceLab\u2019s built-in terminal, users can use an interactive shell to accomplish their tasks.","title":"Overview"},{"location":"user-guides/OpenSARLab_terminal/#how-to-open-a-terminal","text":"If there is no Launcher tab in your workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in the Launcher tab. Live Example :","title":"How to Open a Terminal"},{"location":"user-guides/OpenSARLab_terminal/#how-to-use-the-terminal","text":"Use the command line as you would in any other Unix-like terminal.","title":"How to Use the Terminal"},{"location":"user-guides/OpenSARLab_terminal/#no-root-privileges","text":"Because the jovyan does not have a password, OpenScienceLab users cannot use the sudo command.","title":"No Root Privileges"},{"location":"user-guides/class_notebooks_best_practices/","text":"Return to Table of Contents Developing Notebooks for Classes or Trainings: Best Practices Provide a Conda Environment Capable of Running the Notebooks Provide students with a conda environment that has everything they need. Conda Environments in OpenSARLab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks. Set the Notebook Metadata to Use the Correct Environment Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created). Clear Your Notebook Output Before Saving it Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo. Keep your Conda Environment Up to Date Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARLab to create them, which is located in the /home/jovyan/conda_environments/ directory. Test Notebooks Ahead of Time. If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment. Plan for Students with Poor Internet Access Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner. To avoid issues related to poor internet access, consider following options: Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle . Avoid Changing Directories in Your Code Why? Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students. How? If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"Best Practices for Writing Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#developing-notebooks-for-classes-or-trainings-best-practices","text":"","title":"Developing Notebooks for Classes or Trainings: Best Practices"},{"location":"user-guides/class_notebooks_best_practices/#provide-a-conda-environment-capable-of-running-the-notebooks","text":"Provide students with a conda environment that has everything they need. Conda Environments in OpenSARLab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks.","title":"Provide a Conda Environment Capable of Running the Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#set-the-notebook-metadata-to-use-the-correct-environment","text":"Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created).","title":"Set the Notebook Metadata to Use the Correct Environment"},{"location":"user-guides/class_notebooks_best_practices/#clear-your-notebook-output-before-saving-it","text":"Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo.","title":"Clear Your Notebook Output Before Saving it"},{"location":"user-guides/class_notebooks_best_practices/#keep-your-conda-environment-up-to-date","text":"Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARLab to create them, which is located in the /home/jovyan/conda_environments/ directory.","title":"Keep your Conda Environment Up to Date"},{"location":"user-guides/class_notebooks_best_practices/#test-notebooks-ahead-of-time","text":"If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment.","title":"Test Notebooks Ahead of Time."},{"location":"user-guides/class_notebooks_best_practices/#plan-for-students-with-poor-internet-access","text":"Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner.","title":"Plan for Students with Poor Internet Access"},{"location":"user-guides/class_notebooks_best_practices/#to-avoid-issues-related-to-poor-internet-access-consider-following-options","text":"Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle .","title":"To avoid issues related to poor internet access, consider following options:"},{"location":"user-guides/class_notebooks_best_practices/#avoid-changing-directories-in-your-code","text":"","title":"Avoid Changing Directories in Your Code"},{"location":"user-guides/class_notebooks_best_practices/#why","text":"Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students.","title":"Why?"},{"location":"user-guides/class_notebooks_best_practices/#how","text":"If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"How?"},{"location":"user-guides/conda_environments/","text":"Return to Table of Contents Creating and Using Conda Environments in OpenSARLab OpenScienceLab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following conda environments are by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning autorift nisar_se unavco However, these environments are not pre-built; users must create the desired conda environments. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Here is a live demonstration on how to build the conda environment. Your environment will be ready after running the last cell. NB : It may take a while to generate your conda environment.","title":"Conda Environments"},{"location":"user-guides/conda_environments/#creating-and-using-conda-environments-in-opensarlab","text":"OpenScienceLab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following conda environments are by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning autorift nisar_se unavco However, these environments are not pre-built; users must create the desired conda environments. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Here is a live demonstration on how to build the conda environment. Your environment will be ready after running the last cell. NB : It may take a while to generate your conda environment.","title":"Creating and Using Conda Environments in OpenSARLab"},{"location":"user-guides/git_in_OpenSARLab/","text":"Return to Table of Contents Git in OpenScienceLab Prerequisite Git - Version control systems that allow you to track changes to your files. ASF's Jupyter Notebook - A collection of Jupyter Notebooks used in OpenSARLab. Terminal - A built-in terminal within OpenScienceLab. The user should also have a basic understanding of Bash commands. Gitpuller A nbgitpuller pulls any changes to the notebook repo each time an OpenSARLab deployment server starts up. In short words, nbgitpuller will automatically update the notebooks to the latest version. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: Users will retain two copies of the same notebook. - The user-edited notebook will have a timestamp appended to its name. - The notebook with the original name will contain the new changes made by ASF. Example: Before Edit - Original version: sample_notebook.ipynb After Edit - Updated by user: sample_notebook__20210616165846.ipynb - Updated by ASF: sample_notebook.ipynb NB: The nbgitpuller will only run if you are in the main branch of the asf-jupyter-notebook repo. In the case of a broken Git state Due to its complexity, it is common for users to break Git workflow. A broken Git state can lead to unexpected results, such as notebooks not being updated. Below are the steps users can take if the Git workflow is in a state beyond their ability to repair. Repair Process: Preserve any files/directories you wish to keep under the /home/jovyan/notebooks directory. Either * download or move out items you wish to keep. Delete the entire /home/jovyan/notebooks directory. Use rm -rf /home/jovyan/notebooks on the terminal to delete all. Restart your server. Gitpuller will automatically clone a clean repository into your account. * NB : When downloading a directory, users may compress them first to optimize downloading. The below commands will allow users to (de)compress files/directories: Compress: zip -r <name>.zip <directory> Decompress: unzip <name>.zip Where <name> can be anything you wish. Using Other Git Repos in OpenScienceLab If you wish to utilize repositories other than those hosted by ASF, you can clone them into your OpenScienceLab account. However, users will need to manage the repos that they clone to prevent any issues. When cloning a repository from elsewhere, users must ensure not to nest git repositories, i.e., do not clone a repository within another repository. To avoid complications, clone your repos to /home/jovyan .","title":"Git in OpenSARLab"},{"location":"user-guides/git_in_OpenSARLab/#git-in-opensciencelab","text":"","title":"Git in OpenScienceLab"},{"location":"user-guides/git_in_OpenSARLab/#prerequisite","text":"Git - Version control systems that allow you to track changes to your files. ASF's Jupyter Notebook - A collection of Jupyter Notebooks used in OpenSARLab. Terminal - A built-in terminal within OpenScienceLab. The user should also have a basic understanding of Bash commands.","title":"Prerequisite"},{"location":"user-guides/git_in_OpenSARLab/#gitpuller","text":"A nbgitpuller pulls any changes to the notebook repo each time an OpenSARLab deployment server starts up. In short words, nbgitpuller will automatically update the notebooks to the latest version. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: Users will retain two copies of the same notebook. - The user-edited notebook will have a timestamp appended to its name. - The notebook with the original name will contain the new changes made by ASF. Example: Before Edit - Original version: sample_notebook.ipynb After Edit - Updated by user: sample_notebook__20210616165846.ipynb - Updated by ASF: sample_notebook.ipynb NB: The nbgitpuller will only run if you are in the main branch of the asf-jupyter-notebook repo.","title":"Gitpuller"},{"location":"user-guides/git_in_OpenSARLab/#in-the-case-of-a-broken-git-state","text":"Due to its complexity, it is common for users to break Git workflow. A broken Git state can lead to unexpected results, such as notebooks not being updated. Below are the steps users can take if the Git workflow is in a state beyond their ability to repair. Repair Process: Preserve any files/directories you wish to keep under the /home/jovyan/notebooks directory. Either * download or move out items you wish to keep. Delete the entire /home/jovyan/notebooks directory. Use rm -rf /home/jovyan/notebooks on the terminal to delete all. Restart your server. Gitpuller will automatically clone a clean repository into your account. * NB : When downloading a directory, users may compress them first to optimize downloading. The below commands will allow users to (de)compress files/directories: Compress: zip -r <name>.zip <directory> Decompress: unzip <name>.zip Where <name> can be anything you wish.","title":"In the case of a broken Git state"},{"location":"user-guides/git_in_OpenSARLab/#using-other-git-repos-in-opensciencelab","text":"If you wish to utilize repositories other than those hosted by ASF, you can clone them into your OpenScienceLab account. However, users will need to manage the repos that they clone to prevent any issues. When cloning a repository from elsewhere, users must ensure not to nest git repositories, i.e., do not clone a repository within another repository. To avoid complications, clone your repos to /home/jovyan .","title":"Using Other Git Repos in OpenScienceLab"},{"location":"user-guides/how_to_run_a_notebook/","text":"Return to Table of Contents Getting Started A Light Introduction to Jupyter Notebook Markdown Cells Code Cells Detailed Instructions on Running Jupyter Notebook Selecting Cells Edit Mode vs Non-Edit Mode Select Individual Cell (Non-Edit Mode) Select Multiple Cells (Non-Edit Mode) Select Cell (Edit Mode) Select a Markdown Cell (Edit Mode) Hiding a Cell Hiding via Code Cells Running Cells Run a Single Cell Run Multiple Cells Rerunning a Notebook Clearing Cell Output Before Closing Summary and Demo A Light Introduction to Jupyter Notebook Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has four cell types, we use the following two for the OpenSciencelab: Markdown Cells Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double-click the cell. A markdown cell in edit mode If you wish to proceed through the notebook past the markdown cell or run a markdown cell's code to display its formatted contents, you can: - Click the play button at the top of the notebook - Use the shift + enter shortcut key. A run markdown cell NB : The cell will automatically move to the next cell if you are using the play button to run the cell. Code Cells Code cells contain editable and runnable Python code. You can run them in any order for any number of times. A code cell NB : While the ability to rerun the code cells in arbitrary order can be helpful, it can cause unexpected problems, such as: - Recycled variables may contain unexpected values if you run cells in non-sequential order. - Values from previous cells may trigger a different behavior when running the same cell. Detailed Instructions on Running Jupyter Notebook Now that users have a basic understandings of Jupyter notebook, users can use below manual as a refernce for detailed use. Selecting Cells Users may select cells individually or in a batch; users can then run the selected cells. Edit Mode vs Non-Edit Mode Before we discuss cell selection, it may be helpful to learn the difference between the edit mode and non-edit mode. Edit Mode : If you select a cell using edit mode, you may edit the code and Markdown written on that cell. You can choose the cell in the edit mode by clicking inside the cell box. Non-Edit Mode : If you select the cell outside of the code/Markdown box, you will be selecting the cell in a non-edit mode. While the difference is subtle, it is crucial to know the different modes because some hotkeys are unavailable in the edit mode. For instance, the shift + j command will allow you to select multiple cells in the non-edit mode, but this does not work in the edit mode. Select Individual Cell (Non-Edit Mode) Click on the left side of the cell. A selected cell displays a blue horizontal line on the left edge. Markdown cell will have an additional shaded area that is directly next to the cell. For a visual example, please refer to the hiding cells section. NB : It is crucial to avoid clicking the blue edge as well as the shaded area that is directly next to the cell. We will discuss this in a later section. Select Multiple Cells (Non-Edit Mode) Select a cell in non-edit mode Select multiple cells with: shift + j or shift + Down-Arrow to select additional cells below shift + k or shift + Up-Arrow to select additional cells above Perform batch operations on selected cells with play button or with ctrl + enter . Selected cells will have a blue background Select Cell (Edit Mode) Click inside a cell block. For live demonstration, please refer to the code cells section. NB : The edit mode cell will no longer display green edges. Select a Markdown Cell (Edit Mode) Double click inside a cell. For live demonstration, please refer to the Markdown cells section. NB : The edit mode cell will no longer display green edges. Hiding a Cell Sometimes, having too many cells may feel cumbersome. Below are the ways to hide the cells: Hiding Individual Cell You can hide an individual code cell by clicking the blue vertical line on the left of the code cell. Hiding Multiple Cells Alternatively, you may click the dark-shaded are on left of the Markdown cells to hide all proceeding cells. Running Cells Because you can run code cells in any order, each cell generates a number in the order they ran. Run a Single Cell With the Run Button Select any cell you wish to run. Do one of the following: Click Run button Ctrl + Enter to run a cell Shift + Enter to runs a cell and selects the cell below Alt + Enter to runs a cell and inserts an empty cell below Running selected cell multiple times with ctrl + enter Run Multiple Cells Instead of running just a single cell, you can run multiple cells at once in a following manner: Run every cell above/below selected cell. Run them in a groups of selected cell. Run the entire notebook Running every cell above/below: Select a cell, then: Select Run All Above Selected Cell from the Run menu or select Run Selected Cell and All Below from the Run menu Running all cells above selected. Note that the selected cell is ignored. Run a batch of selected cells Select a group of cells, then: - Use a hotkey - or click the run button Runs a group of cells in a batch using ctrl + enter Run an Entire Notebook If you wish to run the entire notebook from the get-go, you can do one of the following: Select _Run_ > _Run All Cells_ Select _Run_ > _Restart Kernel and Run All Cells..._ The difference is that the former option preserves the values from the previous run while the latter lets you run from a new state\u2014more on this in the next section. Rerunning a Notebook We recommend restarting the notebook kernel before rerunning it since any initialized variables and data structures from a previous run persist in memory along with their values, which can lead to unintended results. For instance, consider the following case : Assume you have a Python list with date-specific data, such as weather, stock prices, etc. If you were to run a cell that appends data from a specific date multiple times, it may yield unpredictable results due to the duplicate data. e.g. In the above example, running the second cell once will append a new date at the end of the list. However, running the same cell will keep appending the same value. Rerunning previous cells can cause unexpected behavior. We recommend restarting the notebook when you are running from the beginning. To restart the notebook, select any of the Restart options from the Kernel Menu. Clearing Cell Output Before Closing We recommend clearing every output from each code cell before closing or saving a notebook. Leaving the output in place can increase the file size of the notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#getting-started","text":"A Light Introduction to Jupyter Notebook Markdown Cells Code Cells Detailed Instructions on Running Jupyter Notebook Selecting Cells Edit Mode vs Non-Edit Mode Select Individual Cell (Non-Edit Mode) Select Multiple Cells (Non-Edit Mode) Select Cell (Edit Mode) Select a Markdown Cell (Edit Mode) Hiding a Cell Hiding via Code Cells Running Cells Run a Single Cell Run Multiple Cells Rerunning a Notebook Clearing Cell Output Before Closing Summary and Demo","title":"Getting Started"},{"location":"user-guides/how_to_run_a_notebook/#a-light-introduction-to-jupyter-notebook","text":"Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has four cell types, we use the following two for the OpenSciencelab:","title":"A Light Introduction to Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#markdown-cells","text":"Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double-click the cell. A markdown cell in edit mode If you wish to proceed through the notebook past the markdown cell or run a markdown cell's code to display its formatted contents, you can: - Click the play button at the top of the notebook - Use the shift + enter shortcut key. A run markdown cell NB : The cell will automatically move to the next cell if you are using the play button to run the cell.","title":"Markdown Cells"},{"location":"user-guides/how_to_run_a_notebook/#code-cells","text":"Code cells contain editable and runnable Python code. You can run them in any order for any number of times. A code cell NB : While the ability to rerun the code cells in arbitrary order can be helpful, it can cause unexpected problems, such as: - Recycled variables may contain unexpected values if you run cells in non-sequential order. - Values from previous cells may trigger a different behavior when running the same cell.","title":"Code Cells"},{"location":"user-guides/how_to_run_a_notebook/#detailed-instructions-on-running-jupyter-notebook","text":"Now that users have a basic understandings of Jupyter notebook, users can use below manual as a refernce for detailed use.","title":"Detailed Instructions on Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#selecting-cells","text":"Users may select cells individually or in a batch; users can then run the selected cells.","title":"Selecting Cells"},{"location":"user-guides/how_to_run_a_notebook/#edit-mode-vs-non-edit-mode","text":"Before we discuss cell selection, it may be helpful to learn the difference between the edit mode and non-edit mode. Edit Mode : If you select a cell using edit mode, you may edit the code and Markdown written on that cell. You can choose the cell in the edit mode by clicking inside the cell box. Non-Edit Mode : If you select the cell outside of the code/Markdown box, you will be selecting the cell in a non-edit mode. While the difference is subtle, it is crucial to know the different modes because some hotkeys are unavailable in the edit mode. For instance, the shift + j command will allow you to select multiple cells in the non-edit mode, but this does not work in the edit mode.","title":"Edit Mode vs Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-individual-cell-non-edit-mode","text":"Click on the left side of the cell. A selected cell displays a blue horizontal line on the left edge. Markdown cell will have an additional shaded area that is directly next to the cell. For a visual example, please refer to the hiding cells section. NB : It is crucial to avoid clicking the blue edge as well as the shaded area that is directly next to the cell. We will discuss this in a later section.","title":"Select Individual Cell (Non-Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-multiple-cells-non-edit-mode","text":"Select a cell in non-edit mode Select multiple cells with: shift + j or shift + Down-Arrow to select additional cells below shift + k or shift + Up-Arrow to select additional cells above Perform batch operations on selected cells with play button or with ctrl + enter . Selected cells will have a blue background","title":"Select Multiple Cells (Non-Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-cell-edit-mode","text":"Click inside a cell block. For live demonstration, please refer to the code cells section. NB : The edit mode cell will no longer display green edges.","title":"Select Cell (Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-a-markdown-cell-edit-mode","text":"Double click inside a cell. For live demonstration, please refer to the Markdown cells section. NB : The edit mode cell will no longer display green edges.","title":"Select a Markdown Cell (Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#hiding-a-cell","text":"Sometimes, having too many cells may feel cumbersome. Below are the ways to hide the cells:","title":"Hiding a Cell"},{"location":"user-guides/how_to_run_a_notebook/#hiding-individual-cell","text":"You can hide an individual code cell by clicking the blue vertical line on the left of the code cell.","title":"Hiding Individual Cell"},{"location":"user-guides/how_to_run_a_notebook/#hiding-multiple-cells","text":"Alternatively, you may click the dark-shaded are on left of the Markdown cells to hide all proceeding cells.","title":"Hiding Multiple Cells"},{"location":"user-guides/how_to_run_a_notebook/#running-cells","text":"Because you can run code cells in any order, each cell generates a number in the order they ran.","title":"Running Cells"},{"location":"user-guides/how_to_run_a_notebook/#run-a-single-cell","text":"","title":"Run a Single Cell"},{"location":"user-guides/how_to_run_a_notebook/#with-the-run-button","text":"Select any cell you wish to run. Do one of the following: Click Run button Ctrl + Enter to run a cell Shift + Enter to runs a cell and selects the cell below Alt + Enter to runs a cell and inserts an empty cell below Running selected cell multiple times with ctrl + enter","title":"With the Run Button"},{"location":"user-guides/how_to_run_a_notebook/#run-multiple-cells","text":"Instead of running just a single cell, you can run multiple cells at once in a following manner: Run every cell above/below selected cell. Run them in a groups of selected cell. Run the entire notebook","title":"Run Multiple Cells"},{"location":"user-guides/how_to_run_a_notebook/#running-every-cell-abovebelow","text":"Select a cell, then: Select Run All Above Selected Cell from the Run menu or select Run Selected Cell and All Below from the Run menu Running all cells above selected. Note that the selected cell is ignored.","title":"Running every cell above/below:"},{"location":"user-guides/how_to_run_a_notebook/#run-a-batch-of-selected-cells","text":"Select a group of cells, then: - Use a hotkey - or click the run button Runs a group of cells in a batch using ctrl + enter","title":"Run a batch of selected cells"},{"location":"user-guides/how_to_run_a_notebook/#run-an-entire-notebook","text":"If you wish to run the entire notebook from the get-go, you can do one of the following: Select _Run_ > _Run All Cells_ Select _Run_ > _Restart Kernel and Run All Cells..._ The difference is that the former option preserves the values from the previous run while the latter lets you run from a new state\u2014more on this in the next section.","title":"Run an Entire Notebook"},{"location":"user-guides/how_to_run_a_notebook/#rerunning-a-notebook","text":"We recommend restarting the notebook kernel before rerunning it since any initialized variables and data structures from a previous run persist in memory along with their values, which can lead to unintended results. For instance, consider the following case : Assume you have a Python list with date-specific data, such as weather, stock prices, etc. If you were to run a cell that appends data from a specific date multiple times, it may yield unpredictable results due to the duplicate data. e.g. In the above example, running the second cell once will append a new date at the end of the list. However, running the same cell will keep appending the same value. Rerunning previous cells can cause unexpected behavior. We recommend restarting the notebook when you are running from the beginning. To restart the notebook, select any of the Restart options from the Kernel Menu.","title":"Rerunning a Notebook"},{"location":"user-guides/how_to_run_a_notebook/#clearing-cell-output-before-closing","text":"We recommend clearing every output from each code cell before closing or saving a notebook. Leaving the output in place can increase the file size of the notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Clearing Cell Output Before Closing"},{"location":"user-guides/installing_software_in_OpenSARLab/","text":"Return to Table of Contents Installing Software in OpenScienceLab pip Install pip package inside conda environment mamba Install conda packages within a running notebook Install conda packages from the terminal apt and apt-get pip What is pip ? pip is a package installer for Python. You can install pip packages onto your account in the following manner: NB : Your installed pip packages are in the /home/jovyan/.local/lib/python3.x/site-packages directory. Open a terminal and use the following command: python -m pip install --user <package_name> Install pip package inside of a conda environment Open a terminal and use the following commands: conda activate <environment_name> python -m pip install --user <package_name> mamba Users can install additional software with mamba in OpenScienceLab. NB: OpenScienceLab began using mamba instead of conda to install conda packages in 2022. Since the syntax for mamba is identical to conda syntax, users who used conda previously should be familiar with the mamba workflow. Nonetheless, it is still worthwhile to reference conda documentation due to its similarity. Packages installed in the base conda environment will not stay after the server shuts down. You will need to reinstall it during subsequent OpenScienceLab sessions. However, changes to non-base will persist. Therefore, we recommend installing new packages in your non-base environments rather than in the base . Install conda packages within a running notebook Edit a notebook code cell Then use the following command: %mamba install <package_name> Run the code cell Install conda packages from the terminal Open a terminal and use following command: mamba activate <environment_name> mamba install <package_name> apt and apt-get At this time, users cannot install software in OpenScienceLab using apt or apt-get .","title":"Installing Software in OpenSARLab"},{"location":"user-guides/installing_software_in_OpenSARLab/#installing-software-in-opensciencelab","text":"pip Install pip package inside conda environment mamba Install conda packages within a running notebook Install conda packages from the terminal apt and apt-get","title":"Installing Software in OpenScienceLab"},{"location":"user-guides/installing_software_in_OpenSARLab/#pip","text":"What is pip ? pip is a package installer for Python. You can install pip packages onto your account in the following manner: NB : Your installed pip packages are in the /home/jovyan/.local/lib/python3.x/site-packages directory. Open a terminal and use the following command: python -m pip install --user <package_name>","title":"pip"},{"location":"user-guides/installing_software_in_OpenSARLab/#install-pip-package-inside-of-a-conda-environment","text":"Open a terminal and use the following commands: conda activate <environment_name> python -m pip install --user <package_name>","title":"Install pip package inside of a conda environment"},{"location":"user-guides/installing_software_in_OpenSARLab/#mamba","text":"Users can install additional software with mamba in OpenScienceLab. NB: OpenScienceLab began using mamba instead of conda to install conda packages in 2022. Since the syntax for mamba is identical to conda syntax, users who used conda previously should be familiar with the mamba workflow. Nonetheless, it is still worthwhile to reference conda documentation due to its similarity. Packages installed in the base conda environment will not stay after the server shuts down. You will need to reinstall it during subsequent OpenScienceLab sessions. However, changes to non-base will persist. Therefore, we recommend installing new packages in your non-base environments rather than in the base .","title":"mamba"},{"location":"user-guides/installing_software_in_OpenSARLab/#install-conda-packages-within-a-running-notebook","text":"Edit a notebook code cell Then use the following command: %mamba install <package_name> Run the code cell","title":"Install conda packages within a running notebook"},{"location":"user-guides/installing_software_in_OpenSARLab/#install-conda-packages-from-the-terminal","text":"Open a terminal and use following command: mamba activate <environment_name> mamba install <package_name>","title":"Install conda packages from the terminal"},{"location":"user-guides/installing_software_in_OpenSARLab/#apt-and-apt-get","text":"At this time, users cannot install software in OpenScienceLab using apt or apt-get .","title":"apt and apt-get"},{"location":"user-guides/jupyter_magic/","text":"Return to Table of Contents Jupyter Line and Cell Magics, and IPython Syntax In addition to running Python code, Jupyter Notebooks allows users to run magic commands with various functionality. While all magic commands are available to users, the below magic commands are used the most in the OpenScienceLab: Shell Assignment Syntax Line Magics Cell Magics Shell Assignment Syntax In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example: Line Magics Line magics start with a single % sign and affect only the line where % is used. The following are the most frequently used line magics: %matplotlib inline Allows non-interactive matplotlib plots to be displayed in a notebook. Example : %matplotlib widget Allows interactive matplotlib plots to be displayed and interacted with inside a Jupyter Notebook. Example : %df This is a custom magic written specifically for OpenScienceLab. It uses the Python function shutil.disk_usage() to check the storage state of the user's volumes. You may use the below flags to output results in a different manner: %df - Returns a human-readable string in GB. %df --raw - Returns a raw data object. %df --on - Returns a string in GB after running every subsequent code cell. %df --off - Turns off the %df --on option. %df -v - Prints additional debugging text. Example : Legacy Note Due to the new url_widget package, the user no longer needs to use the %matplotlib notebook and use the %matplotlib widget instead. Cell Magics Cell magics start with %% and affect the contents of an entire cell. %%javascript (or %%js ) Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell. Example : %%capture Runs the cell but captures all output. We typically use this to suppress the output of a %matplotlib plot that the user does not wish to see.","title":"Jupyter Magic Commands"},{"location":"user-guides/jupyter_magic/#jupyter-line-and-cell-magics-and-ipython-syntax","text":"In addition to running Python code, Jupyter Notebooks allows users to run magic commands with various functionality. While all magic commands are available to users, the below magic commands are used the most in the OpenScienceLab: Shell Assignment Syntax Line Magics Cell Magics","title":"Jupyter Line and Cell Magics, and IPython Syntax"},{"location":"user-guides/jupyter_magic/#shell-assignment-syntax","text":"In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example:","title":"Shell Assignment Syntax"},{"location":"user-guides/jupyter_magic/#line-magics","text":"Line magics start with a single % sign and affect only the line where % is used. The following are the most frequently used line magics:","title":"Line Magics"},{"location":"user-guides/jupyter_magic/#matplotlib-inline","text":"Allows non-interactive matplotlib plots to be displayed in a notebook. Example :","title":"%matplotlib inline"},{"location":"user-guides/jupyter_magic/#matplotlib-widget","text":"Allows interactive matplotlib plots to be displayed and interacted with inside a Jupyter Notebook. Example :","title":"%matplotlib widget"},{"location":"user-guides/jupyter_magic/#df","text":"This is a custom magic written specifically for OpenScienceLab. It uses the Python function shutil.disk_usage() to check the storage state of the user's volumes. You may use the below flags to output results in a different manner: %df - Returns a human-readable string in GB. %df --raw - Returns a raw data object. %df --on - Returns a string in GB after running every subsequent code cell. %df --off - Turns off the %df --on option. %df -v - Prints additional debugging text. Example :","title":"%df"},{"location":"user-guides/jupyter_magic/#legacy-note","text":"Due to the new url_widget package, the user no longer needs to use the %matplotlib notebook and use the %matplotlib widget instead.","title":"Legacy Note"},{"location":"user-guides/jupyter_magic/#cell-magics","text":"Cell magics start with %% and affect the contents of an entire cell.","title":"Cell Magics"},{"location":"user-guides/jupyter_magic/#javascript-or-js","text":"Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell. Example :","title":"%%javascript (or %%js)"},{"location":"user-guides/jupyter_magic/#capture","text":"Runs the cell but captures all output. We typically use this to suppress the output of a %matplotlib plot that the user does not wish to see.","title":"%%capture"},{"location":"user-guides/jupyter_notebook_extensions/","text":"Return to Table of Contents Managing Jupyter Lab Extensions As an OpenSARLab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARLab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARLab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu Managing Jupyter Notebook Extensions As an OpenSARLab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here . Enabling and Disabling Extensions The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-lab-extensions","text":"As an OpenSARLab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARLab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARLab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu","title":"Managing Jupyter Lab Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-notebook-extensions","text":"As an OpenSARLab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here .","title":"Managing Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#enabling-and-disabling-extensions","text":"The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Enabling and Disabling Extensions"},{"location":"user-guides/logging_out_and_server_shutdown/","text":"Return to Table of Contents Logging Out of OpenScienceLab and Shutting Down the Server When you are ready to stop working in OpenScienceLab, please shut down your server and logout. Shutting down your server is the same as shutting down your computer. If you shut down your server and come back the next day (or even the next week), all your files will still be there and you can resume processing your data. Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs. Why Shut Down the Server? Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. Additionally, while OpenScienceLab is and will remain free to our users, resources used do incur costs, which are paid for by the Alaska Satellite Facility. Help us keep OpenScienceLab free by shutting down servers when they are not in use. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before you leave. How to Shut Down The Server and Logout in Jupyter Lab Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button. How to Shut Down The Server and Logout in Jupyter Notebook Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"Logging Out and Server Shutdown"},{"location":"user-guides/logging_out_and_server_shutdown/#logging-out-of-opensciencelab-and-shutting-down-the-server","text":"When you are ready to stop working in OpenScienceLab, please shut down your server and logout. Shutting down your server is the same as shutting down your computer. If you shut down your server and come back the next day (or even the next week), all your files will still be there and you can resume processing your data. Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs.","title":"Logging Out of OpenScienceLab and Shutting Down the Server"},{"location":"user-guides/logging_out_and_server_shutdown/#why-shut-down-the-server","text":"Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. Additionally, while OpenScienceLab is and will remain free to our users, resources used do incur costs, which are paid for by the Alaska Satellite Facility. Help us keep OpenScienceLab free by shutting down servers when they are not in use. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before you leave.","title":"Why Shut Down the Server?"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-lab","text":"Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Lab"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-notebook","text":"Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Notebook"},{"location":"user-guides/mfa/","text":"Return to Table of Contents Configuring Multi-Factor Authentication Watch: Log In and Set Up MFA Before You Begin Setup Steps Troubleshooting Multi-Factor Authentication (MFA) is required to access OpenScienceLab resources. Currently, we support TOTP-based authentication, with plans to add hardware key (ex. Yubikey) authentication in future updates. Watch: Log In and Set Up MFA Before You Begin A TOTP-enabled MFA application is required to use OpenScienceLab resources. While the most widely utilized approach is through a smartphone application, there are also several desktop clients that provide this functionality. Application Desktop Support Android Support iOS Support KeePassXc \u2705 \u274c \u274c Enpass \u2705 \u2705 \u2705 Cisco Duo \u2705 \u2705 \u2705 Authenticator.cc* \u2705 \u2705 \u2705 Bitwarden** \u2705 \u2705 \u2705 FreeOTP \u274c \u2705 \u2705 Google Authenticator \u274c \u2705 \u2705 Microsoft Authenticator \u274c \u2705 \u2705 Aegis Authenticator \u274c \u2705 \u274c * Browser based ** TOTP supported in paid version only Setup Steps Log into OpenScienceLab normally. Leave the \"MFA\" field on the login page blank. Click \"Configure New MFA Device\" in the Navigation Bar at the top of the page. Configure your MFA device: If using a smartphone application, scan the QR code using the application's \"Scan QR Code\" feature. If using a desktop application, copy the OTP Secret into the \"TOTP\" field Enter the current MFA code generated by your application into the \"Code 1\" field on the \"Configure New MFA Device\" page. When the code changes, enter the new code into the \"Code 2\" field on the same page and click the \"Check MFA Codes\" button. If the check is successfull, navigate back to the home page by clicking \"Home\" in the Navigation Bar at the top of the page and continue using OpenScienceLab as normal. Troubleshooting If the MFA Code check is not successful, there are two potential issues: One of the codes was mis-typed. The secret was not properly put into the MFA application. Test again with another two consecutive codes, and if the verification step fails again, check that the OTP Secret on the page matches the secret in your application. If all else fails, refresh the page. This will generate a new code, which you can then use to follow the same steps above. For additional issues and further troubleshooting, please email uso@asf.alaska.edu","title":"Configuring Multi-Factor Authentication"},{"location":"user-guides/mfa/#configuring-multi-factor-authentication","text":"Watch: Log In and Set Up MFA Before You Begin Setup Steps Troubleshooting Multi-Factor Authentication (MFA) is required to access OpenScienceLab resources. Currently, we support TOTP-based authentication, with plans to add hardware key (ex. Yubikey) authentication in future updates.","title":"Configuring Multi-Factor Authentication"},{"location":"user-guides/mfa/#watch-log-in-and-set-up-mfa","text":"","title":"Watch: Log In and Set Up MFA"},{"location":"user-guides/mfa/#before-you-begin","text":"A TOTP-enabled MFA application is required to use OpenScienceLab resources. While the most widely utilized approach is through a smartphone application, there are also several desktop clients that provide this functionality. Application Desktop Support Android Support iOS Support KeePassXc \u2705 \u274c \u274c Enpass \u2705 \u2705 \u2705 Cisco Duo \u2705 \u2705 \u2705 Authenticator.cc* \u2705 \u2705 \u2705 Bitwarden** \u2705 \u2705 \u2705 FreeOTP \u274c \u2705 \u2705 Google Authenticator \u274c \u2705 \u2705 Microsoft Authenticator \u274c \u2705 \u2705 Aegis Authenticator \u274c \u2705 \u274c * Browser based ** TOTP supported in paid version only","title":"Before You Begin"},{"location":"user-guides/mfa/#setup-steps","text":"Log into OpenScienceLab normally. Leave the \"MFA\" field on the login page blank. Click \"Configure New MFA Device\" in the Navigation Bar at the top of the page. Configure your MFA device: If using a smartphone application, scan the QR code using the application's \"Scan QR Code\" feature. If using a desktop application, copy the OTP Secret into the \"TOTP\" field Enter the current MFA code generated by your application into the \"Code 1\" field on the \"Configure New MFA Device\" page. When the code changes, enter the new code into the \"Code 2\" field on the same page and click the \"Check MFA Codes\" button. If the check is successfull, navigate back to the home page by clicking \"Home\" in the Navigation Bar at the top of the page and continue using OpenScienceLab as normal.","title":"Setup Steps"},{"location":"user-guides/mfa/#troubleshooting","text":"If the MFA Code check is not successful, there are two potential issues: One of the codes was mis-typed. The secret was not properly put into the MFA application. Test again with another two consecutive codes, and if the verification step fails again, check that the OTP Secret on the page matches the secret in your application. If all else fails, refresh the page. This will generate a new code, which you can then use to follow the same steps above. For additional issues and further troubleshooting, please email uso@asf.alaska.edu","title":"Troubleshooting"},{"location":"user-guides/restarting_server_and_kernel/","text":"Return to Table of Contents Restarting the OpenSARLab Server and Notebook Kernel Restarting the OpenSARLab Server Overview Restarting the server triggers the nbgitpuller . Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. There is a notebook update that you wish to pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . NB : If you are comfortable with git , you could do a git pull from the terminal or in a notebook instead. However, do not push your changes as it may interfere with the nbgitpuller Steps to Restart the Server Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen. Click the Stop My Server Button Click the Start My Server Button, which may take a few seconds to appear. Click The Launch Server Button Select a Server Profile and Click the Start button Wait for the server to start; this may take some time. Optional: Click The Event Log arrow for detailed startup status information. You may use this information to send a report to the admin if your server does not start. Changing a Notebook Kernel Overview Notebooks in OpenScienceLab run in a variety of conda environments. Since each kernel has different packages, you must pick the right one to confirm it has the required packages. How to Switch Kernel Click the upper right corner. It should have the name of the kernel that you are currently using. A window with a list of built kernel will pop up. Select whichever kernel you need. NB : To build a kernel , refer to the conda environments section. Restarting a Jupyter Notebook Kernel Overview Variables and their assigned values are stored in memory as you run code cells in a notebook. As a result, rerunning a previously ran notebook without restarting the kernel may trigger some issues. Uncleared variables are problematic for various reasons, such as: Increase the file size of the notebook. They occupy limited memory resources. *Previously defined values may cause unexpected results during the rerun (). NB_: _Refer to the Rerunning a Notebook section in the How to Run a Jupyter Notebook * section. The solution for this is to restart the kernel to clear notebook data. How to Clear Notebook Select one of the following from the Kernel Menu: Restart Kernel : Restarts the kernel but leave the old code cell output in place. Restart Kernel and Clear All Outputs... : Restarts the kernel and removes old code cell output. This is generally the preferred option. Restart Kernel and Run up to Selected Cell... : Restarts the kernel and run up to the cell where you selected. Restart Kernel and Run All Cells... : Restarts the kernel and runs all the code cells. This only works if the notebook does not require input from the user. For most use cases, select Restart Kernel and Clear All Outputs...","title":"OpenSARLab Servers and Kernels"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server-and-notebook-kernel","text":"","title":"Restarting the OpenSARLab Server and Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server","text":"","title":"Restarting the OpenSARLab Server"},{"location":"user-guides/restarting_server_and_kernel/#overview","text":"Restarting the server triggers the nbgitpuller . Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. There is a notebook update that you wish to pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . NB : If you are comfortable with git , you could do a git pull from the terminal or in a notebook instead. However, do not push your changes as it may interfere with the nbgitpuller","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#steps-to-restart-the-server","text":"Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen. Click the Stop My Server Button Click the Start My Server Button, which may take a few seconds to appear. Click The Launch Server Button Select a Server Profile and Click the Start button Wait for the server to start; this may take some time. Optional: Click The Event Log arrow for detailed startup status information. You may use this information to send a report to the admin if your server does not start.","title":"Steps to Restart the Server"},{"location":"user-guides/restarting_server_and_kernel/#changing-a-notebook-kernel","text":"","title":"Changing a Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#overview_1","text":"Notebooks in OpenScienceLab run in a variety of conda environments. Since each kernel has different packages, you must pick the right one to confirm it has the required packages.","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#how-to-switch-kernel","text":"Click the upper right corner. It should have the name of the kernel that you are currently using. A window with a list of built kernel will pop up. Select whichever kernel you need. NB : To build a kernel , refer to the conda environments section.","title":"How to Switch Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-a-jupyter-notebook-kernel","text":"","title":"Restarting a Jupyter Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#overview_2","text":"Variables and their assigned values are stored in memory as you run code cells in a notebook. As a result, rerunning a previously ran notebook without restarting the kernel may trigger some issues. Uncleared variables are problematic for various reasons, such as: Increase the file size of the notebook. They occupy limited memory resources. *Previously defined values may cause unexpected results during the rerun (). NB_: _Refer to the Rerunning a Notebook section in the How to Run a Jupyter Notebook * section. The solution for this is to restart the kernel to clear notebook data.","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#how-to-clear-notebook","text":"Select one of the following from the Kernel Menu: Restart Kernel : Restarts the kernel but leave the old code cell output in place. Restart Kernel and Clear All Outputs... : Restarts the kernel and removes old code cell output. This is generally the preferred option. Restart Kernel and Run up to Selected Cell... : Restarts the kernel and run up to the cell where you selected. Restart Kernel and Run All Cells... : Restarts the kernel and runs all the code cells. This only works if the notebook does not require input from the user. For most use cases, select Restart Kernel and Clear All Outputs...","title":"How to Clear Notebook"},{"location":"user-guides/s3_buckets/","text":"Return to Table of Contents Accessing S3 Buckets The commands below can be run from a notebook code cell by prepending an ! or directly from a terminal. For those that are familiar with CLI Those that are comfortable using command line tools, such as Bash, may find that the AWS syntax is not intuitive. e.g. Recursive flag: Bash AWS -r --recursive Key Point : Please remember that AWS has a cumbersome syntax that may differ from other command lines. Generally speaking, the S3 command should look something like the following: aws s3 [optional_flags] <source_path> <destination_Path> Accessing Public S3 Buckets When accessing a public bucket from OpenScienceLab, be sure to include the following flags: - --no-sign-request - --region=<bucket's region> NB: The S3 buckets can have different sets of permissions. The commands below assume public access to the list, read, and write. List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename *Upload a file to a bucket aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename *Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user with the following steps: Open a terminal Run the following command: aws configure set default.s3.multipart_threshold 5000MB Accessing Private S3 Buckets Configure the AWS Client in your OpenScienceLab account (Prerequisite) To configure the AWS Client, you will need: - An AWS Access Key ID to the account holding the bucket - An AWS Secret Access Key to the account holding the bucket - An arn to an AWS IAM role with permission to access the bucket Steps on configuring a terminal : Open a terminal and run: aws configure It will ask you for the following: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty Manually edit the config file to add profile using *Vim. Use vim ~/.aws/config to open the config file. Add the following: [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit Vim *NB: Vim is a command line text editor. Since it does have a learning curve, users that never used Vim are encouraged to reference the Vim Command Cheat Sheet . For Private S3 Bucket List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"S3 Bucket Access in OpenSARLab"},{"location":"user-guides/s3_buckets/#accessing-s3-buckets","text":"The commands below can be run from a notebook code cell by prepending an ! or directly from a terminal.","title":"Accessing S3 Buckets"},{"location":"user-guides/s3_buckets/#for-those-that-are-familiar-with-cli","text":"Those that are comfortable using command line tools, such as Bash, may find that the AWS syntax is not intuitive. e.g. Recursive flag: Bash AWS -r --recursive Key Point : Please remember that AWS has a cumbersome syntax that may differ from other command lines. Generally speaking, the S3 command should look something like the following: aws s3 [optional_flags] <source_path> <destination_Path>","title":"For those that are familiar with CLI"},{"location":"user-guides/s3_buckets/#accessing-public-s3-buckets","text":"When accessing a public bucket from OpenScienceLab, be sure to include the following flags: - --no-sign-request - --region=<bucket's region> NB: The S3 buckets can have different sets of permissions. The commands below assume public access to the list, read, and write. List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename *Upload a file to a bucket aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename *Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user with the following steps: Open a terminal Run the following command: aws configure set default.s3.multipart_threshold 5000MB","title":"Accessing Public S3 Buckets"},{"location":"user-guides/s3_buckets/#accessing-private-s3-buckets","text":"","title":"Accessing Private S3 Buckets"},{"location":"user-guides/s3_buckets/#configure-the-aws-client-in-your-opensciencelab-account","text":"(Prerequisite) To configure the AWS Client, you will need: - An AWS Access Key ID to the account holding the bucket - An AWS Secret Access Key to the account holding the bucket - An arn to an AWS IAM role with permission to access the bucket Steps on configuring a terminal : Open a terminal and run: aws configure It will ask you for the following: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty Manually edit the config file to add profile using *Vim. Use vim ~/.aws/config to open the config file. Add the following: [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit Vim *NB: Vim is a command line text editor. Since it does have a learning curve, users that never used Vim are encouraged to reference the Vim Command Cheat Sheet .","title":"Configure the AWS Client in your OpenScienceLab account"},{"location":"user-guides/s3_buckets/#for-private-s3-bucket","text":"List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"For Private S3 Bucket"},{"location":"user-guides/troubleshooting_guide/","text":"Return to Table of Contents Troubleshooting Guide Why don't any of the deployments appear on the OpenScienceLab home page? You most likely have not yet configured Multi-Factor Authentication (MFA). While any user can log in without MFA, until a user has configured their MFA device, they will be unable to access any OpenScienceLab resources. See Configuring Multi-Factor Authentication for more information and a detailed walkthrough. Why did the kernel die while running a notebook? The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARLab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results. I successfully ran a notebook earlier on the same data but now it is killing the kernel. OpenSARLab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARLab user environment can be found here . When I open a notebook, I receive \"Kernel not found\" message. The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see. The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again. My notebook won't open, opens slowly, or won't save These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARLab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again. I tried to create a new notebook and recieved the error Forbidden This can happen when you have logged out of OpenSARLab and then try to create a new notebook from an OpenSARLab browser tab that was left open. You must log back in before you can create, open, or run a notebook. I am receiving a No space left on device error. OpenSARLab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need. My server won't start and I cannot access OpenSARlab. This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library . The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab. When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name. One of my notebooks looks like it has a mix of code from various versions of the notebook. We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I know there was an update made to an ASF notebook but I still have the old version. We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I am having trouble setting up a web server and developing my web app in OpenSARlab. This cannot be done in OpenSARlab. You will need to do this elsewhere. A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears. This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output. I tried to run a notebook that downloads products from HyP3 and I get an error HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb My issue is not on this list Please contact an OpenSARlab administrator for help.","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#troubleshooting-guide","text":"","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#why-dont-any-of-the-deployments-appear-on-the-opensciencelab-home-page","text":"You most likely have not yet configured Multi-Factor Authentication (MFA). While any user can log in without MFA, until a user has configured their MFA device, they will be unable to access any OpenScienceLab resources. See Configuring Multi-Factor Authentication for more information and a detailed walkthrough.","title":"Why don't any of the deployments appear on the OpenScienceLab home page?"},{"location":"user-guides/troubleshooting_guide/#why-did-the-kernel-die-while-running-a-notebook","text":"The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARLab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results.","title":"Why did the kernel die while running a notebook?"},{"location":"user-guides/troubleshooting_guide/#i-successfully-ran-a-notebook-earlier-on-the-same-data-but-now-it-is-killing-the-kernel","text":"OpenSARLab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARLab user environment can be found here .","title":"I successfully ran a notebook earlier on the same data but now it is killing the kernel."},{"location":"user-guides/troubleshooting_guide/#when-i-open-a-notebook-i-receive-kernel-not-found-message","text":"The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb","title":"When I open a notebook, I receive \"Kernel not found\" message."},{"location":"user-guides/troubleshooting_guide/#i-see-one-or-many-python-3-or-python-kernels-instead-of-kernels-named-after-conda-environments-that-i-expect-to-see","text":"The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again.","title":"I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see."},{"location":"user-guides/troubleshooting_guide/#my-notebook-wont-open-opens-slowly-or-wont-save","text":"These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARLab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again.","title":"My notebook won't open, opens slowly, or won't save"},{"location":"user-guides/troubleshooting_guide/#i-tried-to-create-a-new-notebook-and-recieved-the-error-forbidden","text":"This can happen when you have logged out of OpenSARLab and then try to create a new notebook from an OpenSARLab browser tab that was left open. You must log back in before you can create, open, or run a notebook.","title":"I tried to create a new notebook and recieved the error Forbidden"},{"location":"user-guides/troubleshooting_guide/#i-am-receiving-a-no-space-left-on-device-error","text":"OpenSARLab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need.","title":"I am receiving a No space left on device error."},{"location":"user-guides/troubleshooting_guide/#my-server-wont-start-and-i-cannot-access-opensarlab","text":"This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library .","title":"My server won't start and I cannot access OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#the-edits-i-made-to-an-asf-notebook-have-disappeared-since-the-last-time-i-used-opensarlab","text":"When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name.","title":"The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#one-of-my-notebooks-looks-like-it-has-a-mix-of-code-from-various-versions-of-the-notebook","text":"We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"One of my notebooks looks like it has a mix of code from various versions of the notebook."},{"location":"user-guides/troubleshooting_guide/#i-know-there-was-an-update-made-to-an-asf-notebook-but-i-still-have-the-old-version","text":"We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"I know there was an update made to an ASF notebook but I still have the old version."},{"location":"user-guides/troubleshooting_guide/#i-am-having-trouble-setting-up-a-web-server-and-developing-my-web-app-in-opensarlab","text":"This cannot be done in OpenSARlab. You will need to do this elsewhere.","title":"I am having trouble setting up a web server and developing my web app in OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#a-notebook-wont-load-a-new-browser-tab-opens-and-shows-the-jupyterhub-header-but-no-notebook-appears","text":"This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output.","title":"A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears."},{"location":"user-guides/troubleshooting_guide/#i-tried-to-run-a-notebook-that-downloads-products-from-hyp3-and-i-get-an-error","text":"HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb","title":"I tried to run a notebook that downloads products from HyP3 and I get an error"},{"location":"user-guides/troubleshooting_guide/#my-issue-is-not-on-this-list","text":"Please contact an OpenSARlab administrator for help.","title":"My issue is not on this list"}]}