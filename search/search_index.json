{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenSARlab What is OpenSARlab? OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment. Sign in or create an account to try OpenSARLab How will OpenSARlab benefit my work as a SAR scientist? OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis How will OpenSARlab benefit the class or training I am planning? OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers. How is OpenSARlab different from Binder? Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity) How to Access OpenSARlab As a Paid Service Managed by Alaska Satellite Facility Enterprise Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARlab deployment to suit your needs. Deploy OpenSARlab to Your Own AWS Account (Coming Soon) Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS. Contact Us Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Home"},{"location":"#welcome-to-opensarlab","text":"","title":"Welcome to OpenSARlab"},{"location":"#what-is-opensarlab","text":"OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment. Sign in or create an account to try OpenSARLab","title":"What is OpenSARlab?"},{"location":"#how-will-opensarlab-benefit-my-work-as-a-sar-scientist","text":"OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis","title":"How will OpenSARlab benefit my work as a SAR scientist?"},{"location":"#how-will-opensarlab-benefit-the-class-or-training-i-am-planning","text":"OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers.","title":"How will OpenSARlab benefit the class or training I am planning?"},{"location":"#how-is-opensarlab-different-from-binder","text":"Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity)","title":"How is OpenSARlab different from Binder?"},{"location":"#how-to-access-opensarlab","text":"","title":"How to Access OpenSARlab"},{"location":"#as-a-paid-service-managed-by-alaska-satellite-facility-enterprise","text":"Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARlab deployment to suit your needs.","title":"As a Paid Service Managed by Alaska Satellite Facility Enterprise"},{"location":"#deploy-opensarlab-to-your-own-aws-account-coming-soon","text":"Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS.","title":"Deploy OpenSARlab to Your Own AWS Account (Coming Soon)"},{"location":"#contact-us","text":"Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Contact Us"},{"location":"dev/","text":"System Diagram Deploy OpenSARlab to AWS Conda Environment Options OpenSARlab Notifications Troubleshooting Custom Mintpy Conda Build Instructions","title":"Dev"},{"location":"release_notes/","text":"June 2021 October 2021 February 2022 February 2023 February 2024","title":"Release notes"},{"location":"user/","text":"Welcome to the OpenSARlab User Guide Configuring Multi-Factor Authentication Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"user/#welcome-to-the-opensarlab-user-guide","text":"Configuring Multi-Factor Authentication Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"dev-guides/conda_environments/","text":"Return to Developer Guide There are a few options for creating conda environments in OpenSARlab. Each option come with benefits and drawbacks. Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks Drawbacks Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local , Register Their Kernels, and Run Any Setup Scripts Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts Drawbacks Increases the time it takes to start an OpenSARlab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage. Leave Conda Environment Creation up to the Users Benefits Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts. Drawbacks Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Conda Environment Options"},{"location":"dev-guides/conda_environments/#there-are-a-few-options-for-creating-conda-environments-in-opensarlab","text":"Each option come with benefits and drawbacks.","title":"There are a few options for creating conda environments in OpenSARlab."},{"location":"dev-guides/conda_environments/#create-conda-environments-register-their-kernels-and-run-any-setup-scripts-in-the-docker-images","text":"","title":"Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s"},{"location":"dev-guides/conda_environments/#benefits","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks","text":"Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used.","title":"Drawbacks"},{"location":"dev-guides/conda_environments/#create-conda-environments-in-the-docker-images-then-in-the-hook-script-sync-them-to-homelocal-register-their-kernels-and-run-any-setup-scripts","text":"","title":"Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local, Register Their Kernels, and Run Any Setup Scripts"},{"location":"dev-guides/conda_environments/#benefits_1","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks_1","text":"Increases the time it takes to start an OpenSARlab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage.","title":"Drawbacks"},{"location":"dev-guides/conda_environments/#leave-conda-environment-creation-up-to-the-users","text":"","title":"Leave Conda Environment Creation up to the Users"},{"location":"dev-guides/conda_environments/#benefits_2","text":"Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts.","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks_2","text":"Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Drawbacks"},{"location":"dev-guides/deploy_OpenSARlab/","text":"Return to Developer Guide Deploy OpenSARlab to an AWS account A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" deployment_name \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention. Take AWS SES out of sandbox The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox. Create an AWS Cost Allocation Tag Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\" Add dockerhub credentials to AWS Secrets Manager This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button Setup an iCal calendar for notifications Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section Store your CA certificate OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager Prepare CodeCommit Repos TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a deployment_name -container CodeCommit repo in your AWS account Create a deployment_name -cluster CodeCommit repo Clone the deployment_name -container and deployment_name -cluster repos to your local computer using ssh cd into your local deployment_name -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local deployment_name -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local deployment_name -container repo git pull github main Create a main branch in the deployment_name -container repo git checkout -b main Push to the remote deployment_name -container repo git push origin main cd into your local deployment_name -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local deployment_name -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local deployment_name -cluster repo git pull github main Create a main branch in the deployment_name -cluster repo git checkout -b main Push to the remote deployment_name -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab Customize opensarlab_container code for deployment The opensarlab-container repo contains one example image named helloworld , which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name The image name must be alpha-numeric with no whitespaces or special characters Edit the dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory under FROM release as testing Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Add, commit, and push changes to the remote CodeCommit repo Customize opensarlab_cluster code for deployment Create and add any additional custom jupyter magic commands to the opensarlab/jupyterhub/singleuser/custom_magics directory Add any additional scripts you may have created for use in your image to the opensarlab/jupyterhub/singleuser/hooks directory Duplicate opensarlab/jupyterhub/singleuser/hooks/sar.sh , renaming it after your image name Edit opensarlab/jupyterhub/singleuser/hooks/<image_name>.sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Rename opensarlab/opensarlab.example.yaml to opensarlab/opensarlab.yaml Use the example notes in opensarlab/opensarlab.yaml to define the required and optional fields Update opensarlab/jupyterhub/helm_config.yaml singleuser Add any needed extraFiles hub Add any needed extraFiles Add, commit, and push changes to the remote CodeCommit repo Build the container CloudFormation stack This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the deployment_name -container repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment CodeCommitSourceRepo The CodeCommit repo holding the container code ( deployment_name -container) CodeCommitSourceBranch The name of the production branch of the deployment_name -container CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again Build the cluster CloudFormation stack This CloudFormation stack dynamically creates 3 additional stacks. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select opensarlab/pipeline/cf-pipeline.yaml from your local branch of the cluster repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment. Do not use a stack name that ends in cluster , jupyterhub , or cognito . These are reserved. CodeCommitRepoName The CodeCommit repo holding the container code ( deployment_name -cluster) CodeCommitBranchName The name of the production branch of the deployment_name -cluster CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button Take care of odds and ends Update deployment_url in the cluster repo opensarlab/opensarlab.yaml if you started off using load balancer Don't forget to update your DNS record Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on cluster stack Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: deployment_name Prime the Auto Scaling Group for each profile unless there are active users Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select an autoscaling group Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta\\> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message\\> 1. Your notification message 1. Sign up with your `admin_user_name` account, sign in, and add groups for each profile and sudo 1. Open the `deployment_url` in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the `admin_user_name` parameter of the `opensarlab.yaml` 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the `deployment_name`-auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the `deployment_name`-cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the `deployment_name`-cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam:: :user/ \" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS profile on your local machine 1. Example profile: 1. yaml [profile profile_name] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam:: :role/ - -cluster-user-access cluster_name = -cluster ``` 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl","title":"Deploy OpenSARlab to AWS"},{"location":"dev-guides/deploy_OpenSARlab/#deploy-opensarlab-to-an-aws-account","text":"A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" deployment_name \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention.","title":"Deploy OpenSARlab to an AWS account"},{"location":"dev-guides/deploy_OpenSARlab/#take-aws-ses-out-of-sandbox","text":"The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox.","title":"Take AWS SES out of sandbox"},{"location":"dev-guides/deploy_OpenSARlab/#create-an-aws-cost-allocation-tag","text":"Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\"","title":"Create an AWS Cost Allocation Tag"},{"location":"dev-guides/deploy_OpenSARlab/#add-dockerhub-credentials-to-aws-secrets-manager","text":"This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button","title":"Add dockerhub credentials to AWS Secrets Manager"},{"location":"dev-guides/deploy_OpenSARlab/#setup-an-ical-calendar-for-notifications","text":"Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section","title":"Setup an iCal calendar for notifications"},{"location":"dev-guides/deploy_OpenSARlab/#store-your-ca-certificate","text":"OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager","title":"Store your CA certificate"},{"location":"dev-guides/deploy_OpenSARlab/#prepare-codecommit-repos","text":"TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a deployment_name -container CodeCommit repo in your AWS account Create a deployment_name -cluster CodeCommit repo Clone the deployment_name -container and deployment_name -cluster repos to your local computer using ssh cd into your local deployment_name -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local deployment_name -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local deployment_name -container repo git pull github main Create a main branch in the deployment_name -container repo git checkout -b main Push to the remote deployment_name -container repo git push origin main cd into your local deployment_name -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local deployment_name -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local deployment_name -cluster repo git pull github main Create a main branch in the deployment_name -cluster repo git checkout -b main Push to the remote deployment_name -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab","title":"Prepare CodeCommit Repos"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_container-code-for-deployment","text":"The opensarlab-container repo contains one example image named helloworld , which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name The image name must be alpha-numeric with no whitespaces or special characters Edit the dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory under FROM release as testing Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_container code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_cluster-code-for-deployment","text":"Create and add any additional custom jupyter magic commands to the opensarlab/jupyterhub/singleuser/custom_magics directory Add any additional scripts you may have created for use in your image to the opensarlab/jupyterhub/singleuser/hooks directory Duplicate opensarlab/jupyterhub/singleuser/hooks/sar.sh , renaming it after your image name Edit opensarlab/jupyterhub/singleuser/hooks/<image_name>.sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Rename opensarlab/opensarlab.example.yaml to opensarlab/opensarlab.yaml Use the example notes in opensarlab/opensarlab.yaml to define the required and optional fields Update opensarlab/jupyterhub/helm_config.yaml singleuser Add any needed extraFiles hub Add any needed extraFiles Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_cluster code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-container-cloudformation-stack","text":"This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the deployment_name -container repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment CodeCommitSourceRepo The CodeCommit repo holding the container code ( deployment_name -container) CodeCommitSourceBranch The name of the production branch of the deployment_name -container CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again","title":"Build the container CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-cluster-cloudformation-stack","text":"This CloudFormation stack dynamically creates 3 additional stacks. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select opensarlab/pipeline/cf-pipeline.yaml from your local branch of the cluster repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment. Do not use a stack name that ends in cluster , jupyterhub , or cognito . These are reserved. CodeCommitRepoName The CodeCommit repo holding the container code ( deployment_name -cluster) CodeCommitBranchName The name of the production branch of the deployment_name -cluster CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button","title":"Build the cluster CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#take-care-of-odds-and-ends","text":"Update deployment_url in the cluster repo opensarlab/opensarlab.yaml if you started off using load balancer Don't forget to update your DNS record Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on cluster stack Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: deployment_name Prime the Auto Scaling Group for each profile unless there are active users Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select an autoscaling group Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta\\> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message\\> 1. Your notification message 1. Sign up with your `admin_user_name` account, sign in, and add groups for each profile and sudo 1. Open the `deployment_url` in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the `admin_user_name` parameter of the `opensarlab.yaml` 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the `deployment_name`-auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the `deployment_name`-cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the `deployment_name`-cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam:: :user/ \" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS profile on your local machine 1. Example profile: 1. yaml [profile profile_name] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam:: :role/ - -cluster-user-access cluster_name = -cluster ``` 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl","title":"Take care of odds and ends"},{"location":"dev-guides/destroy_deployment/","text":"Return to Developer Guide Destroy Deployments It is essential to destroy a deployment at the end of its life cycle so that no resources are left in place. With a proper destruction procedure, one can mitigate the accrued cost of AWS. WARNING: Before deleting the deployments, developers will need to account for the following: When deleting the CloudFormation stack, the deletion order matters. Delete some of the CloudFormation stacks before deleting ECR. The name of the items you're deleting may differ depending on the deployment you are taking down. For example, your deployment's CloudFormation stack may not have a region name. Do NOT take down the Cognito and CloudWatch logs. These are used for statistical analysis later on. Removing CloudFormation Stacks When you first go to CloudFormation, it should look something like the following: Because the order of removing deployment is essential, you will need to take down the CloudFormation stacks first. Deleting the following CloudFormation stacks will kill the deployment and remove its resources: <deployment_name>-container <deployment_name>-cluster deployment_name NB : Do not manually delete any S3 buckets after emptying them in the steps below. If you delete the buckets after emptying them, the CloudFormation stack deletions associated with those buckets will fail. To fix the issues regarding deleted buckets, you will have to recreate the empty buckets to proceed. Therefore, let CloudFormation delete the S3 buckets for you once you empty them. Follow the below instructions to properly delete these stacks. Steps to Delete CloudFormation Stacks 1. Prepare to delete the <deployment_name>-container CloudFormation stack NB: The <deployment_name>-container is independent of other stacks, i.e., the deletion order does not matter. Empty the codepipeline-<region>-<deployment_name>-container S3 bucket Navigate to the AWS S3 console Click the codepipeline-<region>-<deployment_name>-container S3 bucket option. Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button 2. Delete ECR repos Navigate to the AWS Elastic Container Registry Before deleting the ECR, you will need to empty them first. Click the repository name first and go to the individual ECR repository. Select all items by clicking the first box. Select delete and confirm the deletion. Go back to the ECR Registry and click the option next to the empty <deployment_name>/<profile_namespace> repository Click the Delete button Confirm the deletion by typing delete in the provided field Click the Delete button Repeat for each profiles 3. Delete the <deployment_name>-container CloudFormation stack Navigate to the AWS CloudFormation console Click the <deployment_name>-container stack option Click the Delete button Click the Delete Stack button Click the <deployment_name>-container stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically NB: Stack deletion process will be similar to above instructions for other stacks. Deleting the rest of the CloudFormation Stacks Unlike the container stack, the deletion order matters for the rest. On the bright side, the deletion process should be similar to the container stacks. Specifically, you will need to delete your stacks in the following order: <deployment_name>-jupyterhub stack <deployment_name>-cluster stack * <deployment_name>-cluster-pipeline stack NB:*These stacks have additional steps. WARNING: As mentioned earlier, do NOT delete the <deployment_name>-cognito stack. In above order, follow these steps (except for stack 3): Delete the <deployment_name>-<stack_name> CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the <deployment_name>-<stack_name> stack Click the Delete button 1. Click the Delete stack button Click the <deployment_name>-<stack_name> stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically If you are deleting stack 3 ( <deployment_name>-cluster-pipeline ), you will need to follow these steps first before deleting the stack: Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Delete <deployment_name>/hub and <deployment_name>/notifications repository. NB: Refer to the Delete ECR Repos section for how to delete ECR repos. Empty the codepipeline and lambda S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline-<region>-<deployment_name> S3 bucket Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button Repeat the same process for the <region>-<deployment_name>-lambda Delete EBS Snapshots and Volumes To mitigate the cost associated with storage space, it is crucial to deallocate unused resources. The below steps will guide you on how to do so. First, navigate to the AWS EC2 console - this step should be identical for both EBS snapshots and EBS volumes. Delete EBS snapshots Click the Snapshots link in the sidebar menu Filter by <cost allocation tag> : <deployment_name> Double check that you filtered for the correct deployment! Select all snapshots Select Delete from the Actions menu Confirm by clicking the Yes, delete button Delete EBS volumes Navigate to the AWS EC2 console Click the Volumes link in the sidebar menu Filter by osl-stackname: <deployment_name> Double check that you filtered for the correct deployment! Select all volumes Select Delete volumes from the Actions menu Confirm by clicking the Yes, delete button (Optional) Delete the CodeCommit repositories This section will guide you on how to remove the <deployment_name>-container and <deployment_name>-cluster repositories located in the CodeCommit. Important Note: Often, it would be in your best interest to preserve the CodeCommit repositories since the cost of maintaining them are minuscule. If you believe that you may re-deploy the same deployment, you may want to ease future work in one of the following manners: Leaving these repositories in place, i.e., don't delete them. Download the zip of your repositories, store them in S3, and then delete them. In another word, delete the CodeCommit repositories if and only if you are sure that you don't need them. First, navigate to the AWS CodeCommit console: Then delete the <deployment_name>-container and <deployment_name>-cluster in any order. The deletion process for these two repositories is following: Check the option next to the repository Click the Delete repository button Confirm the deletion by typing delete in the provided field Click the Delete button (Optional) Confirm that all resources have been deleted Once you've taken down the deployment, you may want to verify the resource usage. Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the Tag Editor link in the sidebar menu and fill in the following: Regions: <current-region> Tags: Key : Cost allocation tag Value : <deployment_name> Click the Search resources button Identify and delete any remaining resources Delete Calendar Now that you are done with taking down the deployment, you will need to delete the calendar notifications. Go to your Google Calendar Choose a deployment you wish to remove Open Settings and Sharing Click Delete under Remove calendar","title":"Destroy Deployments"},{"location":"dev-guides/destroy_deployment/#destroy-deployments","text":"It is essential to destroy a deployment at the end of its life cycle so that no resources are left in place. With a proper destruction procedure, one can mitigate the accrued cost of AWS.","title":"Destroy Deployments"},{"location":"dev-guides/destroy_deployment/#warning-before-deleting-the-deployments-developers-will-need-to-account-for-the-following","text":"When deleting the CloudFormation stack, the deletion order matters. Delete some of the CloudFormation stacks before deleting ECR. The name of the items you're deleting may differ depending on the deployment you are taking down. For example, your deployment's CloudFormation stack may not have a region name. Do NOT take down the Cognito and CloudWatch logs. These are used for statistical analysis later on.","title":"WARNING: Before deleting the deployments, developers will need to account for the following:"},{"location":"dev-guides/destroy_deployment/#removing-cloudformation-stacks","text":"When you first go to CloudFormation, it should look something like the following: Because the order of removing deployment is essential, you will need to take down the CloudFormation stacks first. Deleting the following CloudFormation stacks will kill the deployment and remove its resources: <deployment_name>-container <deployment_name>-cluster deployment_name NB : Do not manually delete any S3 buckets after emptying them in the steps below. If you delete the buckets after emptying them, the CloudFormation stack deletions associated with those buckets will fail. To fix the issues regarding deleted buckets, you will have to recreate the empty buckets to proceed. Therefore, let CloudFormation delete the S3 buckets for you once you empty them. Follow the below instructions to properly delete these stacks.","title":"Removing CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#steps-to-delete-cloudformation-stacks","text":"","title":"Steps to Delete CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#1-prepare-to-delete-the-deployment_name-container-cloudformation-stack","text":"NB: The <deployment_name>-container is independent of other stacks, i.e., the deletion order does not matter. Empty the codepipeline-<region>-<deployment_name>-container S3 bucket Navigate to the AWS S3 console Click the codepipeline-<region>-<deployment_name>-container S3 bucket option. Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button","title":"1. Prepare to delete the &lt;deployment_name&gt;-container CloudFormation stack"},{"location":"dev-guides/destroy_deployment/#2-delete-ecr-repos","text":"Navigate to the AWS Elastic Container Registry Before deleting the ECR, you will need to empty them first. Click the repository name first and go to the individual ECR repository. Select all items by clicking the first box. Select delete and confirm the deletion. Go back to the ECR Registry and click the option next to the empty <deployment_name>/<profile_namespace> repository Click the Delete button Confirm the deletion by typing delete in the provided field Click the Delete button Repeat for each profiles","title":"2. Delete ECR repos"},{"location":"dev-guides/destroy_deployment/#3-delete-the-deployment_name-container-cloudformation-stack","text":"Navigate to the AWS CloudFormation console Click the <deployment_name>-container stack option Click the Delete button Click the Delete Stack button Click the <deployment_name>-container stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically NB: Stack deletion process will be similar to above instructions for other stacks.","title":"3. Delete the &lt;deployment_name&gt;-container CloudFormation stack"},{"location":"dev-guides/destroy_deployment/#deleting-the-rest-of-the-cloudformation-stacks","text":"Unlike the container stack, the deletion order matters for the rest. On the bright side, the deletion process should be similar to the container stacks. Specifically, you will need to delete your stacks in the following order: <deployment_name>-jupyterhub stack <deployment_name>-cluster stack * <deployment_name>-cluster-pipeline stack NB:*These stacks have additional steps. WARNING: As mentioned earlier, do NOT delete the <deployment_name>-cognito stack. In above order, follow these steps (except for stack 3): Delete the <deployment_name>-<stack_name> CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the <deployment_name>-<stack_name> stack Click the Delete button 1. Click the Delete stack button Click the <deployment_name>-<stack_name> stack name Click the Events tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically If you are deleting stack 3 ( <deployment_name>-cluster-pipeline ), you will need to follow these steps first before deleting the stack: Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Delete <deployment_name>/hub and <deployment_name>/notifications repository. NB: Refer to the Delete ECR Repos section for how to delete ECR repos. Empty the codepipeline and lambda S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline-<region>-<deployment_name> S3 bucket Click the Empty button Confirm the deletion of bucket contents by typing permanently delete in the provided field Click the Empty button Repeat the same process for the <region>-<deployment_name>-lambda","title":"Deleting the rest of the CloudFormation Stacks"},{"location":"dev-guides/destroy_deployment/#delete-ebs-snapshots-and-volumes","text":"To mitigate the cost associated with storage space, it is crucial to deallocate unused resources. The below steps will guide you on how to do so. First, navigate to the AWS EC2 console - this step should be identical for both EBS snapshots and EBS volumes.","title":"Delete EBS Snapshots and Volumes"},{"location":"dev-guides/destroy_deployment/#delete-ebs-snapshots","text":"Click the Snapshots link in the sidebar menu Filter by <cost allocation tag> : <deployment_name> Double check that you filtered for the correct deployment! Select all snapshots Select Delete from the Actions menu Confirm by clicking the Yes, delete button","title":"Delete EBS snapshots"},{"location":"dev-guides/destroy_deployment/#delete-ebs-volumes","text":"Navigate to the AWS EC2 console Click the Volumes link in the sidebar menu Filter by osl-stackname: <deployment_name> Double check that you filtered for the correct deployment! Select all volumes Select Delete volumes from the Actions menu Confirm by clicking the Yes, delete button","title":"Delete EBS volumes"},{"location":"dev-guides/destroy_deployment/#optional-delete-the-codecommit-repositories","text":"This section will guide you on how to remove the <deployment_name>-container and <deployment_name>-cluster repositories located in the CodeCommit. Important Note: Often, it would be in your best interest to preserve the CodeCommit repositories since the cost of maintaining them are minuscule. If you believe that you may re-deploy the same deployment, you may want to ease future work in one of the following manners: Leaving these repositories in place, i.e., don't delete them. Download the zip of your repositories, store them in S3, and then delete them. In another word, delete the CodeCommit repositories if and only if you are sure that you don't need them. First, navigate to the AWS CodeCommit console: Then delete the <deployment_name>-container and <deployment_name>-cluster in any order. The deletion process for these two repositories is following: Check the option next to the repository Click the Delete repository button Confirm the deletion by typing delete in the provided field Click the Delete button","title":"(Optional) Delete the CodeCommit repositories"},{"location":"dev-guides/destroy_deployment/#optional-confirm-that-all-resources-have-been-deleted","text":"Once you've taken down the deployment, you may want to verify the resource usage. Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the Tag Editor link in the sidebar menu and fill in the following: Regions: <current-region> Tags: Key : Cost allocation tag Value : <deployment_name> Click the Search resources button Identify and delete any remaining resources","title":"(Optional) Confirm that all resources have been deleted"},{"location":"dev-guides/destroy_deployment/#delete-calendar","text":"Now that you are done with taking down the deployment, you will need to delete the calendar notifications. Go to your Google Calendar Choose a deployment you wish to remove Open Settings and Sharing Click Delete under Remove calendar","title":"Delete Calendar"},{"location":"dev-guides/mintpy_conda/","text":"Build latest mintpy and push to custom OpenSARlab conda channel Sometimes the latest version of MintPy is desired within a notebook but the official release is not current. These instructions will show how to build MintPy and push to a custom conda channel. This assumes that the user has an Anaconda.org account and that this account is attached to a opensarlab conda channel. Instructions on how to do this are not provided here. A. Create working directory mkdir conda-mintpy cd conda-mintpy B. Create conda environment conda create -n conda-build python=3.10 anaconda-client conda activate conda-build C. Clone conda feedstock of mintpy. https://github.com/conda-forge/mintpy-feedstock git clone git@github.com:conda-forge/mintpy-feedstock.git D. Clone mintpy code repo. https://github.com/insarlab/mintpy. Only the latest commit is included to speed up download. Drop the --depth 1 to get the whole history. git clone --depth 1 git@github.com:insarlab/MintPy.git E. Assuming mintpy main is used and is also the latest, we won't make any changes to mintpy The feedstock is a framework for building. It does not have the source code to build within it. Copy Mintpy source to feedstock recipes. cp -r MintPy mintpy-feedstock/recipe/MintPy F. Make versioning clear and build MintPy from local. The VERSION here is a working example. The actual version is subject to change. VERSION=1.3.3.dev.COMMIT_SHORT_HASH G. Edit mintpy-feedstock/recipe/meta.yaml -{% set version = \"1.3.2\" %} +{% set version = 1.3.3.dev.COMMIT_SHORT_HASH %} source: - url: https://github.com/insarlab/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz - sha256: 6c1242dee74f13b96aa4b1f8d50c45ec486397796e4bd4bf3f67849f921bfe7f + path: MintPy H. Build This may take a while. cd mintpy-feedstock python3.10 build-locally.py cd .. I. After a successful build, check for artifacts in mintpy-feedstock/build_artifacts/ J. Push changes to OpenSARlab conda channel. # Login to individual account where you are an owner of OpenSARlab org anaconda login # Upload artifacts anaconda upload --user opensarlab mintpy-feedstock/build_artifacts/noarch/mintpy-*.tar.bz2 K. Download and check version mamba install -c opensarlab -c conda-forge mintpy=VERSION conda list | grep -i mintpy L. Cleanup as desired cd ../ ls rm -rf conda-mintpy conda deactivate conda env remove -n conda-build conda env list","title":"Custom Mintpy Conda Build Instructions"},{"location":"dev-guides/mintpy_conda/#build-latest-mintpy-and-push-to-custom-opensarlab-conda-channel","text":"Sometimes the latest version of MintPy is desired within a notebook but the official release is not current. These instructions will show how to build MintPy and push to a custom conda channel. This assumes that the user has an Anaconda.org account and that this account is attached to a opensarlab conda channel. Instructions on how to do this are not provided here. A. Create working directory mkdir conda-mintpy cd conda-mintpy B. Create conda environment conda create -n conda-build python=3.10 anaconda-client conda activate conda-build C. Clone conda feedstock of mintpy. https://github.com/conda-forge/mintpy-feedstock git clone git@github.com:conda-forge/mintpy-feedstock.git D. Clone mintpy code repo. https://github.com/insarlab/mintpy. Only the latest commit is included to speed up download. Drop the --depth 1 to get the whole history. git clone --depth 1 git@github.com:insarlab/MintPy.git E. Assuming mintpy main is used and is also the latest, we won't make any changes to mintpy The feedstock is a framework for building. It does not have the source code to build within it. Copy Mintpy source to feedstock recipes. cp -r MintPy mintpy-feedstock/recipe/MintPy F. Make versioning clear and build MintPy from local. The VERSION here is a working example. The actual version is subject to change. VERSION=1.3.3.dev.COMMIT_SHORT_HASH G. Edit mintpy-feedstock/recipe/meta.yaml -{% set version = \"1.3.2\" %} +{% set version = 1.3.3.dev.COMMIT_SHORT_HASH %} source: - url: https://github.com/insarlab/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz - sha256: 6c1242dee74f13b96aa4b1f8d50c45ec486397796e4bd4bf3f67849f921bfe7f + path: MintPy H. Build This may take a while. cd mintpy-feedstock python3.10 build-locally.py cd .. I. After a successful build, check for artifacts in mintpy-feedstock/build_artifacts/ J. Push changes to OpenSARlab conda channel. # Login to individual account where you are an owner of OpenSARlab org anaconda login # Upload artifacts anaconda upload --user opensarlab mintpy-feedstock/build_artifacts/noarch/mintpy-*.tar.bz2 K. Download and check version mamba install -c opensarlab -c conda-forge mintpy=VERSION conda list | grep -i mintpy L. Cleanup as desired cd ../ ls rm -rf conda-mintpy conda deactivate conda env remove -n conda-build conda env list","title":"Build latest mintpy and push to custom OpenSARlab conda channel"},{"location":"dev-guides/notifications/","text":"Return to Developer Guide Create OpenSARlab Notifications Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"Notifications"},{"location":"dev-guides/notifications/#create-opensarlab-notifications","text":"Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"Create OpenSARlab Notifications"},{"location":"dev-guides/system_diagram/","text":"Return to Table of Contents OpenSARlab System Diagram","title":"System diagram"},{"location":"dev-guides/system_diagram/#opensarlab-system-diagram","text":"","title":"OpenSARlab System Diagram"},{"location":"dev-guides/troubleshooting/","text":"Return to Developer Guide A. Users 1. OSL Servers Time Out Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"Troubelshooting Guide"},{"location":"dev-guides/troubleshooting/#a-users","text":"","title":"A. Users"},{"location":"dev-guides/troubleshooting/#1-osl-servers-time-out","text":"Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"1. OSL Servers Time Out"},{"location":"release-notes/release_02-2022/","text":"Welcome to the February 2022 OpenSARlab Update! Changes: Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update Ubuntu 20.04.3 LTS JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04 JupyterLab There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th. Matplotlib widget matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook. Url-widget The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks. New Memory Monitor Location JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen. Notebook Debugger JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs Mamba The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba. Mamba Gator is installed mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu. Spellchecker The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen. Custom Extensions We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles. Recommended Jupyter Notebook Changes The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"February 2022"},{"location":"release-notes/release_02-2022/#welcome-to-the-february-2022-opensarlab-update","text":"","title":"Welcome to the February 2022 OpenSARlab Update!"},{"location":"release-notes/release_02-2022/#changes","text":"Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update","title":"Changes:"},{"location":"release-notes/release_02-2022/#ubuntu-20043-lts","text":"JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04","title":"Ubuntu 20.04.3 LTS"},{"location":"release-notes/release_02-2022/#jupyterlab","text":"There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th.","title":"JupyterLab"},{"location":"release-notes/release_02-2022/#matplotlib-widget","text":"matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook.","title":"Matplotlib widget"},{"location":"release-notes/release_02-2022/#url-widget","text":"The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks.","title":"Url-widget"},{"location":"release-notes/release_02-2022/#new-memory-monitor-location","text":"JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen.","title":"New Memory Monitor Location"},{"location":"release-notes/release_02-2022/#notebook-debugger","text":"JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs","title":"Notebook Debugger"},{"location":"release-notes/release_02-2022/#mamba","text":"The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba.","title":"Mamba"},{"location":"release-notes/release_02-2022/#mamba-gator-is-installed","text":"mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu.","title":"Mamba Gator is installed"},{"location":"release-notes/release_02-2022/#spellchecker","text":"The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen.","title":"Spellchecker"},{"location":"release-notes/release_02-2022/#custom-extensions","text":"We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles.","title":"Custom Extensions"},{"location":"release-notes/release_02-2022/#recommended-jupyter-notebook-changes","text":"The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"Recommended Jupyter Notebook Changes"},{"location":"release-notes/release_02-2023/","text":"Welcome to the February 2023 OpenScienceLab Update! Changes: OpenScienceLab Single Sign On Automatic Authentication User Access IP Filter User Request Multi Download Kernel Usage Recommended Jupyter Notebook Changes OpenScienceLab OpenSARLab is now a part of OpenScienceLab with significant changes. Users can access different courses from a single page. Single Sign On New single sign-on (SSO) to simplify the logging-in process. SSO eliminates the burden of the sign-on process by listing all accounts on a single web page with additional layers of security. Automatic Authentication Users who sign up to the OpenScienceLab no longer need admins to activate their accounts. User Access Admins can easily give users access to different profiles from a single page. Admins can toggle access lists based on various deployments. IP Filter OpenScienceLab will prevent users in one of NASA\u2019s designated countries list from creating an account so that we are compliant with NASA\u2019s security protocol. User Request Users can send a request to the OpenScienceLab team from the portal. Multi Download Users can download multiple files at once through OpenScienceLab. NB : Downloading multiple directories requires users to compress the directories beforehand. Kernel Usage The latest update of JupyterLab has a built-in kernel monitor that tracks detailed resource usage, such as: Kernel Host Memory consumption and availability CPU usage Recommended Jupyter Notebook Changes The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab: MintPy Update: Due to the recent MintPy update, import syntax for version 1.4.1+ differs from previous versions. As a result, some notebooks may be incompatible with the latest version of MintPy. Below are examples of how to import MintPy depending on which versions you have: ``` python version 1.4.0 and below: import mintpy.view as view import mintpy.tsview as tsview . . . version 1.4.1+ from mintpy.cli import view, tsview, ... ``` For additional backward compatibility changes, please refer to the previous release notes.","title":"February 2023"},{"location":"release-notes/release_02-2023/#welcome-to-the-february-2023-opensciencelab-update","text":"","title":"Welcome to the February 2023 OpenScienceLab Update!"},{"location":"release-notes/release_02-2023/#changes","text":"OpenScienceLab Single Sign On Automatic Authentication User Access IP Filter User Request Multi Download Kernel Usage Recommended Jupyter Notebook Changes","title":"Changes:"},{"location":"release-notes/release_02-2023/#opensciencelab","text":"OpenSARLab is now a part of OpenScienceLab with significant changes. Users can access different courses from a single page.","title":"OpenScienceLab"},{"location":"release-notes/release_02-2023/#single-sign-on","text":"New single sign-on (SSO) to simplify the logging-in process. SSO eliminates the burden of the sign-on process by listing all accounts on a single web page with additional layers of security.","title":"Single Sign On"},{"location":"release-notes/release_02-2023/#automatic-authentication","text":"Users who sign up to the OpenScienceLab no longer need admins to activate their accounts.","title":"Automatic Authentication"},{"location":"release-notes/release_02-2023/#user-access","text":"Admins can easily give users access to different profiles from a single page. Admins can toggle access lists based on various deployments.","title":"User Access"},{"location":"release-notes/release_02-2023/#ip-filter","text":"OpenScienceLab will prevent users in one of NASA\u2019s designated countries list from creating an account so that we are compliant with NASA\u2019s security protocol.","title":"IP Filter"},{"location":"release-notes/release_02-2023/#user-request","text":"Users can send a request to the OpenScienceLab team from the portal.","title":"User Request"},{"location":"release-notes/release_02-2023/#multi-download","text":"Users can download multiple files at once through OpenScienceLab. NB : Downloading multiple directories requires users to compress the directories beforehand.","title":"Multi Download"},{"location":"release-notes/release_02-2023/#kernel-usage","text":"The latest update of JupyterLab has a built-in kernel monitor that tracks detailed resource usage, such as: Kernel Host Memory consumption and availability CPU usage","title":"Kernel Usage"},{"location":"release-notes/release_02-2023/#recommended-jupyter-notebook-changes","text":"The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab: MintPy Update: Due to the recent MintPy update, import syntax for version 1.4.1+ differs from previous versions. As a result, some notebooks may be incompatible with the latest version of MintPy. Below are examples of how to import MintPy depending on which versions you have: ``` python","title":"Recommended Jupyter Notebook Changes"},{"location":"release-notes/release_02-2023/#version-140-and-below","text":"import mintpy.view as view import mintpy.tsview as tsview . . .","title":"version 1.4.0 and below:"},{"location":"release-notes/release_02-2023/#version-141","text":"from mintpy.cli import view, tsview, ... ``` For additional backward compatibility changes, please refer to the previous release notes.","title":"version 1.4.1+"},{"location":"release-notes/release_02-2024/","text":"Welcome to the February 2024 OpenScienceLab Update! Changes: Multi-Factor Authentication Multi-Factor Authentication OpenScienceLab now requires users to set up Multi-Factor Authentication (MFA) to access OpenScienceLab resources. Directions on how to do so, as well as troubleshooting recommendations, can be found on the new MFA documentation page","title":"February 2024"},{"location":"release-notes/release_02-2024/#welcome-to-the-february-2024-opensciencelab-update","text":"","title":"Welcome to the February 2024 OpenScienceLab Update!"},{"location":"release-notes/release_02-2024/#changes","text":"Multi-Factor Authentication","title":"Changes:"},{"location":"release-notes/release_02-2024/#multi-factor-authentication","text":"OpenScienceLab now requires users to set up Multi-Factor Authentication (MFA) to access OpenScienceLab resources. Directions on how to do so, as well as troubleshooting recommendations, can be found on the new MFA documentation page","title":"Multi-Factor Authentication"},{"location":"release-notes/release_06-2021/","text":"Welcome to the June 2021 OpenSARlab Upgrade! Changes: conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier Conda Environments What is conda and what are conda environments? Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases. How did OpenSARlab use conda previously? OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized. What problems did this cause? Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult What has changed? All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like What will happen if I don't create any new environments? You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError). I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now? Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add. nbgitpuller Patch What is nbgitpuller? nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update. What was wrong with nbgitpuller? nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail. How was this issue addressed? We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line Installed jupyter-resource-usage What is jupyter-resource-usage? jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc. Added profile identifier The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"June 2021"},{"location":"release-notes/release_06-2021/#welcome-to-the-june-2021-opensarlab-upgrade","text":"","title":"Welcome to the June 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_06-2021/#changes","text":"conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier","title":"Changes:"},{"location":"release-notes/release_06-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_06-2021/#what-is-conda-and-what-are-conda-environments","text":"Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases.","title":"What is conda and what are conda environments?"},{"location":"release-notes/release_06-2021/#how-did-opensarlab-use-conda-previously","text":"OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized.","title":"How did OpenSARlab use conda previously?"},{"location":"release-notes/release_06-2021/#what-problems-did-this-cause","text":"Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult","title":"What problems did this cause?"},{"location":"release-notes/release_06-2021/#what-has-changed","text":"All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like","title":"What has changed?"},{"location":"release-notes/release_06-2021/#what-will-happen-if-i-dont-create-any-new-environments","text":"You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError).","title":"What will happen if I don't create any new environments?"},{"location":"release-notes/release_06-2021/#i-dont-want-to-wait-for-the-notebooks-to-yell-at-me-and-give-me-environment-warnings-how-can-i-create-the-environments-id-like-right-now","text":"Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add.","title":"I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now?"},{"location":"release-notes/release_06-2021/#nbgitpuller-patch","text":"","title":"nbgitpuller Patch"},{"location":"release-notes/release_06-2021/#what-is-nbgitpuller","text":"nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update.","title":"What is nbgitpuller?"},{"location":"release-notes/release_06-2021/#what-was-wrong-with-nbgitpuller","text":"nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail.","title":"What was wrong with nbgitpuller?"},{"location":"release-notes/release_06-2021/#how-was-this-issue-addressed","text":"We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line","title":"How was this issue addressed?"},{"location":"release-notes/release_06-2021/#installed-jupyter-resource-usage","text":"","title":"Installed jupyter-resource-usage"},{"location":"release-notes/release_06-2021/#what-is-jupyter-resource-usage","text":"jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc.","title":"What is jupyter-resource-usage?"},{"location":"release-notes/release_06-2021/#added-profile-identifier","text":"The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"Added profile identifier"},{"location":"release-notes/release_10-2021/","text":"Welcome to the October 2021 OpenSARlab Upgrade! Changes: Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page Conda Environments Pull in The GEOS657_MRS Repository The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location. OpenSARlab documentation changes OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab. Add current profile name to the top of every page Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"October 2021"},{"location":"release-notes/release_10-2021/#welcome-to-the-october-2021-opensarlab-upgrade","text":"","title":"Welcome to the October 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_10-2021/#changes","text":"Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page","title":"Changes:"},{"location":"release-notes/release_10-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_10-2021/#pull-in-the-geos657_mrs-repository","text":"The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location.","title":"Pull in The GEOS657_MRS Repository"},{"location":"release-notes/release_10-2021/#opensarlab-documentation-changes","text":"OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab.","title":"OpenSARlab documentation changes"},{"location":"release-notes/release_10-2021/#add-current-profile-name-to-the-top-of-every-page","text":"Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"Add current profile name to the top of every page"},{"location":"user-guides/OpenSARlab_environment/","text":"Return to Table of Contents The OpenScienceLab Environment and the Account Lifecycle Account Lifecycle Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 35 , 40 , 44 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation. NB : While the above is valid for the OpenSARLab deployment, each deployment's lifecycle period may differ. Since the OpenSARLab is the most commonly used deployment, the information listed on this page will mainly focus on the OpenSARLab deployment. OpenSARLab Environment Every OpenSARLab user has access to an Amazon AWS EC2 instance. Depending on demand, the individual user shares their resources with up to 3 users. Operating System Ubuntu 20.04.3 LTS Volume (storage) *500GB of EBS volume storage per user. * Volume size subject to change . OpenSARLab uses Amazon AWS EBS volumes mounted on user servers' home directories for storage. Since volumes are costly to maintain, any inactive accounts will have their volumes destroyed and replaced with the latest snapshot as a backup within a few days. Important Notes about Volume If your volume has been destroyed, the latest snapshot will regenerate a new EBS volume upon the next login. Upon successful login, your account should be identical from your previous session. When regenerating the volume from the snapshot, it can take some time (10+ minutes) to restore all the data. Notebooks may load slower than usual during this period. If users occupy more space than allocated, it will trigger the out-of-storage exception that will prevent users from logging in. It is the users' responsibility to manage their storage. Please contact an OpenScienceLab administrator if you need help logging in. Snapshot (Backup) A snapshot is a backup copy of users' volume. The snapshot of each volume is taken every day at 10:00 UTC. Only the most recent snapshot is retained. The user storage is persistent; you will only lose saved work if your account is inactive for 46 days. Memory (RAM) RAM allocated per user: 6GB - 16GB The amount of memory available to each user depends on the overall instance and may vary from 6 GB to 16 GB. The number of users on the same instance determines the amount of usable memory. Privileges Users do not have root (i.e., sudo ) privileges.","title":"OpenSARlab Account Details"},{"location":"user-guides/OpenSARlab_environment/#the-opensciencelab-environment-and-the-account-lifecycle","text":"","title":"The OpenScienceLab Environment and the Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#account-lifecycle","text":"Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 35 , 40 , 44 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation. NB : While the above is valid for the OpenSARLab deployment, each deployment's lifecycle period may differ. Since the OpenSARLab is the most commonly used deployment, the information listed on this page will mainly focus on the OpenSARLab deployment.","title":"Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#opensarlab-environment","text":"Every OpenSARLab user has access to an Amazon AWS EC2 instance. Depending on demand, the individual user shares their resources with up to 3 users.","title":"OpenSARLab Environment"},{"location":"user-guides/OpenSARlab_environment/#operating-system","text":"Ubuntu 20.04.3 LTS","title":"Operating System"},{"location":"user-guides/OpenSARlab_environment/#volume-storage","text":"*500GB of EBS volume storage per user. * Volume size subject to change . OpenSARLab uses Amazon AWS EBS volumes mounted on user servers' home directories for storage. Since volumes are costly to maintain, any inactive accounts will have their volumes destroyed and replaced with the latest snapshot as a backup within a few days.","title":"Volume (storage)"},{"location":"user-guides/OpenSARlab_environment/#important-notes-about-volume","text":"If your volume has been destroyed, the latest snapshot will regenerate a new EBS volume upon the next login. Upon successful login, your account should be identical from your previous session. When regenerating the volume from the snapshot, it can take some time (10+ minutes) to restore all the data. Notebooks may load slower than usual during this period. If users occupy more space than allocated, it will trigger the out-of-storage exception that will prevent users from logging in. It is the users' responsibility to manage their storage. Please contact an OpenScienceLab administrator if you need help logging in.","title":"Important Notes about Volume"},{"location":"user-guides/OpenSARlab_environment/#snapshot-backup","text":"A snapshot is a backup copy of users' volume. The snapshot of each volume is taken every day at 10:00 UTC. Only the most recent snapshot is retained. The user storage is persistent; you will only lose saved work if your account is inactive for 46 days.","title":"Snapshot (Backup)"},{"location":"user-guides/OpenSARlab_environment/#memory-ram","text":"RAM allocated per user: 6GB - 16GB The amount of memory available to each user depends on the overall instance and may vary from 6 GB to 16 GB. The number of users on the same instance determines the amount of usable memory.","title":"Memory (RAM)"},{"location":"user-guides/OpenSARlab_environment/#privileges","text":"Users do not have root (i.e., sudo ) privileges.","title":"Privileges"},{"location":"user-guides/OpenSARlab_terminal/","text":"Return to Table of Contents Using the Terminal in OpenSARlab Overview Users sometimes need to use an interactive shell to organize their accounts. With OpenSceinceLab\u2019s built-in terminal, users can use an interactive shell to accomplish their tasks. How to Open a Terminal If there is no Launcher tab in your workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in the Launcher tab. Live Example : How to Use the Terminal Use the command line as you would in any other Unix-like terminal. No Root Privileges Because the jovyan does not have a password, OpenScienceLab users cannot use the sudo command.","title":"OpenSARlab Terminal"},{"location":"user-guides/OpenSARlab_terminal/#using-the-terminal-in-opensarlab","text":"","title":"Using the Terminal in OpenSARlab"},{"location":"user-guides/OpenSARlab_terminal/#overview","text":"Users sometimes need to use an interactive shell to organize their accounts. With OpenSceinceLab\u2019s built-in terminal, users can use an interactive shell to accomplish their tasks.","title":"Overview"},{"location":"user-guides/OpenSARlab_terminal/#how-to-open-a-terminal","text":"If there is no Launcher tab in your workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in the Launcher tab. Live Example :","title":"How to Open a Terminal"},{"location":"user-guides/OpenSARlab_terminal/#how-to-use-the-terminal","text":"Use the command line as you would in any other Unix-like terminal.","title":"How to Use the Terminal"},{"location":"user-guides/OpenSARlab_terminal/#no-root-privileges","text":"Because the jovyan does not have a password, OpenScienceLab users cannot use the sudo command.","title":"No Root Privileges"},{"location":"user-guides/class_notebooks_best_practices/","text":"Return to Table of Contents Developing Notebooks for Classes or Trainings: Best Practices Provide a Conda Environment Capable of Running the Notebooks Provide students with a conda environment that has everything they need. Conda Environments in OpenSARlab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks. Set the Notebook Metadata to Use the Correct Environment Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created). Clear Your Notebook Output Before Saving it Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo. Keep your Conda Environment Up to Date Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARlab to create them, which is located in the /home/jovyan/conda_environments/ directory. Test Notebooks Ahead of Time. If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment. Plan for Students with Poor Internet Access Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner. To avoid issues related to poor internet access, consider following options: Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle . Avoid Changing Directories in Your Code Why? Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students. How? If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"Best Practices for Writing Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#developing-notebooks-for-classes-or-trainings-best-practices","text":"","title":"Developing Notebooks for Classes or Trainings: Best Practices"},{"location":"user-guides/class_notebooks_best_practices/#provide-a-conda-environment-capable-of-running-the-notebooks","text":"Provide students with a conda environment that has everything they need. Conda Environments in OpenSARlab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks.","title":"Provide a Conda Environment Capable of Running the Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#set-the-notebook-metadata-to-use-the-correct-environment","text":"Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created).","title":"Set the Notebook Metadata to Use the Correct Environment"},{"location":"user-guides/class_notebooks_best_practices/#clear-your-notebook-output-before-saving-it","text":"Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo.","title":"Clear Your Notebook Output Before Saving it"},{"location":"user-guides/class_notebooks_best_practices/#keep-your-conda-environment-up-to-date","text":"Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARlab to create them, which is located in the /home/jovyan/conda_environments/ directory.","title":"Keep your Conda Environment Up to Date"},{"location":"user-guides/class_notebooks_best_practices/#test-notebooks-ahead-of-time","text":"If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment.","title":"Test Notebooks Ahead of Time."},{"location":"user-guides/class_notebooks_best_practices/#plan-for-students-with-poor-internet-access","text":"Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner.","title":"Plan for Students with Poor Internet Access"},{"location":"user-guides/class_notebooks_best_practices/#to-avoid-issues-related-to-poor-internet-access-consider-following-options","text":"Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle .","title":"To avoid issues related to poor internet access, consider following options:"},{"location":"user-guides/class_notebooks_best_practices/#avoid-changing-directories-in-your-code","text":"","title":"Avoid Changing Directories in Your Code"},{"location":"user-guides/class_notebooks_best_practices/#why","text":"Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students.","title":"Why?"},{"location":"user-guides/class_notebooks_best_practices/#how","text":"If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"How?"},{"location":"user-guides/conda_environments/","text":"Return to Table of Contents Creating and Using Conda Environments in OpenSARlab OpenScienceLab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following conda environments are by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning autorift nisar_se unavco However, these environments are not pre-built; users must create the desired conda environments. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Here is a live demonstration on how to build the conda environment. Your environment will be ready after running the last cell. NB : It may take a while to generate your conda environment.","title":"Conda Environments"},{"location":"user-guides/conda_environments/#creating-and-using-conda-environments-in-opensarlab","text":"OpenScienceLab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following conda environments are by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning autorift nisar_se unavco However, these environments are not pre-built; users must create the desired conda environments. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Here is a live demonstration on how to build the conda environment. Your environment will be ready after running the last cell. NB : It may take a while to generate your conda environment.","title":"Creating and Using Conda Environments in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/","text":"Return to Table of Contents Git in OpenScienceLab Prerequisite Git - Version control systems that allow you to track changes to your files. ASF's Jupyter Notebook - A collection of Jupyter Notebooks used in OpenSARLab. Terminal - A built-in terminal within OpenScienceLab. The user should also have a basic understanding of Bash commands. Gitpuller A nbgitpuller pulls any changes to the notebook repo each time an OpenSARLab deployment server starts up. In short words, nbgitpuller will automatically update the notebooks to the latest version. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: Users will retain two copies of the same notebook. - The user-edited notebook will have a timestamp appended to its name. - The notebook with the original name will contain the new changes made by ASF. Example: Before Edit - Original version: sample_notebook.ipynb After Edit - Updated by user: sample_notebook__20210616165846.ipynb - Updated by ASF: sample_notebook.ipynb NB: The nbgitpuller will only run if you are in the main branch of the asf-jupyter-notebook repo. In the case of a broken Git state Due to its complexity, it is common for users to break Git workflow. A broken Git state can lead to unexpected results, such as notebooks not being updated. Below are the steps users can take if the Git workflow is in a state beyond their ability to repair. Repair Process: Preserve any files/directories you wish to keep under the /home/jovyan/notebooks directory. Either * download or move out items you wish to keep. Delete the entire /home/jovyan/notebooks directory. Use rm -rf /home/jovyan/notebooks on the terminal to delete all. Restart your server. Gitpuller will automatically clone a clean repository into your account. * NB : When downloading a directory, users may compress them first to optimize downloading. The below commands will allow users to (de)compress files/directories: Compress: zip -r <name>.zip <directory> Decompress: unzip <name>.zip Where <name> can be anything you wish. Using Other Git Repos in OpenScienceLab If you wish to utilize repositories other than those hosted by ASF, you can clone them into your OpenScienceLab account. However, users will need to manage the repos that they clone to prevent any issues. When cloning a repository from elsewhere, users must ensure not to nest git repositories, i.e., do not clone a repository within another repository. To avoid complications, clone your repos to /home/jovyan .","title":"Git in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/#git-in-opensciencelab","text":"","title":"Git in OpenScienceLab"},{"location":"user-guides/git_in_OpenSARlab/#prerequisite","text":"Git - Version control systems that allow you to track changes to your files. ASF's Jupyter Notebook - A collection of Jupyter Notebooks used in OpenSARLab. Terminal - A built-in terminal within OpenScienceLab. The user should also have a basic understanding of Bash commands.","title":"Prerequisite"},{"location":"user-guides/git_in_OpenSARlab/#gitpuller","text":"A nbgitpuller pulls any changes to the notebook repo each time an OpenSARLab deployment server starts up. In short words, nbgitpuller will automatically update the notebooks to the latest version. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: Users will retain two copies of the same notebook. - The user-edited notebook will have a timestamp appended to its name. - The notebook with the original name will contain the new changes made by ASF. Example: Before Edit - Original version: sample_notebook.ipynb After Edit - Updated by user: sample_notebook__20210616165846.ipynb - Updated by ASF: sample_notebook.ipynb NB: The nbgitpuller will only run if you are in the main branch of the asf-jupyter-notebook repo.","title":"Gitpuller"},{"location":"user-guides/git_in_OpenSARlab/#in-the-case-of-a-broken-git-state","text":"Due to its complexity, it is common for users to break Git workflow. A broken Git state can lead to unexpected results, such as notebooks not being updated. Below are the steps users can take if the Git workflow is in a state beyond their ability to repair. Repair Process: Preserve any files/directories you wish to keep under the /home/jovyan/notebooks directory. Either * download or move out items you wish to keep. Delete the entire /home/jovyan/notebooks directory. Use rm -rf /home/jovyan/notebooks on the terminal to delete all. Restart your server. Gitpuller will automatically clone a clean repository into your account. * NB : When downloading a directory, users may compress them first to optimize downloading. The below commands will allow users to (de)compress files/directories: Compress: zip -r <name>.zip <directory> Decompress: unzip <name>.zip Where <name> can be anything you wish.","title":"In the case of a broken Git state"},{"location":"user-guides/git_in_OpenSARlab/#using-other-git-repos-in-opensciencelab","text":"If you wish to utilize repositories other than those hosted by ASF, you can clone them into your OpenScienceLab account. However, users will need to manage the repos that they clone to prevent any issues. When cloning a repository from elsewhere, users must ensure not to nest git repositories, i.e., do not clone a repository within another repository. To avoid complications, clone your repos to /home/jovyan .","title":"Using Other Git Repos in OpenScienceLab"},{"location":"user-guides/how_to_run_a_notebook/","text":"Return to Table of Contents Getting Started A Light Introduction to Jupyter Notebook Markdown Cells Code Cells Detailed Instructions on Running Jupyter Notebook Selecting Cells Edit Mode vs Non-Edit Mode Select Individual Cell (Non-Edit Mode) Select Multiple Cells (Non-Edit Mode) Select Cell (Edit Mode) Select a Markdown Cell (Edit Mode) Hiding a Cell Hiding via Code Cells Running Cells Run a Single Cell Run Multiple Cells Rerunning a Notebook Clearing Cell Output Before Closing Summary and Demo A Light Introduction to Jupyter Notebook Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has four cell types, we use the following two for the OpenSciencelab: Markdown Cells Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double-click the cell. A markdown cell in edit mode If you wish to proceed through the notebook past the markdown cell or run a markdown cell's code to display its formatted contents, you can: - Click the play button at the top of the notebook - Use the shift + enter shortcut key. A run markdown cell NB : The cell will automatically move to the next cell if you are using the play button to run the cell. Code Cells Code cells contain editable and runnable Python code. You can run them in any order for any number of times. A code cell NB : While the ability to rerun the code cells in arbitrary order can be helpful, it can cause unexpected problems, such as: - Recycled variables may contain unexpected values if you run cells in non-sequential order. - Values from previous cells may trigger a different behavior when running the same cell. Detailed Instructions on Running Jupyter Notebook Now that users have a basic understandings of Jupyter notebook, users can use below manual as a refernce for detailed use. Selecting Cells Users may select cells individually or in a batch; users can then run the selected cells. Edit Mode vs Non-Edit Mode Before we discuss cell selection, it may be helpful to learn the difference between the edit mode and non-edit mode. Edit Mode : If you select a cell using edit mode, you may edit the code and Markdown written on that cell. You can choose the cell in the edit mode by clicking inside the cell box. Non-Edit Mode : If you select the cell outside of the code/Markdown box, you will be selecting the cell in a non-edit mode. While the difference is subtle, it is crucial to know the different modes because some hotkeys are unavailable in the edit mode. For instance, the shift + j command will allow you to select multiple cells in the non-edit mode, but this does not work in the edit mode. Select Individual Cell (Non-Edit Mode) Click on the left side of the cell. A selected cell displays a blue horizontal line on the left edge. Markdown cell will have an additional shaded area that is directly next to the cell. For a visual example, please refer to the hiding cells section. NB : It is crucial to avoid clicking the blue edge as well as the shaded area that is directly next to the cell. We will discuss this in a later section. Select Multiple Cells (Non-Edit Mode) Select a cell in non-edit mode Select multiple cells with: shift + j or shift + Down-Arrow to select additional cells below shift + k or shift + Up-Arrow to select additional cells above Perform batch operations on selected cells with play button or with ctrl + enter . Selected cells will have a blue background Select Cell (Edit Mode) Click inside a cell block. For live demonstration, please refer to the code cells section. NB : The edit mode cell will no longer display green edges. Select a Markdown Cell (Edit Mode) Double click inside a cell. For live demonstration, please refer to the Markdown cells section. NB : The edit mode cell will no longer display green edges. Hiding a Cell Sometimes, having too many cells may feel cumbersome. Below are the ways to hide the cells: Hiding Individual Cell You can hide an individual code cell by clicking the blue vertical line on the left of the code cell. Hiding Multiple Cells Alternatively, you may click the dark-shaded are on left of the Markdown cells to hide all proceeding cells. Running Cells Because you can run code cells in any order, each cell generates a number in the order they ran. Run a Single Cell With the Run Button Select any cell you wish to run. Do one of the following: Click Run button Ctrl + Enter to run a cell Shift + Enter to runs a cell and selects the cell below Alt + Enter to runs a cell and inserts an empty cell below Running selected cell multiple times with ctrl + enter Run Multiple Cells Instead of running just a single cell, you can run multiple cells at once in a following manner: Run every cell above/below selected cell. Run them in a groups of selected cell. Run the entire notebook Running every cell above/below: Select a cell, then: Select Run All Above Selected Cell from the Run menu or select Run Selected Cell and All Below from the Run menu Running all cells above selected. Note that the selected cell is ignored. Run a batch of selected cells Select a group of cells, then: - Use a hotkey - or click the run button Runs a group of cells in a batch using ctrl + enter Run an Entire Notebook If you wish to run the entire notebook from the get-go, you can do one of the following: Select _Run_ > _Run All Cells_ Select _Run_ > _Restart Kernel and Run All Cells..._ The difference is that the former option preserves the values from the previous run while the latter lets you run from a new state\u2014more on this in the next section. Rerunning a Notebook We recommend restarting the notebook kernel before rerunning it since any initialized variables and data structures from a previous run persist in memory along with their values, which can lead to unintended results. For instance, consider the following case : Assume you have a Python list with date-specific data, such as weather, stock prices, etc. If you were to run a cell that appends data from a specific date multiple times, it may yield unpredictable results due to the duplicate data. e.g. In the above example, running the second cell once will append a new date at the end of the list. However, running the same cell will keep appending the same value. Rerunning previous cells can cause unexpected behavior. We recommend restarting the notebook when you are running from the beginning. To restart the notebook, select any of the Restart options from the Kernel Menu. Clearing Cell Output Before Closing We recommend clearing every output from each code cell before closing or saving a notebook. Leaving the output in place can increase the file size of the notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#getting-started","text":"A Light Introduction to Jupyter Notebook Markdown Cells Code Cells Detailed Instructions on Running Jupyter Notebook Selecting Cells Edit Mode vs Non-Edit Mode Select Individual Cell (Non-Edit Mode) Select Multiple Cells (Non-Edit Mode) Select Cell (Edit Mode) Select a Markdown Cell (Edit Mode) Hiding a Cell Hiding via Code Cells Running Cells Run a Single Cell Run Multiple Cells Rerunning a Notebook Clearing Cell Output Before Closing Summary and Demo","title":"Getting Started"},{"location":"user-guides/how_to_run_a_notebook/#a-light-introduction-to-jupyter-notebook","text":"Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has four cell types, we use the following two for the OpenSciencelab:","title":"A Light Introduction to Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#markdown-cells","text":"Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double-click the cell. A markdown cell in edit mode If you wish to proceed through the notebook past the markdown cell or run a markdown cell's code to display its formatted contents, you can: - Click the play button at the top of the notebook - Use the shift + enter shortcut key. A run markdown cell NB : The cell will automatically move to the next cell if you are using the play button to run the cell.","title":"Markdown Cells"},{"location":"user-guides/how_to_run_a_notebook/#code-cells","text":"Code cells contain editable and runnable Python code. You can run them in any order for any number of times. A code cell NB : While the ability to rerun the code cells in arbitrary order can be helpful, it can cause unexpected problems, such as: - Recycled variables may contain unexpected values if you run cells in non-sequential order. - Values from previous cells may trigger a different behavior when running the same cell.","title":"Code Cells"},{"location":"user-guides/how_to_run_a_notebook/#detailed-instructions-on-running-jupyter-notebook","text":"Now that users have a basic understandings of Jupyter notebook, users can use below manual as a refernce for detailed use.","title":"Detailed Instructions on Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#selecting-cells","text":"Users may select cells individually or in a batch; users can then run the selected cells.","title":"Selecting Cells"},{"location":"user-guides/how_to_run_a_notebook/#edit-mode-vs-non-edit-mode","text":"Before we discuss cell selection, it may be helpful to learn the difference between the edit mode and non-edit mode. Edit Mode : If you select a cell using edit mode, you may edit the code and Markdown written on that cell. You can choose the cell in the edit mode by clicking inside the cell box. Non-Edit Mode : If you select the cell outside of the code/Markdown box, you will be selecting the cell in a non-edit mode. While the difference is subtle, it is crucial to know the different modes because some hotkeys are unavailable in the edit mode. For instance, the shift + j command will allow you to select multiple cells in the non-edit mode, but this does not work in the edit mode.","title":"Edit Mode vs Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-individual-cell-non-edit-mode","text":"Click on the left side of the cell. A selected cell displays a blue horizontal line on the left edge. Markdown cell will have an additional shaded area that is directly next to the cell. For a visual example, please refer to the hiding cells section. NB : It is crucial to avoid clicking the blue edge as well as the shaded area that is directly next to the cell. We will discuss this in a later section.","title":"Select Individual Cell (Non-Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-multiple-cells-non-edit-mode","text":"Select a cell in non-edit mode Select multiple cells with: shift + j or shift + Down-Arrow to select additional cells below shift + k or shift + Up-Arrow to select additional cells above Perform batch operations on selected cells with play button or with ctrl + enter . Selected cells will have a blue background","title":"Select Multiple Cells (Non-Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-cell-edit-mode","text":"Click inside a cell block. For live demonstration, please refer to the code cells section. NB : The edit mode cell will no longer display green edges.","title":"Select Cell (Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#select-a-markdown-cell-edit-mode","text":"Double click inside a cell. For live demonstration, please refer to the Markdown cells section. NB : The edit mode cell will no longer display green edges.","title":"Select a Markdown Cell (Edit Mode)"},{"location":"user-guides/how_to_run_a_notebook/#hiding-a-cell","text":"Sometimes, having too many cells may feel cumbersome. Below are the ways to hide the cells:","title":"Hiding a Cell"},{"location":"user-guides/how_to_run_a_notebook/#hiding-individual-cell","text":"You can hide an individual code cell by clicking the blue vertical line on the left of the code cell.","title":"Hiding Individual Cell"},{"location":"user-guides/how_to_run_a_notebook/#hiding-multiple-cells","text":"Alternatively, you may click the dark-shaded are on left of the Markdown cells to hide all proceeding cells.","title":"Hiding Multiple Cells"},{"location":"user-guides/how_to_run_a_notebook/#running-cells","text":"Because you can run code cells in any order, each cell generates a number in the order they ran.","title":"Running Cells"},{"location":"user-guides/how_to_run_a_notebook/#run-a-single-cell","text":"","title":"Run a Single Cell"},{"location":"user-guides/how_to_run_a_notebook/#with-the-run-button","text":"Select any cell you wish to run. Do one of the following: Click Run button Ctrl + Enter to run a cell Shift + Enter to runs a cell and selects the cell below Alt + Enter to runs a cell and inserts an empty cell below Running selected cell multiple times with ctrl + enter","title":"With the Run Button"},{"location":"user-guides/how_to_run_a_notebook/#run-multiple-cells","text":"Instead of running just a single cell, you can run multiple cells at once in a following manner: Run every cell above/below selected cell. Run them in a groups of selected cell. Run the entire notebook","title":"Run Multiple Cells"},{"location":"user-guides/how_to_run_a_notebook/#running-every-cell-abovebelow","text":"Select a cell, then: Select Run All Above Selected Cell from the Run menu or select Run Selected Cell and All Below from the Run menu Running all cells above selected. Note that the selected cell is ignored.","title":"Running every cell above/below:"},{"location":"user-guides/how_to_run_a_notebook/#run-a-batch-of-selected-cells","text":"Select a group of cells, then: - Use a hotkey - or click the run button Runs a group of cells in a batch using ctrl + enter","title":"Run a batch of selected cells"},{"location":"user-guides/how_to_run_a_notebook/#run-an-entire-notebook","text":"If you wish to run the entire notebook from the get-go, you can do one of the following: Select _Run_ > _Run All Cells_ Select _Run_ > _Restart Kernel and Run All Cells..._ The difference is that the former option preserves the values from the previous run while the latter lets you run from a new state\u2014more on this in the next section.","title":"Run an Entire Notebook"},{"location":"user-guides/how_to_run_a_notebook/#rerunning-a-notebook","text":"We recommend restarting the notebook kernel before rerunning it since any initialized variables and data structures from a previous run persist in memory along with their values, which can lead to unintended results. For instance, consider the following case : Assume you have a Python list with date-specific data, such as weather, stock prices, etc. If you were to run a cell that appends data from a specific date multiple times, it may yield unpredictable results due to the duplicate data. e.g. In the above example, running the second cell once will append a new date at the end of the list. However, running the same cell will keep appending the same value. Rerunning previous cells can cause unexpected behavior. We recommend restarting the notebook when you are running from the beginning. To restart the notebook, select any of the Restart options from the Kernel Menu.","title":"Rerunning a Notebook"},{"location":"user-guides/how_to_run_a_notebook/#clearing-cell-output-before-closing","text":"We recommend clearing every output from each code cell before closing or saving a notebook. Leaving the output in place can increase the file size of the notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Clearing Cell Output Before Closing"},{"location":"user-guides/installing_software_in_OpenSARlab/","text":"Return to Table of Contents Installing Software in OpenScienceLab pip Install pip package inside conda environment mamba Install conda packages within a running notebook Install conda packages from the terminal apt and apt-get pip What is pip ? pip is a package installer for Python. You can install pip packages onto your account in the following manner: NB : Your installed pip packages are in the /home/jovyan/.local/lib/python3.x/site-packages directory. Open a terminal and use the following command: python -m pip install --user <package_name> Install pip package inside of a conda environment Open a terminal and use the following commands: conda activate <environment_name> python -m pip install --user <package_name> mamba Users can install additional software with mamba in OpenScienceLab. NB: OpenScienceLab began using mamba instead of conda to install conda packages in 2022. Since the syntax for mamba is identical to conda syntax, users who used conda previously should be familiar with the mamba workflow. Nonetheless, it is still worthwhile to reference conda documentation due to its similarity. Packages installed in the base conda environment will not stay after the server shuts down. You will need to reinstall it during subsequent OpenScienceLab sessions. However, changes to non-base will persist. Therefore, we recommend installing new packages in your non-base environments rather than in the base . Install conda packages within a running notebook Edit a notebook code cell Then use the following command: %mamba install <package_name> Run the code cell Install conda packages from the terminal Open a terminal and use following command: mamba activate <environment_name> mamba install <package_name> apt and apt-get At this time, users cannot install software in OpenScienceLab using apt or apt-get .","title":"Installing Software in OpenSARlab"},{"location":"user-guides/installing_software_in_OpenSARlab/#installing-software-in-opensciencelab","text":"pip Install pip package inside conda environment mamba Install conda packages within a running notebook Install conda packages from the terminal apt and apt-get","title":"Installing Software in OpenScienceLab"},{"location":"user-guides/installing_software_in_OpenSARlab/#pip","text":"What is pip ? pip is a package installer for Python. You can install pip packages onto your account in the following manner: NB : Your installed pip packages are in the /home/jovyan/.local/lib/python3.x/site-packages directory. Open a terminal and use the following command: python -m pip install --user <package_name>","title":"pip"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-pip-package-inside-of-a-conda-environment","text":"Open a terminal and use the following commands: conda activate <environment_name> python -m pip install --user <package_name>","title":"Install pip package inside of a conda environment"},{"location":"user-guides/installing_software_in_OpenSARlab/#mamba","text":"Users can install additional software with mamba in OpenScienceLab. NB: OpenScienceLab began using mamba instead of conda to install conda packages in 2022. Since the syntax for mamba is identical to conda syntax, users who used conda previously should be familiar with the mamba workflow. Nonetheless, it is still worthwhile to reference conda documentation due to its similarity. Packages installed in the base conda environment will not stay after the server shuts down. You will need to reinstall it during subsequent OpenScienceLab sessions. However, changes to non-base will persist. Therefore, we recommend installing new packages in your non-base environments rather than in the base .","title":"mamba"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-within-a-running-notebook","text":"Edit a notebook code cell Then use the following command: %mamba install <package_name> Run the code cell","title":"Install conda packages within a running notebook"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-from-the-terminal","text":"Open a terminal and use following command: mamba activate <environment_name> mamba install <package_name>","title":"Install conda packages from the terminal"},{"location":"user-guides/installing_software_in_OpenSARlab/#apt-and-apt-get","text":"At this time, users cannot install software in OpenScienceLab using apt or apt-get .","title":"apt and apt-get"},{"location":"user-guides/jupyter_magic/","text":"Return to Table of Contents Jupyter Line and Cell Magics, and IPython Syntax In addition to running Python code, Jupyter Notebooks allows users to run magic commands with various functionality. While all magic commands are available to users, the below magic commands are used the most in the OpenScienceLab: Shell Assignment Syntax Line Magics Cell Magics Shell Assignment Syntax In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example: Line Magics Line magics start with a single % sign and affect only the line where % is used. The following are the most frequently used line magics: %matplotlib inline Allows non-interactive matplotlib plots to be displayed in a notebook. Example : %matplotlib widget Allows interactive matplotlib plots to be displayed and interacted with inside a Jupyter Notebook. Example : %df This is a custom magic written specifically for OpenScienceLab. It uses the Python function shutil.disk_usage() to check the storage state of the user's volumes. You may use the below flags to output results in a different manner: %df - Returns a human-readable string in GB. %df --raw - Returns a raw data object. %df --on - Returns a string in GB after running every subsequent code cell. %df --off - Turns off the %df --on option. %df -v - Prints additional debugging text. Example : Legacy Note Due to the new url_widget package, the user no longer needs to use the %matplotlib notebook and use the %matplotlib widget instead. Cell Magics Cell magics start with %% and affect the contents of an entire cell. %%javascript (or %%js ) Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell. Example : %%capture Runs the cell but captures all output. We typically use this to suppress the output of a %matplotlib plot that the user does not wish to see.","title":"Jupyter Magic Commands"},{"location":"user-guides/jupyter_magic/#jupyter-line-and-cell-magics-and-ipython-syntax","text":"In addition to running Python code, Jupyter Notebooks allows users to run magic commands with various functionality. While all magic commands are available to users, the below magic commands are used the most in the OpenScienceLab: Shell Assignment Syntax Line Magics Cell Magics","title":"Jupyter Line and Cell Magics, and IPython Syntax"},{"location":"user-guides/jupyter_magic/#shell-assignment-syntax","text":"In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example:","title":"Shell Assignment Syntax"},{"location":"user-guides/jupyter_magic/#line-magics","text":"Line magics start with a single % sign and affect only the line where % is used. The following are the most frequently used line magics:","title":"Line Magics"},{"location":"user-guides/jupyter_magic/#matplotlib-inline","text":"Allows non-interactive matplotlib plots to be displayed in a notebook. Example :","title":"%matplotlib inline"},{"location":"user-guides/jupyter_magic/#matplotlib-widget","text":"Allows interactive matplotlib plots to be displayed and interacted with inside a Jupyter Notebook. Example :","title":"%matplotlib widget"},{"location":"user-guides/jupyter_magic/#df","text":"This is a custom magic written specifically for OpenScienceLab. It uses the Python function shutil.disk_usage() to check the storage state of the user's volumes. You may use the below flags to output results in a different manner: %df - Returns a human-readable string in GB. %df --raw - Returns a raw data object. %df --on - Returns a string in GB after running every subsequent code cell. %df --off - Turns off the %df --on option. %df -v - Prints additional debugging text. Example :","title":"%df"},{"location":"user-guides/jupyter_magic/#legacy-note","text":"Due to the new url_widget package, the user no longer needs to use the %matplotlib notebook and use the %matplotlib widget instead.","title":"Legacy Note"},{"location":"user-guides/jupyter_magic/#cell-magics","text":"Cell magics start with %% and affect the contents of an entire cell.","title":"Cell Magics"},{"location":"user-guides/jupyter_magic/#javascript-or-js","text":"Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell. Example :","title":"%%javascript (or %%js)"},{"location":"user-guides/jupyter_magic/#capture","text":"Runs the cell but captures all output. We typically use this to suppress the output of a %matplotlib plot that the user does not wish to see.","title":"%%capture"},{"location":"user-guides/jupyter_notebook_extensions/","text":"Return to Table of Contents Managing Jupyter Lab Extensions As an OpenSARlab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARlab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARlab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu Managing Jupyter Notebook Extensions As an OpenSARlab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here . Enabling and Disabling Extensions The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-lab-extensions","text":"As an OpenSARlab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARlab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARlab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu","title":"Managing Jupyter Lab Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-notebook-extensions","text":"As an OpenSARlab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here .","title":"Managing Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#enabling-and-disabling-extensions","text":"The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Enabling and Disabling Extensions"},{"location":"user-guides/logging_out_and_server_shutdown/","text":"Return to Table of Contents Logging Out of OpenScienceLab and Shutting Down the Server When you are ready to stop working in OpenScienceLab, please shut down your server and logout. Shutting down your server is the same as shutting down your computer. If you shut down your server and come back the next day (or even the next week), all your files will still be there and you can resume processing your data. Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs. Why Shut Down the Server? Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. Additionally, while OpenScienceLab is and will remain free to our users, resources used do incur costs, which are paid for by the Alaska Satellite Facility. Help us keep OpenScienceLab free by shutting down servers when they are not in use. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before you leave. How to Shut Down The Server and Logout in Jupyter Lab Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button. How to Shut Down The Server and Logout in Jupyter Notebook Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"Logging Out and Server Shutdown"},{"location":"user-guides/logging_out_and_server_shutdown/#logging-out-of-opensciencelab-and-shutting-down-the-server","text":"When you are ready to stop working in OpenScienceLab, please shut down your server and logout. Shutting down your server is the same as shutting down your computer. If you shut down your server and come back the next day (or even the next week), all your files will still be there and you can resume processing your data. Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs.","title":"Logging Out of OpenScienceLab and Shutting Down the Server"},{"location":"user-guides/logging_out_and_server_shutdown/#why-shut-down-the-server","text":"Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. Additionally, while OpenScienceLab is and will remain free to our users, resources used do incur costs, which are paid for by the Alaska Satellite Facility. Help us keep OpenScienceLab free by shutting down servers when they are not in use. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before you leave.","title":"Why Shut Down the Server?"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-lab","text":"Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Lab"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-notebook","text":"Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Notebook"},{"location":"user-guides/mfa/","text":"Return to Table of Contents Configuring Multi-Factor Authentication Watch: Log In and Set Up MFA Before You Begin Setup Steps Troubleshooting Multi-Factor Authentication (MFA) is required to access OpenScienceLab resources. Currently, we support TOTP-based authentication, with plans to add hardware key (ex. Yubikey) authentication in future updates. Watch: Log In and Set Up MFA Before You Begin A TOTP-enabled MFA application is required to use OpenScienceLab resources. While the most widely utilized approach is through a smartphone application, there are also several desktop clients that provide this functionality. Application Desktop Support Android Support iOS Support KeePassXc \u2705 \u274c \u274c Enpass \u2705 \u2705 \u2705 Cisco Duo \u2705 \u2705 \u2705 Authenticator.cc* \u2705 \u2705 \u2705 Bitwarden** \u2705 \u2705 \u2705 FreeOTP \u274c \u2705 \u2705 Google Authenticator \u274c \u2705 \u2705 Microsoft Authenticator \u274c \u2705 \u2705 Aegis Authenticator \u274c \u2705 \u274c * Browser based ** TOTP supported in paid version only Setup Steps Log into OpenScienceLab normally. Leave the \"MFA\" field on the login page blank. Click \"Configure New MFA Device\" in the Navigation Bar at the top of the page. Configure your MFA device: If using a smartphone application, scan the QR code using the application's \"Scan QR Code\" feature. If using a desktop application, copy the OTP Secret into the \"TOTP\" field Enter the current MFA code generated by your application into the \"Code 1\" field on the \"Configure New MFA Device\" page. When the code changes, enter the new code into the \"Code 2\" field on the same page and click the \"Check MFA Codes\" button. If the check is successfull, navigate back to the home page by clicking \"Home\" in the Navigation Bar at the top of the page and continue using OpenScienceLab as normal. Troubleshooting If the MFA Code check is not successful, there are two potential issues: One of the codes was mis-typed. The secret was not properly put into the MFA application. Test again with another two consecutive codes, and if the verification step fails again, check that the OTP Secret on the page matches the secret in your application. If all else fails, refresh the page. This will generate a new code, which you can then use to follow the same steps above. For additional issues and further troubleshooting, please email uso@asf.alaska.edu","title":"Configuring Multi-Factor Authentication"},{"location":"user-guides/mfa/#configuring-multi-factor-authentication","text":"Watch: Log In and Set Up MFA Before You Begin Setup Steps Troubleshooting Multi-Factor Authentication (MFA) is required to access OpenScienceLab resources. Currently, we support TOTP-based authentication, with plans to add hardware key (ex. Yubikey) authentication in future updates.","title":"Configuring Multi-Factor Authentication"},{"location":"user-guides/mfa/#watch-log-in-and-set-up-mfa","text":"","title":"Watch: Log In and Set Up MFA"},{"location":"user-guides/mfa/#before-you-begin","text":"A TOTP-enabled MFA application is required to use OpenScienceLab resources. While the most widely utilized approach is through a smartphone application, there are also several desktop clients that provide this functionality. Application Desktop Support Android Support iOS Support KeePassXc \u2705 \u274c \u274c Enpass \u2705 \u2705 \u2705 Cisco Duo \u2705 \u2705 \u2705 Authenticator.cc* \u2705 \u2705 \u2705 Bitwarden** \u2705 \u2705 \u2705 FreeOTP \u274c \u2705 \u2705 Google Authenticator \u274c \u2705 \u2705 Microsoft Authenticator \u274c \u2705 \u2705 Aegis Authenticator \u274c \u2705 \u274c * Browser based ** TOTP supported in paid version only","title":"Before You Begin"},{"location":"user-guides/mfa/#setup-steps","text":"Log into OpenScienceLab normally. Leave the \"MFA\" field on the login page blank. Click \"Configure New MFA Device\" in the Navigation Bar at the top of the page. Configure your MFA device: If using a smartphone application, scan the QR code using the application's \"Scan QR Code\" feature. If using a desktop application, copy the OTP Secret into the \"TOTP\" field Enter the current MFA code generated by your application into the \"Code 1\" field on the \"Configure New MFA Device\" page. When the code changes, enter the new code into the \"Code 2\" field on the same page and click the \"Check MFA Codes\" button. If the check is successfull, navigate back to the home page by clicking \"Home\" in the Navigation Bar at the top of the page and continue using OpenScienceLab as normal.","title":"Setup Steps"},{"location":"user-guides/mfa/#troubleshooting","text":"If the MFA Code check is not successful, there are two potential issues: One of the codes was mis-typed. The secret was not properly put into the MFA application. Test again with another two consecutive codes, and if the verification step fails again, check that the OTP Secret on the page matches the secret in your application. If all else fails, refresh the page. This will generate a new code, which you can then use to follow the same steps above. For additional issues and further troubleshooting, please email uso@asf.alaska.edu","title":"Troubleshooting"},{"location":"user-guides/restarting_server_and_kernel/","text":"Return to Table of Contents Restarting the OpenSARlab Server and Notebook Kernel Restarting the OpenSARlab Server Overview Restarting the server triggers the nbgitpuller . Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. There is a notebook update that you wish to pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . NB : If you are comfortable with git , you could do a git pull from the terminal or in a notebook instead. However, do not push your changes as it may interfere with the nbgitpuller Steps to Restart the Server Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen. Click the Stop My Server Button Click the Start My Server Button, which may take a few seconds to appear. Click The Launch Server Button Select a Server Profile and Click the Start button Wait for the server to start; this may take some time. Optional: Click The Event Log arrow for detailed startup status information. You may use this information to send a report to the admin if your server does not start. Changing a Notebook Kernel Overview Notebooks in OpenScienceLab run in a variety of conda environments. Since each kernel has different packages, you must pick the right one to confirm it has the required packages. How to Switch Kernel Click the upper right corner. It should have the name of the kernel that you are currently using. A window with a list of built kernel will pop up. Select whichever kernel you need. NB : To build a kernel , refer to the conda environments section. Restarting a Jupyter Notebook Kernel Overview Variables and their assigned values are stored in memory as you run code cells in a notebook. As a result, rerunning a previously ran notebook without restarting the kernel may trigger some issues. Uncleared variables are problematic for various reasons, such as: Increase the file size of the notebook. They occupy limited memory resources. *Previously defined values may cause unexpected results during the rerun (). NB_: _Refer to the Rerunning a Notebook section in the How to Run a Jupyter Notebook * section. The solution for this is to restart the kernel to clear notebook data. How to Clear Notebook Select one of the following from the Kernel Menu: Restart Kernel : Restarts the kernel but leave the old code cell output in place. Restart Kernel and Clear All Outputs... : Restarts the kernel and removes old code cell output. This is generally the preferred option. Restart Kernel and Run up to Selected Cell... : Restarts the kernel and run up to the cell where you selected. Restart Kernel and Run All Cells... : Restarts the kernel and runs all the code cells. This only works if the notebook does not require input from the user. For most use cases, select Restart Kernel and Clear All Outputs...","title":"OpenSARlab Servers and Kernels"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server-and-notebook-kernel","text":"","title":"Restarting the OpenSARlab Server and Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server","text":"","title":"Restarting the OpenSARlab Server"},{"location":"user-guides/restarting_server_and_kernel/#overview","text":"Restarting the server triggers the nbgitpuller . Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. There is a notebook update that you wish to pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . NB : If you are comfortable with git , you could do a git pull from the terminal or in a notebook instead. However, do not push your changes as it may interfere with the nbgitpuller","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#steps-to-restart-the-server","text":"Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen. Click the Stop My Server Button Click the Start My Server Button, which may take a few seconds to appear. Click The Launch Server Button Select a Server Profile and Click the Start button Wait for the server to start; this may take some time. Optional: Click The Event Log arrow for detailed startup status information. You may use this information to send a report to the admin if your server does not start.","title":"Steps to Restart the Server"},{"location":"user-guides/restarting_server_and_kernel/#changing-a-notebook-kernel","text":"","title":"Changing a Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#overview_1","text":"Notebooks in OpenScienceLab run in a variety of conda environments. Since each kernel has different packages, you must pick the right one to confirm it has the required packages.","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#how-to-switch-kernel","text":"Click the upper right corner. It should have the name of the kernel that you are currently using. A window with a list of built kernel will pop up. Select whichever kernel you need. NB : To build a kernel , refer to the conda environments section.","title":"How to Switch Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-a-jupyter-notebook-kernel","text":"","title":"Restarting a Jupyter Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#overview_2","text":"Variables and their assigned values are stored in memory as you run code cells in a notebook. As a result, rerunning a previously ran notebook without restarting the kernel may trigger some issues. Uncleared variables are problematic for various reasons, such as: Increase the file size of the notebook. They occupy limited memory resources. *Previously defined values may cause unexpected results during the rerun (). NB_: _Refer to the Rerunning a Notebook section in the How to Run a Jupyter Notebook * section. The solution for this is to restart the kernel to clear notebook data.","title":"Overview"},{"location":"user-guides/restarting_server_and_kernel/#how-to-clear-notebook","text":"Select one of the following from the Kernel Menu: Restart Kernel : Restarts the kernel but leave the old code cell output in place. Restart Kernel and Clear All Outputs... : Restarts the kernel and removes old code cell output. This is generally the preferred option. Restart Kernel and Run up to Selected Cell... : Restarts the kernel and run up to the cell where you selected. Restart Kernel and Run All Cells... : Restarts the kernel and runs all the code cells. This only works if the notebook does not require input from the user. For most use cases, select Restart Kernel and Clear All Outputs...","title":"How to Clear Notebook"},{"location":"user-guides/s3_buckets/","text":"Return to Table of Contents Accessing S3 Buckets The commands below can be run from a notebook code cell by prepending an ! or directly from a terminal. For those that are familiar with CLI Those that are comfortable using command line tools, such as Bash, may find that the AWS syntax is not intuitive. e.g. Recursive flag: Bash AWS -r --recursive Key Point : Please remember that AWS has a cumbersome syntax that may differ from other command lines. Generally speaking, the S3 command should look something like the following: aws s3 [optional_flags] <source_path> <destination_Path> Accessing Public S3 Buckets When accessing a public bucket from OpenScienceLab, be sure to include the following flags: - --no-sign-request - --region=<bucket's region> NB: The S3 buckets can have different sets of permissions. The commands below assume public access to the list, read, and write. List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename *Upload a file to a bucket aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename *Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user with the following steps: Open a terminal Run the following command: aws configure set default.s3.multipart_threshold 5000MB Accessing Private S3 Buckets Configure the AWS Client in your OpenScienceLab account (Prerequisite) To configure the AWS Client, you will need: - An AWS Access Key ID to the account holding the bucket - An AWS Secret Access Key to the account holding the bucket - An arn to an AWS IAM role with permission to access the bucket Steps on configuring a terminal : Open a terminal and run: aws configure It will ask you for the following: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty Manually edit the config file to add profile using *Vim. Use vim ~/.aws/config to open the config file. Add the following: [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit Vim *NB: Vim is a command line text editor. Since it does have a learning curve, users that never used Vim are encouraged to reference the Vim Command Cheat Sheet . For Private S3 Bucket List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"S3 Bucket Access in OpenSARlab"},{"location":"user-guides/s3_buckets/#accessing-s3-buckets","text":"The commands below can be run from a notebook code cell by prepending an ! or directly from a terminal.","title":"Accessing S3 Buckets"},{"location":"user-guides/s3_buckets/#for-those-that-are-familiar-with-cli","text":"Those that are comfortable using command line tools, such as Bash, may find that the AWS syntax is not intuitive. e.g. Recursive flag: Bash AWS -r --recursive Key Point : Please remember that AWS has a cumbersome syntax that may differ from other command lines. Generally speaking, the S3 command should look something like the following: aws s3 [optional_flags] <source_path> <destination_Path>","title":"For those that are familiar with CLI"},{"location":"user-guides/s3_buckets/#accessing-public-s3-buckets","text":"When accessing a public bucket from OpenScienceLab, be sure to include the following flags: - --no-sign-request - --region=<bucket's region> NB: The S3 buckets can have different sets of permissions. The commands below assume public access to the list, read, and write. List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename *Upload a file to a bucket aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename *Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user with the following steps: Open a terminal Run the following command: aws configure set default.s3.multipart_threshold 5000MB","title":"Accessing Public S3 Buckets"},{"location":"user-guides/s3_buckets/#accessing-private-s3-buckets","text":"","title":"Accessing Private S3 Buckets"},{"location":"user-guides/s3_buckets/#configure-the-aws-client-in-your-opensciencelab-account","text":"(Prerequisite) To configure the AWS Client, you will need: - An AWS Access Key ID to the account holding the bucket - An AWS Secret Access Key to the account holding the bucket - An arn to an AWS IAM role with permission to access the bucket Steps on configuring a terminal : Open a terminal and run: aws configure It will ask you for the following: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty Manually edit the config file to add profile using *Vim. Use vim ~/.aws/config to open the config file. Add the following: [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit Vim *NB: Vim is a command line text editor. Since it does have a learning curve, users that never used Vim are encouraged to reference the Vim Command Cheat Sheet .","title":"Configure the AWS Client in your OpenScienceLab account"},{"location":"user-guides/s3_buckets/#for-private-s3-bucket","text":"List of Useful AWS commands : Action AWS Command List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"For Private S3 Bucket"},{"location":"user-guides/troubleshooting_guide/","text":"Return to Table of Contents Troubleshooting Guide Why don't any of the deployments appear on the OpenScienceLab home page? You most likely have not yet configured Multi-Factor Authentication (MFA). While any user can log in without MFA, until a user has configured their MFA device, they will be unable to access any OpenScienceLab resources. See Configuring Multi-Factor Authentication for more information and a detailed walkthrough. Why did the kernel die while running a notebook? The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results. I successfully ran a notebook earlier on the same data but now it is killing the kernel. OpenSARlab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARlab user environment can be found here . When I open a notebook, I receive \"Kernel not found\" message. The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see. The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again. My notebook won't open, opens slowly, or won't save These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARlab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again. I tried to create a new notebook and recieved the error Forbidden This can happen when you have logged out of OpenSARlab and then try to create a new notebook from an OpenSARlab browser tab that was left open. You must log back in before you can create, open, or run a notebook. I am receiving a No space left on device error. OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need. My server won't start and I cannot access OpenSARlab. This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library . The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab. When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name. One of my notebooks looks like it has a mix of code from various versions of the notebook. We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I know there was an update made to an ASF notebook but I still have the old version. We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I am having trouble setting up a web server and developing my web app in OpenSARlab. This cannot be done in OpenSARlab. You will need to do this elsewhere. A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears. This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output. I tried to run a notebook that downloads products from HyP3 and I get an error HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb My issue is not on this list Please contact an OpenSARlab administrator for help.","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#troubleshooting-guide","text":"","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#why-dont-any-of-the-deployments-appear-on-the-opensciencelab-home-page","text":"You most likely have not yet configured Multi-Factor Authentication (MFA). While any user can log in without MFA, until a user has configured their MFA device, they will be unable to access any OpenScienceLab resources. See Configuring Multi-Factor Authentication for more information and a detailed walkthrough.","title":"Why don't any of the deployments appear on the OpenScienceLab home page?"},{"location":"user-guides/troubleshooting_guide/#why-did-the-kernel-die-while-running-a-notebook","text":"The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results.","title":"Why did the kernel die while running a notebook?"},{"location":"user-guides/troubleshooting_guide/#i-successfully-ran-a-notebook-earlier-on-the-same-data-but-now-it-is-killing-the-kernel","text":"OpenSARlab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARlab user environment can be found here .","title":"I successfully ran a notebook earlier on the same data but now it is killing the kernel."},{"location":"user-guides/troubleshooting_guide/#when-i-open-a-notebook-i-receive-kernel-not-found-message","text":"The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb","title":"When I open a notebook, I receive \"Kernel not found\" message."},{"location":"user-guides/troubleshooting_guide/#i-see-one-or-many-python-3-or-python-kernels-instead-of-kernels-named-after-conda-environments-that-i-expect-to-see","text":"The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again.","title":"I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see."},{"location":"user-guides/troubleshooting_guide/#my-notebook-wont-open-opens-slowly-or-wont-save","text":"These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARlab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again.","title":"My notebook won't open, opens slowly, or won't save"},{"location":"user-guides/troubleshooting_guide/#i-tried-to-create-a-new-notebook-and-recieved-the-error-forbidden","text":"This can happen when you have logged out of OpenSARlab and then try to create a new notebook from an OpenSARlab browser tab that was left open. You must log back in before you can create, open, or run a notebook.","title":"I tried to create a new notebook and recieved the error Forbidden"},{"location":"user-guides/troubleshooting_guide/#i-am-receiving-a-no-space-left-on-device-error","text":"OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need.","title":"I am receiving a No space left on device error."},{"location":"user-guides/troubleshooting_guide/#my-server-wont-start-and-i-cannot-access-opensarlab","text":"This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library .","title":"My server won't start and I cannot access OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#the-edits-i-made-to-an-asf-notebook-have-disappeared-since-the-last-time-i-used-opensarlab","text":"When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name.","title":"The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#one-of-my-notebooks-looks-like-it-has-a-mix-of-code-from-various-versions-of-the-notebook","text":"We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"One of my notebooks looks like it has a mix of code from various versions of the notebook."},{"location":"user-guides/troubleshooting_guide/#i-know-there-was-an-update-made-to-an-asf-notebook-but-i-still-have-the-old-version","text":"We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"I know there was an update made to an ASF notebook but I still have the old version."},{"location":"user-guides/troubleshooting_guide/#i-am-having-trouble-setting-up-a-web-server-and-developing-my-web-app-in-opensarlab","text":"This cannot be done in OpenSARlab. You will need to do this elsewhere.","title":"I am having trouble setting up a web server and developing my web app in OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#a-notebook-wont-load-a-new-browser-tab-opens-and-shows-the-jupyterhub-header-but-no-notebook-appears","text":"This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output.","title":"A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears."},{"location":"user-guides/troubleshooting_guide/#i-tried-to-run-a-notebook-that-downloads-products-from-hyp3-and-i-get-an-error","text":"HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb","title":"I tried to run a notebook that downloads products from HyP3 and I get an error"},{"location":"user-guides/troubleshooting_guide/#my-issue-is-not-on-this-list","text":"Please contact an OpenSARlab administrator for help.","title":"My issue is not on this list"}]}