{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenSARlab What is OpenSARlab? OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment. How will OpenSARlab benefit my work as a SAR scientist? OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis How will OpenSARlab benefit the class or training I am planning? OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers. How is OpenSARlab different from Binder? Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity) How to Access OpenSARlab As a Paid Service Managed by Alaska Satellite Facility Enterprise Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARlab deployment to suit your needs. Deploy OpenSARlab to Your Own AWS Account (Coming Soon) Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS. Contact Us Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Home"},{"location":"#welcome-to-opensarlab","text":"","title":"Welcome to OpenSARlab"},{"location":"#what-is-opensarlab","text":"OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. Groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment.","title":"What is OpenSARlab?"},{"location":"#how-will-opensarlab-benefit-my-work-as-a-sar-scientist","text":"OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis","title":"How will OpenSARlab benefit my work as a SAR scientist?"},{"location":"#how-will-opensarlab-benefit-the-class-or-training-i-am-planning","text":"OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers.","title":"How will OpenSARlab benefit the class or training I am planning?"},{"location":"#how-is-opensarlab-different-from-binder","text":"Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity)","title":"How is OpenSARlab different from Binder?"},{"location":"#how-to-access-opensarlab","text":"","title":"How to Access OpenSARlab"},{"location":"#as-a-paid-service-managed-by-alaska-satellite-facility-enterprise","text":"Contact ASF-E ( uaf-jupyterhub-asf@alaska.edu ) to discuss options for setting up an OpenSARlab deployment to suit your needs.","title":"As a Paid Service Managed by Alaska Satellite Facility Enterprise"},{"location":"#deploy-opensarlab-to-your-own-aws-account-coming-soon","text":"Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS.","title":"Deploy OpenSARlab to Your Own AWS Account (Coming Soon)"},{"location":"#contact-us","text":"Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Contact Us"},{"location":"dev/","text":"System Diagram Deploy OpenSARlab to AWS Conda Environment Options OpenSARlab Notifications Troubleshooting","title":"Dev"},{"location":"release_notes/","text":"June 2021 October 2021 February 2022","title":"Release notes"},{"location":"user/","text":"Welcome to the OpenSARlab User Guide Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"user/#welcome-to-the-opensarlab-user-guide","text":"Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"dev-guides/conda_environments/","text":"Return to Developer Guide There are a few options for creating conda environments in OpenSARlab. Each option come with benefits and drawbacks. Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks Drawbacks Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local , Register Their Kernels, and Run Any Setup Scripts Benefits Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts Drawbacks Increases the time it takes to start an OpenSARlab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage. Leave Conda Environment Creation up to the Users Benefits Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts. Drawbacks Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Conda Environment Options"},{"location":"dev-guides/conda_environments/#there-are-a-few-options-for-creating-conda-environments-in-opensarlab","text":"Each option come with benefits and drawbacks.","title":"There are a few options for creating conda environments in OpenSARlab."},{"location":"dev-guides/conda_environments/#create-conda-environments-register-their-kernels-and-run-any-setup-scripts-in-the-docker-images","text":"","title":"Create Conda Environments, Register Their Kernels, and Run Any Setup Scripts in the Docker Image/s"},{"location":"dev-guides/conda_environments/#benefits","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda; they can just start running notebooks","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks","text":"Users cannot install additional packages into their conda environments Changes to environments on the docker image involve rebuilding the container and CodePipelines Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used.","title":"Drawbacks"},{"location":"dev-guides/conda_environments/#create-conda-environments-in-the-docker-images-then-in-the-hook-script-sync-them-to-homelocal-register-their-kernels-and-run-any-setup-scripts","text":"","title":"Create Conda Environments in the Docker Image/s. Then, in the Hook Script, Sync Them to $Home/.local, Register Their Kernels, and Run Any Setup Scripts"},{"location":"dev-guides/conda_environments/#benefits_1","text":"Users don't have to create conda environments, which saves them time Users don't need to know much about conda at all; they can just start running notebooks The environments are stored in $HOME/.local , so users have permissions to install, update, remove, and debug packages Environments are synced, not copied, so changes made by users will persist across server restarts","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks_1","text":"Increases the time it takes to start an OpenSARlab server Syncing environments from the docker image to $HOME/.local , registering their kernels, and running any needed setup scripts all happens at server startup Large environments may overrun the 20GB root volume mounted on each EC2 instance, requiring that larger, more expensive root volumes be used. By storing the environment on both user volumes and EC2 node volumes, you effectively double pay for that storage.","title":"Drawbacks"},{"location":"dev-guides/conda_environments/#leave-conda-environment-creation-up-to-the-users","text":"","title":"Leave Conda Environment Creation up to the Users"},{"location":"dev-guides/conda_environments/#benefits_2","text":"Docker images remain small, avoiding potential storage overruns on the EC2 nodes' 20GB volumes. Server start ups do not require copying or syncing environments, and so require less time. Users have full control over their conda environments and their changes will persist across server restarts.","title":"Benefits"},{"location":"dev-guides/conda_environments/#drawbacks_2","text":"Users have to create their own conda environments This requires some knowledge of conda and takes time. Note: There is an ASF notebook repo to aid users in building their own environments.","title":"Drawbacks"},{"location":"dev-guides/deploy_OpenSARlab/","text":"Return to Developer Guide Deploy OpenSARlab to an AWS account A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" deployment_name \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention. Take AWS SES out of sandbox The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox. Create an AWS Cost Allocation Tag Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\" Add dockerhub credentials to AWS Secrets Manager This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button Setup an iCal calendar for notifications Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section Store your CA certificate OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager Prepare CodeCommit Repos TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a deployment_name -container CodeCommit repo in your AWS account Create a deployment_name -cluster CodeCommit repo Clone the deployment_name -container and deployment_name -cluster repos to your local computer using ssh cd into your local deployment_name -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local deployment_name -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local deployment_name -container repo git pull github main Create a main branch in the deployment_name -container repo git checkout -b main Push to the remote deployment_name -container repo git push origin main cd into your local deployment_name -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local deployment_name -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local deployment_name -cluster repo git pull github main Create a main branch in the deployment_name -cluster repo git checkout -b main Push to the remote deployment_name -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab Customize opensarlab_container code for deployment The opensarlab-container repo contains one example image named helloworld , which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name The image name must be alpha-numeric with no whitespaces or special characters Edit the dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory under FROM release as testing Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Add, commit, and push changes to the remote CodeCommit repo Customize opensarlab_cluster code for deployment Create and add any additional custom jupyter magic commands to the opensarlab/jupyterhub/singleuser/custom_magics directory Add any additional scripts you may have created for use in your image to the opensarlab/jupyterhub/singleuser/hooks directory Duplicate opensarlab/jupyterhub/singleuser/hooks/sar.sh , renaming it after your image name Edit opensarlab/jupyterhub/singleuser/hooks/<image_name>.sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Rename opensarlab/opensarlab.example.yaml to opensarlab/opensarlab.yaml Use the example notes in opensarlab/opensarlab.yaml to define the required and optional fields Update opensarlab/jupyterhub/helm_config.yaml singleuser Add any needed extraFiles hub Add any needed extraFiles Add, commit, and push changes to the remote CodeCommit repo Build the container CloudFormation stack This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the deployment_name -container repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment CodeCommitSourceRepo The CodeCommit repo holding the container code ( deployment_name -container) CodeCommitSourceBranch The name of the production branch of the deployment_name -container CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again Build the cluster CloudFormation stack This CloudFormation stack dynamically creates 3 additional stacks. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select opensarlab/pipeline/cf-pipeline.yaml from your local branch of the cluster repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment. Do not use a stack name that ends in cluster , jupyterhub , or cognito . These are reserved. CodeCommitRepoName The CodeCommit repo holding the container code ( deployment_name -cluster) CodeCommitBranchName The name of the production branch of the deployment_name -cluster CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button Take care of odds and ends Update deployment_url in the cluster repo opensarlab/opensarlab.yaml if you started off using load balancer Don't forget to update your DNS record Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on cluster stack Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: deployment_name Prime the Auto Scaling Group for each profile unless there are active users Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select an autoscaling group Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta\\> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message\\> 1. Your notification message 1. Sign up with your `admin_user_name` account, sign in, and add groups for each profile and sudo 1. Open the `deployment_url` in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the `admin_user_name` parameter of the `opensarlab.yaml` 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the `deployment_name`-auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the `deployment_name`-cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the `deployment_name`-cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam:: :user/ \" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS profile on your local machine 1. Example profile: 1. yaml [profile profile_name] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam:: :role/ - -cluster-user-access cluster_name = -cluster ``` 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl Destroy Deployments At the end of a deployment's lifecycle, it is important to destroy it properly so no resources are left in place, costing you money. Deleting the deployment_name -container, deployment_name -auth, deployment_name -cluster, and deployment_name CloudFormation stacks will kill the deployment and remove its resources but there are a couple of prerequisite steps to prepare for proper deletion. Note: In the steps below, do not manually delete any S3 buckets after emptying them. If you delete the buckets, the CloudFormation stack deletions associated with those buckets will fail and you will have to recreate the empty buckets to proceed. Empty them and let CloudFormation delete them for you. Delete the deployment_name -container CloudFormation stack Empty the codepipeline- - deployment_name -container-container S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - deployment_name -container-container S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete ECR repos for each profile Navigate to the AWS Elastic Container Registry Click the box next to the deployment_name / repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat for each profile Delete the deployment_name -container CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -container stack Click the \"Delete\" button Click the \"Delete stack\" button Click the deployment_name -container stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name -auth CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -auth stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name -auth stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name -cluster CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -cluster stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name -cluster stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name CloudFormation stack Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Click the box next to the deployment_name /hub repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the deployment_name /notifications repository Empty the codepipeline- - deployment_name S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - deployment_name S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete the deployment_name CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete EBS snapshots Navigate to the AWS EC2 console Click the \"Snapshots\" link in the sidebar menu Filter by osl-stackname: deployment_name Double check that you filtered for the correct deployment! Select all snapshots Select \"Delete\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button Delete EBS volumes Navigate to the AWS EC2 console Click the \"Volumes\" link in the sidebar menu Filter by osl-stackname: deployment_name Double check that you filtered for the correct deployment! Select all volumes Select \"Delete volumes\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button (Optional) Delete the deployment_name -cluster and deployment_name -container CodeCommit repositories CodeCommit repos are cheap If you think you may re-deploy the same deployment, you may want to ease future work by leaving these repos in place You could also download a zip of your repos, store them in S3, and then delete them Navigate to the AWS CodeCommit console Check the box next to the deployment_name -container repo Click the \"Delete repository\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the deployment_name -cluster repo Confirm that all resources have been deleted Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the \"Tag Editor\" link in the sidebar menu Tags: Key: Cost allocation tag Value: deployment_name Click the \"Search resources\" button Identify and delete any remaining resources","title":"Deploy OpenSARlab to AWS"},{"location":"dev-guides/deploy_OpenSARlab/#deploy-opensarlab-to-an-aws-account","text":"A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" deployment_name \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention.","title":"Deploy OpenSARlab to an AWS account"},{"location":"dev-guides/deploy_OpenSARlab/#take-aws-ses-out-of-sandbox","text":"The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox.","title":"Take AWS SES out of sandbox"},{"location":"dev-guides/deploy_OpenSARlab/#create-an-aws-cost-allocation-tag","text":"Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\"","title":"Create an AWS Cost Allocation Tag"},{"location":"dev-guides/deploy_OpenSARlab/#add-dockerhub-credentials-to-aws-secrets-manager","text":"This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button","title":"Add dockerhub credentials to AWS Secrets Manager"},{"location":"dev-guides/deploy_OpenSARlab/#setup-an-ical-calendar-for-notifications","text":"Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section","title":"Setup an iCal calendar for notifications"},{"location":"dev-guides/deploy_OpenSARlab/#store-your-ca-certificate","text":"OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager","title":"Store your CA certificate"},{"location":"dev-guides/deploy_OpenSARlab/#prepare-codecommit-repos","text":"TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a deployment_name -container CodeCommit repo in your AWS account Create a deployment_name -cluster CodeCommit repo Clone the deployment_name -container and deployment_name -cluster repos to your local computer using ssh cd into your local deployment_name -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local deployment_name -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local deployment_name -container repo git pull github main Create a main branch in the deployment_name -container repo git checkout -b main Push to the remote deployment_name -container repo git push origin main cd into your local deployment_name -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local deployment_name -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local deployment_name -cluster repo git pull github main Create a main branch in the deployment_name -cluster repo git checkout -b main Push to the remote deployment_name -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab","title":"Prepare CodeCommit Repos"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_container-code-for-deployment","text":"The opensarlab-container repo contains one example image named helloworld , which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name The image name must be alpha-numeric with no whitespaces or special characters Edit the dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory under FROM release as testing Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_container code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_cluster-code-for-deployment","text":"Create and add any additional custom jupyter magic commands to the opensarlab/jupyterhub/singleuser/custom_magics directory Add any additional scripts you may have created for use in your image to the opensarlab/jupyterhub/singleuser/hooks directory Duplicate opensarlab/jupyterhub/singleuser/hooks/sar.sh , renaming it after your image name Edit opensarlab/jupyterhub/singleuser/hooks/<image_name>.sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Rename opensarlab/opensarlab.example.yaml to opensarlab/opensarlab.yaml Use the example notes in opensarlab/opensarlab.yaml to define the required and optional fields Update opensarlab/jupyterhub/helm_config.yaml singleuser Add any needed extraFiles hub Add any needed extraFiles Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_cluster code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-container-cloudformation-stack","text":"This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the deployment_name -container repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment CodeCommitSourceRepo The CodeCommit repo holding the container code ( deployment_name -container) CodeCommitSourceBranch The name of the production branch of the deployment_name -container CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again","title":"Build the container CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-cluster-cloudformation-stack","text":"This CloudFormation stack dynamically creates 3 additional stacks. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select opensarlab/pipeline/cf-pipeline.yaml from your local branch of the cluster repo Click the \"Next\" button Page 2: Specify stack details Stack Name Use a recognizable name that makes sense for your deployment. Do not use a stack name that ends in cluster , jupyterhub , or cognito . These are reserved. CodeCommitRepoName The CodeCommit repo holding the container code ( deployment_name -cluster) CodeCommitBranchName The name of the production branch of the deployment_name -cluster CodeCommit repo CostTagKey The cost allocation key you registered for tracking deployment costs CostTagValue deployment_name Page 3: Configure stack options Tags: Key: Cost allocation tag Value: deployment_name Click the \"Next\" button Page 4: Review Stack Name Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button","title":"Build the cluster CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#take-care-of-odds-and-ends","text":"Update deployment_url in the cluster repo opensarlab/opensarlab.yaml if you started off using load balancer Don't forget to update your DNS record Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on cluster stack Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: deployment_name Prime the Auto Scaling Group for each profile unless there are active users Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select an autoscaling group Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta\\> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message\\> 1. Your notification message 1. Sign up with your `admin_user_name` account, sign in, and add groups for each profile and sudo 1. Open the `deployment_url` in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the `admin_user_name` parameter of the `opensarlab.yaml` 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the `deployment_name`-auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the `deployment_name`-cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the `deployment_name`-cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam:: :user/ \" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS profile on your local machine 1. Example profile: 1. yaml [profile profile_name] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam:: :role/ - -cluster-user-access cluster_name = -cluster ``` 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl","title":"Take care of odds and ends"},{"location":"dev-guides/deploy_OpenSARlab/#destroy-deployments","text":"At the end of a deployment's lifecycle, it is important to destroy it properly so no resources are left in place, costing you money. Deleting the deployment_name -container, deployment_name -auth, deployment_name -cluster, and deployment_name CloudFormation stacks will kill the deployment and remove its resources but there are a couple of prerequisite steps to prepare for proper deletion. Note: In the steps below, do not manually delete any S3 buckets after emptying them. If you delete the buckets, the CloudFormation stack deletions associated with those buckets will fail and you will have to recreate the empty buckets to proceed. Empty them and let CloudFormation delete them for you. Delete the deployment_name -container CloudFormation stack Empty the codepipeline- - deployment_name -container-container S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - deployment_name -container-container S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete ECR repos for each profile Navigate to the AWS Elastic Container Registry Click the box next to the deployment_name / repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat for each profile Delete the deployment_name -container CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -container stack Click the \"Delete\" button Click the \"Delete stack\" button Click the deployment_name -container stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name -auth CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -auth stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name -auth stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name -cluster CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name -cluster stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name -cluster stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the deployment_name CloudFormation stack Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Click the box next to the deployment_name /hub repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the deployment_name /notifications repository Empty the codepipeline- - deployment_name S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - deployment_name S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete the deployment_name CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the deployment_name stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the deployment_name stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete EBS snapshots Navigate to the AWS EC2 console Click the \"Snapshots\" link in the sidebar menu Filter by osl-stackname: deployment_name Double check that you filtered for the correct deployment! Select all snapshots Select \"Delete\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button Delete EBS volumes Navigate to the AWS EC2 console Click the \"Volumes\" link in the sidebar menu Filter by osl-stackname: deployment_name Double check that you filtered for the correct deployment! Select all volumes Select \"Delete volumes\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button (Optional) Delete the deployment_name -cluster and deployment_name -container CodeCommit repositories CodeCommit repos are cheap If you think you may re-deploy the same deployment, you may want to ease future work by leaving these repos in place You could also download a zip of your repos, store them in S3, and then delete them Navigate to the AWS CodeCommit console Check the box next to the deployment_name -container repo Click the \"Delete repository\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the deployment_name -cluster repo Confirm that all resources have been deleted Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the \"Tag Editor\" link in the sidebar menu Tags: Key: Cost allocation tag Value: deployment_name Click the \"Search resources\" button Identify and delete any remaining resources","title":"Destroy Deployments"},{"location":"dev-guides/notifications/","text":"Return to Developer Guide Create OpenSARlab Notifications Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"Notifications"},{"location":"dev-guides/notifications/#create-opensarlab-notifications","text":"Create a new event in your notification calendar The event title corresponds to the notification title Select the time or date range for which you would like to display the notification The remaining notification details are included in the event description Add a <meta> tag Define profiles for which to display notification (profile names may contain spaces) profile: profile_1, profile_2, profile_n Note: comma separated with no spaces Define notification type type: info is blue type: warning is yellow type: success is green type: error is red Optional hide notification mute: true Add a <message> tag Add your message body as html Add line breaks </br> Add links <a href=\"https://url.com\" target=\"blank\"><span style=\"color: blue\">link</span></a> Note: You must unlink the URL using the unlink button in the calendar message tool bar for it to work. Turn off text automated formatting Select all the text in the message body Click the remove formatting button in the message toolbar","title":"Create OpenSARlab Notifications"},{"location":"dev-guides/system_diagram/","text":"Return to Table of Contents OpenSARlab System Diagram","title":"System diagram"},{"location":"dev-guides/system_diagram/#opensarlab-system-diagram","text":"","title":"OpenSARlab System Diagram"},{"location":"dev-guides/troubleshooting/","text":"Return to Developer Guide A. Users 1. OSL Servers Time Out Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"Troubelshooting Guide"},{"location":"dev-guides/troubleshooting/#a-users","text":"","title":"A. Users"},{"location":"dev-guides/troubleshooting/#1-osl-servers-time-out","text":"Problem : An user's server consistently times out while other users have no difficulty starting a server. Solution : Respawn the hub pod. Sometimes an internal state within hub gets out of sync with the actual state. To respawn the hub pod, in a terminal: sk <name-of-cluster> kubectl get pods kubectl delete pod <name-of-hub-pod>","title":"1. OSL Servers Time Out"},{"location":"release-notes/release_02-2022/","text":"Welcome to the February 2022 OpenSARlab Update! Changes: Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update Ubuntu 20.04.3 LTS JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04 JupyterLab There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th. Matplotlib widget matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook. Url-widget The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks. New Memory Monitor Location JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen. Notebook Debugger JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs Mamba The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba. Mamba Gator is installed mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu. Spellchecker The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen. Custom Extensions We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles. Recommended Jupyter Notebook Changes The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"February 2022"},{"location":"release-notes/release_02-2022/#welcome-to-the-february-2022-opensarlab-update","text":"","title":"Welcome to the February 2022 OpenSARlab Update!"},{"location":"release-notes/release_02-2022/#changes","text":"Ubuntu 20.04.3 LTS JupyterLab Matplotlib widget Url-widget New memory monitor location Notebook Debugger Mamba Mamba Gator Spellchecker Custom extensions Recommended Jupyter Notebook changes related to update","title":"Changes:"},{"location":"release-notes/release_02-2022/#ubuntu-20043-lts","text":"JupyterHub is now running on Ubuntu 20.04.3 LTS, updated from Ubuntu 18.04","title":"Ubuntu 20.04.3 LTS"},{"location":"release-notes/release_02-2022/#jupyterlab","text":"There are now JupyterLab profiles available alongside the Classic Jupyter Notebook profiles. JupyterLab comes with many more features than Classic Jupyter Notebook (see the JupyterLab Docs ) for more information. Classic Jupyter Notebook profiles will remain active for 1 month before being deprecated on March 7th.","title":"JupyterLab"},{"location":"release-notes/release_02-2022/#matplotlib-widget","text":"matplotlib notebook has been replaced with matplotlib widget for interactive matplotlib plots. matplotlib notebook will not work in JupyterLab, whereas matplotlib widget works in both JupyterLab and Classic Jupyter Notebook.","title":"Matplotlib widget"},{"location":"release-notes/release_02-2022/#url-widget","text":"The url-widget package is now installed, allowing notebook Python kernels access to the current notebook's URL. This is useful for dynamically creating links to files and notebooks in OpenSARlab, and it is used in the kernel checking code at the beginning of ASF provided notebooks.","title":"Url-widget"},{"location":"release-notes/release_02-2022/#new-memory-monitor-location","text":"JupyterLab comes with a built-in memory monitor, replacing the jupyter-resource-usage extension. The new memory monitor can be found in the status bar at the bottom of the JupyterLab screen.","title":"New Memory Monitor Location"},{"location":"release-notes/release_02-2022/#notebook-debugger","text":"JupyterLab comes with a built-in notebook debugger. JupyterLab Debugger Docs","title":"Notebook Debugger"},{"location":"release-notes/release_02-2022/#mamba","text":"The mamba package manager is now available in OpenSARlab. Mamba is a multi-threaded \"reimplementation of the conda package manager in C++.\" It creates environments much more quickly than conda. The opensarlab-envs repo has been updated to use mamba.","title":"Mamba"},{"location":"release-notes/release_02-2022/#mamba-gator-is-installed","text":"mamba gator provides a GUI for managing conda/mamba environments that is accessible in JupyterLab. Access mamba gator by selecting the Conda Packages Manager from the Settings menu.","title":"Mamba Gator is installed"},{"location":"release-notes/release_02-2022/#spellchecker","text":"The spellchecker extension is installed. It checks spelling in markdown cells. The language may be changed in the status bar at the bottom of the screen.","title":"Spellchecker"},{"location":"release-notes/release_02-2022/#custom-extensions","text":"We have added some custom JupyterLab extensions to duplicate custom features previously added in OpenSARlab for Jupyter Notebooks. opensarlab-profile-label provides the name of the current OpenSARlab profile in the topbar. opensarlab-doc-link provides a link to the OpenSARlab documentation in the topbar. opensarlab-controlbtn provides a Shutdown and Logout Page button in the topbar. opensarlab-notifications provides similar functionality to the popup notifications used in the OpenSARlab Classic Jupyter Notebook profiles.","title":"Custom Extensions"},{"location":"release-notes/release_02-2022/#recommended-jupyter-notebook-changes","text":"The following bullet points cover code changes you may need to make to your notebooks for them to work in JupyterLab note: These changes are backwards compatible and updated notebooks will still run in Jupyter Notebook. note: All ASF notebooks have already been updated. The javascript variable Jupyter.notebook.kernel does not exist in JupyterLab. If you need a Python variable containing a notebook's current Python kernel, run: env = !echo $CONDA_PREFIX The javascript variable window.location does not exist in JupyterLab If you need the current url of your Jupyter workspace, install the url-widget package in your conda environment and use it to retrieve the url: # In one cell import url_widget as url_w notebook_url = url_w.URLWidget() display(notebook_url) # In a following cell notebook_url = notebook_url.value %matplotlib notebook does not work for interactive plotting in JupyterLab Instead, use: %matplotlib widget asf_notebook.py is deprecated and has been replaced with opensarlab-lib : https://github.com/ASFOpenSARlab/opensarlab-lib asf_notebook.py still works (with deprecation warnings) but it is not being maintained. Install opensarlab-lib with one of the following commands: python -m pip install opensarlab-lib conda install -n <environment_name> -c conda-forge opensarlab-lib Alternatively, you may add environment.yml as a dependency and use it instead.","title":"Recommended Jupyter Notebook Changes"},{"location":"release-notes/release_06-2021/","text":"Welcome to the June 2021 OpenSARlab Upgrade! Changes: conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier Conda Environments What is conda and what are conda environments? Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases. How did OpenSARlab use conda previously? OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized. What problems did this cause? Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult What has changed? All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like What will happen if I don't create any new environments? You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError). I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now? Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add. nbgitpuller Patch What is nbgitpuller? nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update. What was wrong with nbgitpuller? nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail. How was this issue addressed? We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line Installed jupyter-resource-usage What is jupyter-resource-usage? jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc. Added profile identifier The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"June 2021"},{"location":"release-notes/release_06-2021/#welcome-to-the-june-2021-opensarlab-upgrade","text":"","title":"Welcome to the June 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_06-2021/#changes","text":"conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier","title":"Changes:"},{"location":"release-notes/release_06-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_06-2021/#what-is-conda-and-what-are-conda-environments","text":"Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases.","title":"What is conda and what are conda environments?"},{"location":"release-notes/release_06-2021/#how-did-opensarlab-use-conda-previously","text":"OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized.","title":"How did OpenSARlab use conda previously?"},{"location":"release-notes/release_06-2021/#what-problems-did-this-cause","text":"Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult","title":"What problems did this cause?"},{"location":"release-notes/release_06-2021/#what-has-changed","text":"All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like","title":"What has changed?"},{"location":"release-notes/release_06-2021/#what-will-happen-if-i-dont-create-any-new-environments","text":"You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError).","title":"What will happen if I don't create any new environments?"},{"location":"release-notes/release_06-2021/#i-dont-want-to-wait-for-the-notebooks-to-yell-at-me-and-give-me-environment-warnings-how-can-i-create-the-environments-id-like-right-now","text":"Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add.","title":"I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now?"},{"location":"release-notes/release_06-2021/#nbgitpuller-patch","text":"","title":"nbgitpuller Patch"},{"location":"release-notes/release_06-2021/#what-is-nbgitpuller","text":"nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update.","title":"What is nbgitpuller?"},{"location":"release-notes/release_06-2021/#what-was-wrong-with-nbgitpuller","text":"nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail.","title":"What was wrong with nbgitpuller?"},{"location":"release-notes/release_06-2021/#how-was-this-issue-addressed","text":"We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line","title":"How was this issue addressed?"},{"location":"release-notes/release_06-2021/#installed-jupyter-resource-usage","text":"","title":"Installed jupyter-resource-usage"},{"location":"release-notes/release_06-2021/#what-is-jupyter-resource-usage","text":"jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc.","title":"What is jupyter-resource-usage?"},{"location":"release-notes/release_06-2021/#added-profile-identifier","text":"The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"Added profile identifier"},{"location":"release-notes/release_10-2021/","text":"Welcome to the October 2021 OpenSARlab Upgrade! Changes: Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page Conda Environments Pull in The GEOS657_MRS Repository The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location. OpenSARlab documentation changes OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab. Add current profile name to the top of every page Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"October 2021"},{"location":"release-notes/release_10-2021/#welcome-to-the-october-2021-opensarlab-upgrade","text":"","title":"Welcome to the October 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_10-2021/#changes","text":"Pull in the GEOS657_MRS repository OpenSARlab documentation changes Add OpenSARlab documentation link to the top of every page Add current profile name to the top of every page","title":"Changes:"},{"location":"release-notes/release_10-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_10-2021/#pull-in-the-geos657_mrs-repository","text":"The GEOS_657_Labs directory has been removed from the opnesarlab-notebooks repository and moved into its own repository, uafgeoteach/GEOS657_MRS The notebooks in the /home/jovyan/notebooks/ASF/GEOS_657_Labs directory can now be found in the /home/jovyan/GEOS_657_Labs directory. If you made any changes to the notebooks in their original location, you may still see them there, but necessary scripts may be missing and you should start working out of the new location.","title":"Pull in The GEOS657_MRS Repository"},{"location":"release-notes/release_10-2021/#opensarlab-documentation-changes","text":"OpenSARlab documentation is no longer being stored in /home/jovyan/opensarlab_docs in Jupyter Notebook form. There is now a link to the OpenSARlab-docs website at the top of every page in OpenSARlab.","title":"OpenSARlab documentation changes"},{"location":"release-notes/release_10-2021/#add-current-profile-name-to-the-top-of-every-page","text":"Different OpenSARlab profiles allow for varying resource allotments. The current profile name now appears at the top of every OpenSARlab page to serve as a reminder to the user of which profile they are running in.","title":"Add current profile name to the top of every page"},{"location":"user-guides/OpenSARlab_environment/","text":"Return to Table of Contents The OpenSARlab Environment and Account Lifecycle Account Lifecycle Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 37 , 41 , 43 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation. OpenSARlab Environment Every OpenSARlab user has access to an Amazon AWS EC2 instance. Individual instances are shared among groups of 1 - 3 users, depending on demand. Operating System Ubuntu 20.04.3 LTS Volume (storage) OpenSARlab uses Amazon AWS EBS volumes , which are mounted on user servers' home directories for storage. A snapshot of each volume is taken everyday at 10:00 UTC. Only the most recent snapshot is retained. Any volumes unused for two days are destroyed, with the latest snapshot retained as a backup. If a volume has been destroyed, a new EBS volume is created upon the next login, and it is populated with data from the snapshot. While the volume is created and becomes usable very quickly, populating it with data from the snapshot can take some time (tens of minutes) and users may notice that notebooks load slowly during this period. The important takeaway is that user storage is persistent; you will not lose saved work unless your account is deactivated after 46 days of non-use. It is users' responsibility to manage their storage. Using up all your storage space will prevent you from logging into OpenSARlab. Please contact an OpenSARlab administrator if this occurs. 500GB Amazon AWS EBS volume (volume size subject to change) allocated to each user. Memory (RAM) EC2 instances are shared among users. This happens behind the scenes and is generally not noticeable when using OpenSARlab, with the exception of memory availability. The amount of memory available to each user depends on overall use of an instance, and may vary from 6GB to 16GB. RAM allocated per user: 6GB - 16GB Privileges Users do not have root privileges ( i.e. no sudo ).","title":"OpenSARlab Account Details"},{"location":"user-guides/OpenSARlab_environment/#the-opensarlab-environment-and-account-lifecycle","text":"","title":"The OpenSARlab Environment and Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#account-lifecycle","text":"Accounts will be deactivated on the 46th day of inactivity. Warning emails are sent to inactive users after 30 , 37 , 41 , 43 , and 45 days. The user volume and snapshot are permanently destroyed upon account deactivation.","title":"Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#opensarlab-environment","text":"Every OpenSARlab user has access to an Amazon AWS EC2 instance. Individual instances are shared among groups of 1 - 3 users, depending on demand.","title":"OpenSARlab Environment"},{"location":"user-guides/OpenSARlab_environment/#operating-system","text":"Ubuntu 20.04.3 LTS","title":"Operating System"},{"location":"user-guides/OpenSARlab_environment/#volume-storage","text":"OpenSARlab uses Amazon AWS EBS volumes , which are mounted on user servers' home directories for storage. A snapshot of each volume is taken everyday at 10:00 UTC. Only the most recent snapshot is retained. Any volumes unused for two days are destroyed, with the latest snapshot retained as a backup. If a volume has been destroyed, a new EBS volume is created upon the next login, and it is populated with data from the snapshot. While the volume is created and becomes usable very quickly, populating it with data from the snapshot can take some time (tens of minutes) and users may notice that notebooks load slowly during this period. The important takeaway is that user storage is persistent; you will not lose saved work unless your account is deactivated after 46 days of non-use. It is users' responsibility to manage their storage. Using up all your storage space will prevent you from logging into OpenSARlab. Please contact an OpenSARlab administrator if this occurs. 500GB Amazon AWS EBS volume (volume size subject to change) allocated to each user.","title":"Volume (storage)"},{"location":"user-guides/OpenSARlab_environment/#memory-ram","text":"EC2 instances are shared among users. This happens behind the scenes and is generally not noticeable when using OpenSARlab, with the exception of memory availability. The amount of memory available to each user depends on overall use of an instance, and may vary from 6GB to 16GB. RAM allocated per user: 6GB - 16GB","title":"Memory (RAM)"},{"location":"user-guides/OpenSARlab_environment/#privileges","text":"Users do not have root privileges ( i.e. no sudo ).","title":"Privileges"},{"location":"user-guides/OpenSARlab_terminal/","text":"Return to Table of Contents Using the Terminal in OpenSARlab Open a Terminal In JupyterLab If there is no Launcher tab in you workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in a Launcher tab. In Jupyter Notebook Select Terminal from the New menu in the JupyterHub file manager Use the Terminal Use the command line as you would in any other Linux terminal No Root Privileges OpenSARlab users do not have sudo privileges ( jovyan does not have a password).","title":"OpenSARlab Terminal"},{"location":"user-guides/OpenSARlab_terminal/#using-the-terminal-in-opensarlab","text":"","title":"Using the Terminal in OpenSARlab"},{"location":"user-guides/OpenSARlab_terminal/#open-a-terminal","text":"","title":"Open a Terminal"},{"location":"user-guides/OpenSARlab_terminal/#in-jupyterlab","text":"If there is no Launcher tab in you workspace, open one by clicking the blue + button at the upper left of the screen. Click the Terminal button in a Launcher tab.","title":"In JupyterLab"},{"location":"user-guides/OpenSARlab_terminal/#in-jupyter-notebook","text":"Select Terminal from the New menu in the JupyterHub file manager","title":"In Jupyter Notebook"},{"location":"user-guides/OpenSARlab_terminal/#use-the-terminal","text":"Use the command line as you would in any other Linux terminal","title":"Use the Terminal"},{"location":"user-guides/OpenSARlab_terminal/#no-root-privileges","text":"OpenSARlab users do not have sudo privileges ( jovyan does not have a password).","title":"No Root Privileges"},{"location":"user-guides/class_notebooks_best_practices/","text":"Return to Table of Contents Developing Notebooks for Classes or Trainings: Best Practices Provide a Conda Environment Capable of Running the Notebooks Provide students with a conda environment that has everything they need. Conda Environments in OpenSARlab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks. Set the Notebook Metadata to Use the Correct Environment Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created). Clear Your Notebook Output Before Saving it Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo. Keep your Conda Environment Up to Date Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARlab to create them, which is located in the /home/jovyan/conda_environments/ directory. Test Notebooks Ahead of Time. If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment. Plan for Students with Poor Internet Access Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner. To avoid issues related to poor internet access, consider following options: Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle . Avoid Changing Directories in Your Code Why? Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students. How? If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"Best Practices for Writing Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#developing-notebooks-for-classes-or-trainings-best-practices","text":"","title":"Developing Notebooks for Classes or Trainings: Best Practices"},{"location":"user-guides/class_notebooks_best_practices/#provide-a-conda-environment-capable-of-running-the-notebooks","text":"Provide students with a conda environment that has everything they need. Conda Environments in OpenSARlab. Students can mimic instructor's environment by: Distributing environment.yml file Upload environment.yml into following directory: /home/jovyan/conda_environments/Environment_Configs/ Create using Create_OSL_Conda_Environments.ipynb notebook located in /home/jovyan/conda_environments You may encounter dependency conflicts that can prevent you from installing essential software to run all of your notebooks in a single environment. In such cases, create multiple conda environments to run different notebooks.","title":"Provide a Conda Environment Capable of Running the Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#set-the-notebook-metadata-to-use-the-correct-environment","text":"Open your notebook, change into your conda environment's kernel, and save the notebook. Push the update to your notebook repo. When students pull in your notebook repo, the notebooks will automatically run the correct kernel with no intervention (as long as the required environment has been created).","title":"Set the Notebook Metadata to Use the Correct Environment"},{"location":"user-guides/class_notebooks_best_practices/#clear-your-notebook-output-before-saving-it","text":"Saving a notebook with previous output(s) increases its file size and slows down the time it takes to load. Restart your kernel and clear the notebook output before saving and pushing notebooks to your repo.","title":"Clear Your Notebook Output Before Saving it"},{"location":"user-guides/class_notebooks_best_practices/#keep-your-conda-environment-up-to-date","text":"Libraries and packages installed in your conda environment will be updated over time. If you are using a conda environment that was used in a previous class or training, try re-creating it first to confirm that it still builds without any conflicts. You can use the Create_OSL_Conda_Environments notebook in OpenSARlab to create them, which is located in the /home/jovyan/conda_environments/ directory.","title":"Keep your Conda Environment Up to Date"},{"location":"user-guides/class_notebooks_best_practices/#test-notebooks-ahead-of-time","text":"If there are assignment sections requiring students to write or refactor code, test the notebook with the correct solutions first. This will alert you to potential issues that you may miss otherwise. Example: The notebook successfuly runs the instructor provided code, but crashes the kernel due to insufficient memory when students add new code to complete the assignment.","title":"Test Notebooks Ahead of Time."},{"location":"user-guides/class_notebooks_best_practices/#plan-for-students-with-poor-internet-access","text":"Saving a notebook without clearing its output first will increase the file size substantially. If notebooks are too big and students have poor internet connections, the notebook autosave functionality may fail. Due to the above reasons, it is risky for students to submit their assignments by running notebooks, saving their results, and submitting them afterwards. Students without a strong internet connection may not be able to save and turn in their work in this manner.","title":"Plan for Students with Poor Internet Access"},{"location":"user-guides/class_notebooks_best_practices/#to-avoid-issues-related-to-poor-internet-access-consider-following-options","text":"Allow assignments to be turned in as screenshots pasted into a word processor and converted into pdf. Split assignments into 2 notebooks: one for content/examples and another one for assignment. Pass required data structures from the content notebook to the assignment notebook using a Python pickle .","title":"To avoid issues related to poor internet access, consider following options:"},{"location":"user-guides/class_notebooks_best_practices/#avoid-changing-directories-in-your-code","text":"","title":"Avoid Changing Directories in Your Code"},{"location":"user-guides/class_notebooks_best_practices/#why","text":"Users can run Jupyter Notebook code cells in any order. Users can skip over cells and/or re-run previous cells, which can cause unexpected problems. Example: Consider a Python list that contain a data specific to each day (e.g. temperature, stock price, etc.). To store today's data, you can use lst.append(todays_data) . However, you may end with duplicate data in your list if you run this code multiple times since previous output is preserved and thus you are appending todays_data multiple times. This may result in breaking code and/or a confusion for students.","title":"Why?"},{"location":"user-guides/class_notebooks_best_practices/#how","text":"If possible, don't change directories. Instead, provide absolute paths to functions that need them. If you are running a script that requires you to be in a particular working directory, use a context manager to handle directory changes. This will allow you to change to the correct working directory, call the script, and then change back to the original directory. For context manager, write a following function first: import contextlib from pathlib import Path @contextlib.contextmanager def work_dir(work_pth): cwd = Path.cwd() os.chdir(work_pth) try: yield finally: os.chdir(cwd) Then, call it using with keyword: with work_dir(work_pth): !python my_script.py","title":"How?"},{"location":"user-guides/conda_environments/","text":"Return to Table of Contents Creating and Using Conda Environments in OpenSARlab OpenSARlab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following 5 conda environments are provided by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning However, these environments are not pre-built and must be created by each user. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Users can also create their own custom conda environments in OpenSARlab and use them to run Jupyter Notebooks. Use the notebook located here to complete the process: /home/jovyan/opensarlab_docs/OpenSARlab_supplements/Jupyter_Conda_Environments.ipynb","title":"Conda Environments"},{"location":"user-guides/conda_environments/#creating-and-using-conda-environments-in-opensarlab","text":"OpenSARlab comes with a default base conda environment with a minimum amount of software installed. Users must create their own conda environments to run Jupyter Notebooks or Python scripts. The following 5 conda environments are provided by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning However, these environments are not pre-built and must be created by each user. We have provided a notebook to help install conda environments located at the following path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Users can also create their own custom conda environments in OpenSARlab and use them to run Jupyter Notebooks. Use the notebook located here to complete the process: /home/jovyan/opensarlab_docs/OpenSARlab_supplements/Jupyter_Conda_Environments.ipynb","title":"Creating and Using Conda Environments in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/","text":"Return to Table of Contents Git in OpenSARlab ASF's Jupyter Notebook Library ASF's OpenSARlab Jupyter Notebook library lives in the asf-jupyter-notebooks GitHub repository . Gitpuller A nbgitpuller pulls any changes to the notebook repo each time a user's OpenSARlab server starts up. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: User will end up with two copies of that notebook. User's version of the notebook will have a timestamp appended to its name. The notebook with the original, unaltered name will contain the new changes made by ASF. Example: Original version: sample_notebook.ipynb Updated by user: sample_notebook__20210616165846.ipynb Updated by ASF: sample_notebook.ipynb Please note that the nbgitpuller will not run if you are not in the main branch of the asf-jupyter-notebook repo. What do I do if my git state is broken beyond my ability to repair it? Download or move any files you wish to preserve out of the /home/jovyan/notebooks directory (or its subdirectories). Delete the entire /home/jovyan/notebooks directory by: Opening a terminal. Use rm -rf /home/jovyan/notebooks to remove everything. Restart your server. A fresh copy of the repo will be cloned into your account. Using Other Git Repositories in OpenSARlab Users can use any repo they wish in OpenSARlab. It is best to clone any additional repos alongside or above the notebooks directory, where the asf-jupyter-notebooks repo is stored. This prevents issues that may arise from nesting repositories inside each other. Clone your repos to /home/jovyan , which can be done in the terminal .","title":"Git in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/#git-in-opensarlab","text":"","title":"Git in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/#asfs-jupyter-notebook-library","text":"ASF's OpenSARlab Jupyter Notebook library lives in the asf-jupyter-notebooks GitHub repository .","title":"ASF's Jupyter Notebook Library"},{"location":"user-guides/git_in_OpenSARlab/#gitpuller","text":"A nbgitpuller pulls any changes to the notebook repo each time a user's OpenSARlab server starts up. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the following will occur: User will end up with two copies of that notebook. User's version of the notebook will have a timestamp appended to its name. The notebook with the original, unaltered name will contain the new changes made by ASF. Example: Original version: sample_notebook.ipynb Updated by user: sample_notebook__20210616165846.ipynb Updated by ASF: sample_notebook.ipynb Please note that the nbgitpuller will not run if you are not in the main branch of the asf-jupyter-notebook repo.","title":"Gitpuller"},{"location":"user-guides/git_in_OpenSARlab/#what-do-i-do-if-my-git-state-is-broken-beyond-my-ability-to-repair-it","text":"Download or move any files you wish to preserve out of the /home/jovyan/notebooks directory (or its subdirectories). Delete the entire /home/jovyan/notebooks directory by: Opening a terminal. Use rm -rf /home/jovyan/notebooks to remove everything. Restart your server. A fresh copy of the repo will be cloned into your account.","title":"What do I do if my git state is broken beyond my ability to repair it?"},{"location":"user-guides/git_in_OpenSARlab/#using-other-git-repositories-in-opensarlab","text":"Users can use any repo they wish in OpenSARlab. It is best to clone any additional repos alongside or above the notebooks directory, where the asf-jupyter-notebooks repo is stored. This prevents issues that may arise from nesting repositories inside each other. Clone your repos to /home/jovyan , which can be done in the terminal .","title":"Using Other Git Repositories in OpenSARlab"},{"location":"user-guides/how_to_run_a_notebook/","text":"Return to Table of Contents How to Run a Jupyter Notebook A Light Introduction to Jupyter Notebook Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, which is typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has 4 cell types, we use the following two for the OpenSARlab: Markdown cells Code cells Markdown Cells Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double click it. A markdown cell in edit mode If you: Want to proceed through the notebook past the markdown cell. Run a markdown cell's code to display its formatted contents. Click the play button at the top of the notebook or hit shift + enter . A run markdown cell Code Cells Code cells contain editable and runnable Python code. You can run them in any order for any number of times. The ability to run/rerun code cells in arbitrary order can be helpful, but it can also cause problems. e.g. Recycled variables may end up with unexpected values if cells are run in non-sequential order. A code cell Selecting Cells Select a Single Cell in Non-Edit Mode Click to the left of a cell A selected cell in non-edit mode is surrounded by a box with a blue left edge Select Multiple Cells in Non-Edit Mode Select a cell in non-edit mode Select additional cells with: Shift + J or Shift + Down-Arrow to select additional cells below Shift + K or Shift + Up-Arrow to select additional cells above Perform batch operations on selected cells Multiple selected cells are surrounded by a blue highlighted box Select a Code Cell in Edit Mode Click inside a cell A selected cell in non-edit mode is surrounded by a box with a green left edge Select a Markdown Cell in Edit Mode Double click inside a cell A markdown cell in edit mode is surrounded by a box with a green left edge Running Cells Since code cells may be run in any order, they are numbered in the order they ran. Run a Single Code or Markdown Cell With the Run Button Select a cell in edit or non-edit mode Click the Run button Click the Run button to run a selected cell With Hotkeys Select a cell in edit or non-edit mode Ctrl + Enter runs a cell Shift + Enter runs a cell and selects the cell below Alt + Enter runs a cell and inserts an empty cell below Run a Cell and every Cell Above or Below It Select a cell in edit or non-edit mode Select Run All Above from the \"Cell\" menu Select Run All Below from the \"Cell\" menu Run a batch of selected cells Select a group of cells Click the Run Button Select Run Cells from the \"Cell\" menu Run an Entire Notebook Select Run All from the \"Cell\" menu (does not restart the kernel) Select Restart & Run All from the \"Kernel\" Menu (restarts kernel) Rerunning a Notebook It is a recommended to restart the notebook kernel before rerunning a notebook. This is because any initialized variables and data structures from a previous run will be stored in memory along with their values, which can lead to unintended results. Example of unintended results: Consider a case where you have a Python list with date specific data, such as weather, stock prices, etc. If you were to append specific day's data, you would do something like lst.append(today) . However, running this cell more than once will yield unpredictable result due to duplication of today's data. e.g. Running this cell n times will result in [day_before, yesterday, today, today, today, today, ...] . To restart notebook, do one of the following: Select Restart from the Kernel Menu Select Restart & Clear Output from the Kernel Menu Select Restart & Run All from the Kernel Menu Clearing Cell Output Before Closing It is recommended to clear every output from each code cells prior to closing or saving a notebook. Leaving the output in place can increase file size of notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#how-to-run-a-jupyter-notebook","text":"","title":"How to Run a Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#a-light-introduction-to-jupyter-notebook","text":"Jupyter Notebook is a web application that allows users to display: Interactive and runnable code cells, which is typically written in Python Markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, images, etc. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. While Jupyter Notebook has 4 cell types, we use the following two for the OpenSARlab: Markdown cells Code cells","title":"A Light Introduction to Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#markdown-cells","text":"Markdown cells contain documentation in Markdown, HTML, and/or LaTeX. They are often used to display text, images, hyperlinks, formulas, tables, pseudocode, plots, figures, etc. To enter edit mode in a markdown cell, double click it. A markdown cell in edit mode If you: Want to proceed through the notebook past the markdown cell. Run a markdown cell's code to display its formatted contents. Click the play button at the top of the notebook or hit shift + enter . A run markdown cell","title":"Markdown Cells"},{"location":"user-guides/how_to_run_a_notebook/#code-cells","text":"Code cells contain editable and runnable Python code. You can run them in any order for any number of times. The ability to run/rerun code cells in arbitrary order can be helpful, but it can also cause problems. e.g. Recycled variables may end up with unexpected values if cells are run in non-sequential order. A code cell","title":"Code Cells"},{"location":"user-guides/how_to_run_a_notebook/#selecting-cells","text":"","title":"Selecting Cells"},{"location":"user-guides/how_to_run_a_notebook/#select-a-single-cell-in-non-edit-mode","text":"Click to the left of a cell A selected cell in non-edit mode is surrounded by a box with a blue left edge","title":"Select a Single Cell in Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-multiple-cells-in-non-edit-mode","text":"Select a cell in non-edit mode Select additional cells with: Shift + J or Shift + Down-Arrow to select additional cells below Shift + K or Shift + Up-Arrow to select additional cells above Perform batch operations on selected cells Multiple selected cells are surrounded by a blue highlighted box","title":"Select Multiple Cells in Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-a-code-cell-in-edit-mode","text":"Click inside a cell A selected cell in non-edit mode is surrounded by a box with a green left edge","title":"Select a Code Cell in Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-a-markdown-cell-in-edit-mode","text":"Double click inside a cell A markdown cell in edit mode is surrounded by a box with a green left edge","title":"Select a Markdown Cell in Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#running-cells","text":"Since code cells may be run in any order, they are numbered in the order they ran.","title":"Running Cells"},{"location":"user-guides/how_to_run_a_notebook/#run-a-single-code-or-markdown-cell","text":"","title":"Run a Single Code or Markdown Cell"},{"location":"user-guides/how_to_run_a_notebook/#with-the-run-button","text":"Select a cell in edit or non-edit mode Click the Run button Click the Run button to run a selected cell","title":"With the Run Button"},{"location":"user-guides/how_to_run_a_notebook/#with-hotkeys","text":"Select a cell in edit or non-edit mode Ctrl + Enter runs a cell Shift + Enter runs a cell and selects the cell below Alt + Enter runs a cell and inserts an empty cell below","title":"With Hotkeys"},{"location":"user-guides/how_to_run_a_notebook/#run-a-cell-and-every-cell-above-or-below-it","text":"Select a cell in edit or non-edit mode Select Run All Above from the \"Cell\" menu Select Run All Below from the \"Cell\" menu","title":"Run a Cell and every Cell Above or Below It"},{"location":"user-guides/how_to_run_a_notebook/#run-a-batch-of-selected-cells","text":"Select a group of cells Click the Run Button Select Run Cells from the \"Cell\" menu","title":"Run a batch of selected cells"},{"location":"user-guides/how_to_run_a_notebook/#run-an-entire-notebook","text":"Select Run All from the \"Cell\" menu (does not restart the kernel) Select Restart & Run All from the \"Kernel\" Menu (restarts kernel)","title":"Run an Entire Notebook"},{"location":"user-guides/how_to_run_a_notebook/#rerunning-a-notebook","text":"It is a recommended to restart the notebook kernel before rerunning a notebook. This is because any initialized variables and data structures from a previous run will be stored in memory along with their values, which can lead to unintended results. Example of unintended results: Consider a case where you have a Python list with date specific data, such as weather, stock prices, etc. If you were to append specific day's data, you would do something like lst.append(today) . However, running this cell more than once will yield unpredictable result due to duplication of today's data. e.g. Running this cell n times will result in [day_before, yesterday, today, today, today, today, ...] . To restart notebook, do one of the following: Select Restart from the Kernel Menu Select Restart & Clear Output from the Kernel Menu Select Restart & Run All from the Kernel Menu","title":"Rerunning a Notebook"},{"location":"user-guides/how_to_run_a_notebook/#clearing-cell-output-before-closing","text":"It is recommended to clear every output from each code cells prior to closing or saving a notebook. Leaving the output in place can increase file size of notebook, which will use up more of your volume and cause slower notebook loading times (especially if you have a slow internet connection).","title":"Clearing Cell Output Before Closing"},{"location":"user-guides/installing_software_in_OpenSARlab/","text":"Return to Table of Contents Installing Software in OpenSARlab pip You can install pip packages to your /home/jovyan/.local/lib/python3.7/site-packages directory Open a terminal and use the following command: python -m pip install --user <package_name> To install a pip package inside of a conda environment Open a terminal and use the following command: conda activate <environment_name> python -m pip install --user <package_name> apt and apt-get At this time, users cannot install software in OpenSARlab using apt or apt-get . conda Users can install additional software with conda in OpenSARlab. Packages installed in the base conda environment will not persist after the server shuts down. They will need to be reinstalled during subsequent OpenSARlab sessions. However, changes to non-base environments will persist. Therefore, it is recommended to install new packages in your non-base environments instead of in base. Install conda packages from within a notebook running in an environment Edit a notebook code cell Then use the following command: %conda install <package_name> Run the code cell Install conda packages from the terminal Open a terminal and use following command: conda activate <environment_name> conda install <package_name>","title":"Installing Software in OpenSARlab"},{"location":"user-guides/installing_software_in_OpenSARlab/#installing-software-in-opensarlab","text":"","title":"Installing Software in OpenSARlab"},{"location":"user-guides/installing_software_in_OpenSARlab/#pip","text":"","title":"pip"},{"location":"user-guides/installing_software_in_OpenSARlab/#you-can-install-pip-packages-to-your-homejovyanlocallibpython37site-packages-directory","text":"Open a terminal and use the following command: python -m pip install --user <package_name>","title":"You can install pip packages to your /home/jovyan/.local/lib/python3.7/site-packages directory"},{"location":"user-guides/installing_software_in_OpenSARlab/#to-install-a-pip-package-inside-of-a-conda-environment","text":"Open a terminal and use the following command: conda activate <environment_name> python -m pip install --user <package_name>","title":"To install a pip package inside of a conda environment"},{"location":"user-guides/installing_software_in_OpenSARlab/#apt-and-apt-get","text":"At this time, users cannot install software in OpenSARlab using apt or apt-get .","title":"apt and apt-get"},{"location":"user-guides/installing_software_in_OpenSARlab/#conda","text":"Users can install additional software with conda in OpenSARlab. Packages installed in the base conda environment will not persist after the server shuts down. They will need to be reinstalled during subsequent OpenSARlab sessions. However, changes to non-base environments will persist. Therefore, it is recommended to install new packages in your non-base environments instead of in base.","title":"conda"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-from-within-a-notebook-running-in-an-environment","text":"Edit a notebook code cell Then use the following command: %conda install <package_name> Run the code cell","title":"Install conda packages from within a notebook running in an environment"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-from-the-terminal","text":"Open a terminal and use following command: conda activate <environment_name> conda install <package_name>","title":"Install conda packages from the terminal"},{"location":"user-guides/jupyter_magic/","text":"Return to Table of Contents Jupyter Line and Cell Magics, and IPython Syntax Jupyter Notebook magic commands provide shortcuts and extra functionality to the notebooks in addition to what can be done with pure Python code. An exhaustive list of magic commands can be found in the IPython docs . While all magic commands are available to users, we tend to use a relatively small selection of them in OpenSARlab. Magic commands that are frequently used are following: IPython's Shell Assignment Syntax Line Magics Cell Magics IPython Shell Assignment with ! In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example: !pwd will print the current working directory. Line Magics Line magics start with a single % and effect only the line on which they are used. Following line magics are commonly used: %matplotlib inline Allows non-interactive matplotlib plots to be displayed in a notebook. %matplotlib notebook Allows interactive matplotlib plots to be displayed and interacted with inside of a Jupyter Notebook. %df This is a custom magic written specifically for OpenSARlab. It uses the python function shutil.disk_usage() to check the state of storage on user's volumes. %df returns a human readable string in GB. %df --raw returns a raw data object %df --on returns a string in GB after every subsequent code cell is run %df --off turns %df --on back off %df -v prints additional debugging text Cell Magics Cell magics start with %% and effect the contents of an entire cell. %%javascript or %%js Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell. %%capture Runs the cell but captures all output. We typically use this to suppress output of a matplotlib plot that the user does not wish to see.","title":"Jupyter Magic Commands"},{"location":"user-guides/jupyter_magic/#jupyter-line-and-cell-magics-and-ipython-syntax","text":"Jupyter Notebook magic commands provide shortcuts and extra functionality to the notebooks in addition to what can be done with pure Python code. An exhaustive list of magic commands can be found in the IPython docs . While all magic commands are available to users, we tend to use a relatively small selection of them in OpenSARlab. Magic commands that are frequently used are following: IPython's Shell Assignment Syntax Line Magics Cell Magics","title":"Jupyter Line and Cell Magics, and IPython Syntax"},{"location":"user-guides/jupyter_magic/#ipython-shell-assignment-with","text":"In IPython syntax, the exclamation mark (!) allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with ! and it will run the command in the shell. Example: !pwd will print the current working directory.","title":"IPython Shell Assignment with !"},{"location":"user-guides/jupyter_magic/#line-magics","text":"Line magics start with a single % and effect only the line on which they are used. Following line magics are commonly used:","title":"Line Magics"},{"location":"user-guides/jupyter_magic/#matplotlib-inline","text":"Allows non-interactive matplotlib plots to be displayed in a notebook.","title":"%matplotlib inline"},{"location":"user-guides/jupyter_magic/#matplotlib-notebook","text":"Allows interactive matplotlib plots to be displayed and interacted with inside of a Jupyter Notebook.","title":"%matplotlib notebook"},{"location":"user-guides/jupyter_magic/#df","text":"This is a custom magic written specifically for OpenSARlab. It uses the python function shutil.disk_usage() to check the state of storage on user's volumes. %df returns a human readable string in GB. %df --raw returns a raw data object %df --on returns a string in GB after every subsequent code cell is run %df --off turns %df --on back off %df -v prints additional debugging text","title":"%df"},{"location":"user-guides/jupyter_magic/#cell-magics","text":"Cell magics start with %% and effect the contents of an entire cell.","title":"Cell Magics"},{"location":"user-guides/jupyter_magic/#javascript-or-js","text":"Runs a JavaScript code cell. Note: leave a blank line above the magic command in the beginning of the code cell.","title":"%%javascript or %%js"},{"location":"user-guides/jupyter_magic/#capture","text":"Runs the cell but captures all output. We typically use this to suppress output of a matplotlib plot that the user does not wish to see.","title":"%%capture"},{"location":"user-guides/jupyter_notebook_extensions/","text":"Return to Table of Contents Managing Jupyter Lab Extensions As an OpenSARlab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARlab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARlab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu Managing Jupyter Notebook Extensions As an OpenSARlab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here . Enabling and Disabling Extensions The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-lab-extensions","text":"As an OpenSARlab Jupyter Lab user, you have limited access to third-party extensions. Server extensions must be installed in the OpenSARlab Docker container and cannot be installed by users. Lab extensions can be installed, enabled, and disabled from the terminal but they will not persist across server restarts and will need to be reinstalled. If you feel that OpenSARlab is lacking an important Jupyter Lab extension, please contact us to request it at uaf-jupyterhub-asf@alaska.edu","title":"Managing Jupyter Lab Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-notebook-extensions","text":"As an OpenSARlab Jupyter Notebook user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of extensions is available here .","title":"Managing Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#enabling-and-disabling-extensions","text":"The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function. You can also choose to enable or disable each extension. Select an extension to learn more about them and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Enabling and Disabling Extensions"},{"location":"user-guides/logging_out_and_server_shutdown/","text":"Return to Table of Contents Logging Out of OpenSARlab and Shutting Down the Server When you are ready to stop working in OpenSARlab, it is important to shut down your server and also logout. Why Shut Down the Server? Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs. Example: Consider a case where you ran some process, logged out, and decided to log back in for the first time in a few days. Upon logging in, you noticed that the Jupyter Notebook from the previous session is still running. This implies that your server was running for the past few days, which used up unnecessary resources. Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before your leave. How to Shut Down The Server and Logout in Jupyter Lab Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button. How to Shut Down The Server and Logout in Jupyter Notebook Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"Logging Out and Server Shutdown"},{"location":"user-guides/logging_out_and_server_shutdown/#logging-out-of-opensarlab-and-shutting-down-the-server","text":"When you are ready to stop working in OpenSARlab, it is important to shut down your server and also logout.","title":"Logging Out of OpenSARlab and Shutting Down the Server"},{"location":"user-guides/logging_out_and_server_shutdown/#why-shut-down-the-server","text":"Logging out will not shut down the server on its own. While the server may shut down automatically after an hour of inactivity, users should not rely on this feature. The server will stay alive while there are any notebooks open in active browser tabs. Example: Consider a case where you ran some process, logged out, and decided to log back in for the first time in a few days. Upon logging in, you noticed that the Jupyter Notebook from the previous session is still running. This implies that your server was running for the past few days, which used up unnecessary resources. Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. In some instances, you may need to leave your server running. For example, you have a notebook performing a very time intensive analysis and wish to let it run overnight. It is acceptable for you to keep your server running in cases like this. Summary: Unless you intend to run your server for a long period of time, make sure to shut it down before your leave.","title":"Why Shut Down the Server?"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-lab","text":"Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Lab"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout-in-jupyter-notebook","text":"Click The Control Panel Button Click the Control Panel button at the top right corner of the file manager or in an open notebook. Click The Stop My Server Button Click the Stop My Server button that appears. Click The Logout Button Click the Logout button.","title":"How to Shut Down The Server and Logout in Jupyter Notebook"},{"location":"user-guides/restarting_server_and_kernel/","text":"Return to Table of Contents Restarting the OpenSARlab Server and Notebook Kernel Restarting the OpenSARlab Server Why? Restarting the server triggers the nbgitpuller to run. Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. You know that a notebook update was just made and you would like to immediately pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . Note: If you are comfortable with git, you could do a git pull from the terminal or in a notebook instead. How? In JupyterLab , Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen or In Jupyter Notebook , Click The Control Panel Button Click the Control Panel button located at the top right corner of the file manager or in an open notebook Click The Stop My Server Button Click the Stop My Server button that appears Click The Start My Server Button Click the Start My Server button, which may take a few seconds to appear Click The Launch Server Button Click the Launch Server button that appears Select a Server Option and Click Start Select a server option and click the Start button Wait For the Server To Start Wait for the server to start Optional: Click The Event Log Arrow For Detailed Startup Status Information Click the Event log arrow to view logs documenting the status of the startup process Changing a Notebook Kernel Why? Notebooks in OpenSARlab run in a variety of conda environments. If a notebook is set to the kernel of the wrong environment, it may not have all the software packages it requires. How? From the Kernel menu, click Change kernel and select the desired kernel. Restarting a Jupyter Notebook Kernel Why? As you run code cells in a notebook, initialized variables and their assigned values are stored in memory. If you decide to start over and re-run a previously ran notebook without restarting the kernel, you may encounter some issues. The issues are caused by previously defined variables that persist in your memory. This is problematic for various reasons, such as: They use up instance's limited memory resources. Increase file size of notebook. Previously defined values may cause unintended results when re-runing the code. Example of unintended results: Consider a notebook that builds a string, which starts empty and appends substrings systematically. When re-running this notebook, the string will no longer start empty and the resultant string will contain an unintended substring at its start. The solution for this is to restart the kernel to clear notebook data that are stored in memory. How? Select Restart , Restart & Clear Output , or Restart & Run All From The Kernel Menu Restart will restart the kernel but leave old code cell output in place. Restart & Clear Output restarts the kernel and removes old code cell output. This is generally the preferred option. Restart & Run All restarts the kernel and runs all the code cells. This only works if the notebook does not require input from user. For most use cases, select Restart & Clear Output","title":"OpenSARlab Servers and Kernels"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server-and-notebook-kernel","text":"","title":"Restarting the OpenSARlab Server and Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server","text":"","title":"Restarting the OpenSARlab Server"},{"location":"user-guides/restarting_server_and_kernel/#why","text":"Restarting the server triggers the nbgitpuller to run. Consider a case where: You have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. You know that a notebook update was just made and you would like to immediately pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server to run the nbgitpuller . Note: If you are comfortable with git, you could do a git pull from the terminal or in a notebook instead.","title":"Why?"},{"location":"user-guides/restarting_server_and_kernel/#how","text":"In JupyterLab , Select Hub Control Panel from the File menu or Click the Shutdown and Logout Page button in the upper right corner of the screen or In Jupyter Notebook , Click The Control Panel Button Click the Control Panel button located at the top right corner of the file manager or in an open notebook Click The Stop My Server Button Click the Stop My Server button that appears Click The Start My Server Button Click the Start My Server button, which may take a few seconds to appear Click The Launch Server Button Click the Launch Server button that appears Select a Server Option and Click Start Select a server option and click the Start button Wait For the Server To Start Wait for the server to start Optional: Click The Event Log Arrow For Detailed Startup Status Information Click the Event log arrow to view logs documenting the status of the startup process","title":"How?"},{"location":"user-guides/restarting_server_and_kernel/#changing-a-notebook-kernel","text":"","title":"Changing a Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#why_1","text":"Notebooks in OpenSARlab run in a variety of conda environments. If a notebook is set to the kernel of the wrong environment, it may not have all the software packages it requires.","title":"Why?"},{"location":"user-guides/restarting_server_and_kernel/#how_1","text":"From the Kernel menu, click Change kernel and select the desired kernel.","title":"How?"},{"location":"user-guides/restarting_server_and_kernel/#restarting-a-jupyter-notebook-kernel","text":"","title":"Restarting a Jupyter Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#why_2","text":"As you run code cells in a notebook, initialized variables and their assigned values are stored in memory. If you decide to start over and re-run a previously ran notebook without restarting the kernel, you may encounter some issues. The issues are caused by previously defined variables that persist in your memory. This is problematic for various reasons, such as: They use up instance's limited memory resources. Increase file size of notebook. Previously defined values may cause unintended results when re-runing the code. Example of unintended results: Consider a notebook that builds a string, which starts empty and appends substrings systematically. When re-running this notebook, the string will no longer start empty and the resultant string will contain an unintended substring at its start. The solution for this is to restart the kernel to clear notebook data that are stored in memory.","title":"Why?"},{"location":"user-guides/restarting_server_and_kernel/#how_2","text":"Select Restart , Restart & Clear Output , or Restart & Run All From The Kernel Menu Restart will restart the kernel but leave old code cell output in place. Restart & Clear Output restarts the kernel and removes old code cell output. This is generally the preferred option. Restart & Run All restarts the kernel and runs all the code cells. This only works if the notebook does not require input from user. For most use cases, select Restart & Clear Output","title":"How?"},{"location":"user-guides/s3_buckets/","text":"Return to Table of Contents Accessing Public and Private S3 Buckets from OpenSARlab The commands below can be run in a terminal or from a notebook code cell by prepending an ! Accessing Public S3 Buckets When accessing a public bucket from OpenSARlab, be sure to include the --no-sign-request and --region=<bucket's region> flags Note that buckets can have many variations on set permissions. The commands below assume public access to list, read, and write List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user Open a terminal aws configure set default.s3.multipart_threshold 5000MB aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename Accessing Private S3 Buckets Configure the AWS Client in your OpenSARlab account To configure the AWS Client, you will need: An AWS Access Key ID to the account holding the bucket An AWS Secret Access Key to the account holding the bucket An arn to an AWS IAM role with permission to access the bucket Open a terminal aws configure When prompted enter the: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty vim ~/.aws/config vim is a command line text editor Vim Command Cheat Sheet Add the following to the config file [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit vim Access a private S3 bucket List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"S3 Bucket Access in OpenSARlab"},{"location":"user-guides/s3_buckets/#accessing-public-and-private-s3-buckets-from-opensarlab","text":"The commands below can be run in a terminal or from a notebook code cell by prepending an !","title":"Accessing Public and Private S3 Buckets from OpenSARlab"},{"location":"user-guides/s3_buckets/#accessing-public-s3-buckets","text":"When accessing a public bucket from OpenSARlab, be sure to include the --no-sign-request and --region=<bucket's region> flags Note that buckets can have many variations on set permissions. The commands below assume public access to list, read, and write List the contents of a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --no-sign-request --region <bucket's region> s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --no-sign-request --region <bucket's region> s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket Increase your s3.multipart_threshold to allow uploading up to 5000MB as an anonymous user Open a terminal aws configure set default.s3.multipart_threshold 5000MB aws --no-sign-request --region <bucket's region> s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"Accessing Public S3 Buckets"},{"location":"user-guides/s3_buckets/#accessing-private-s3-buckets","text":"","title":"Accessing Private S3 Buckets"},{"location":"user-guides/s3_buckets/#configure-the-aws-client-in-your-opensarlab-account","text":"To configure the AWS Client, you will need: An AWS Access Key ID to the account holding the bucket An AWS Secret Access Key to the account holding the bucket An arn to an AWS IAM role with permission to access the bucket Open a terminal aws configure When prompted enter the: AWS Access Key ID AWS Secret Access Key Default region name Optional: You can enter the region where your bucket is located but this just sets a default and you will be able to enter your bucket's region in a following step Default output format Optional: you can leave this empty vim ~/.aws/config vim is a command line text editor Vim Command Cheat Sheet Add the following to the config file [profile osl] source_profile = default region = <your bucket's region> role_arn = <arn to your iam role> Save and exit vim","title":"Configure the AWS Client in your OpenSARlab account"},{"location":"user-guides/s3_buckets/#access-a-private-s3-bucket","text":"List the contents of a bucket aws --profile osl s3 ls s3://bucket_name/ List the contents of a directory in a bucket aws --profile osl s3 ls s3://bucket_name/directory_name/ Download a file from a bucket aws --profile osl s3 cp s3://bucket_name/directory_name/filename destination/path/filename Upload a file to a bucket aws --profile osl s3 cp source/path/filename s3://bucket_name/destination/path/filename","title":"Access a private S3 bucket"},{"location":"user-guides/troubleshooting_guide/","text":"Return to Table of Contents Troubleshooting Guide Why did the kernel die while running a notebook? The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results. I successfully ran a notebook earlier on the same data but now it is killing the kernel. OpenSARlab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARlab user environment can be found here . When I open a notebook, I receive \"Kernel not found\" message. The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see. The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again. My notebook won't open, opens slowly, or won't save These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARlab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again. I tried to create a new notebook and recieved the error Forbidden This can happen when you have logged out of OpenSARlab and then try to create a new notebook from an OpenSARlab browser tab that was left open. You must log back in before you can create, open, or run a notebook. I am receiving a No space left on device error. OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need. My server won't start and I cannot access OpenSARlab. This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library . The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab. When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name. One of my notebooks looks like it has a mix of code from various versions of the notebook. We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I know there was an update made to an ASF notebook but I still have the old version. We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I am having trouble setting up a web server and developing my web app in OpenSARlab. This cannot be done in OpenSARlab. You will need to do this elsewhere. A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears. This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output. I tried to run a notebook that downloads products from HyP3 and I get an error HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb My issue is not on this list Please contact an OpenSARlab administrator for help.","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#troubleshooting-guide","text":"","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#why-did-the-kernel-die-while-running-a-notebook","text":"The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This occurs frequently when running a time-series or change detection algorithm on data stack that is either too deep or covers too large of an area-of-interest (AOI) for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller AOI. This may take some experimentation because memory is shared among users, i.e. amount of available memory fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. Summary: If you are running a resource hungry program, your kernel might die. Try subsetting your data, processing it in batches, and mosaicing your results.","title":"Why did the kernel die while running a notebook?"},{"location":"user-guides/troubleshooting_guide/#i-successfully-ran-a-notebook-earlier-on-the-same-data-but-now-it-is-killing-the-kernel","text":"OpenSARlab EC2 instances are shared among 1~3 users. The memory available to each user depends on overall activity on the EC2. It is likely that there was enough memory available for your process before, but not enough memory during later attempt(s). More details on the OpenSARlab user environment can be found here .","title":"I successfully ran a notebook earlier on the same data but now it is killing the kernel."},{"location":"user-guides/troubleshooting_guide/#when-i-open-a-notebook-i-receive-kernel-not-found-message","text":"The message that appears when a notebook kernel cannot be found You either have: Not created the required conda environment yet A mix-up between the environment name and prefix If you think you already installed the environment, select it from the pull-down menu that appears and click the Set Kernel button. If you have not created the environment yet, use the following notebook: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb","title":"When I open a notebook, I receive \"Kernel not found\" message."},{"location":"user-guides/troubleshooting_guide/#i-see-one-or-many-python-3-or-python-kernels-instead-of-kernels-named-after-conda-environments-that-i-expect-to-see","text":"The default display names for conda environment kernels are Python 3 or Python (depending on when and how they were created). We use the kernda package to change a kernel's display name to that of the environment name. There is code in the Create_OSL_Conda_Environments.ipynb notebook that does this. If you have created an environment without using the Create_OSL_Conda_Environments.ipynb notebook, run the following command in a terminal to change its display name: mamba run -n <environment name> kernda --display-name <environment name> -o <environment directory>/share/jupyter/kernels/python3/kernel.json Note: it is important to only run the above kernda command once. Running it more than once will create a malformed kernel.json . If you accidentally run it more than once, recreate the environment and try again.","title":"I see one or many Python 3 or Python kernels instead of kernels named after conda environments that I expect to see."},{"location":"user-guides/troubleshooting_guide/#my-notebook-wont-open-opens-slowly-or-wont-save","text":"These are all signs that the notebook contains a lot of output and is too large to easily open or save over the internet in OpenSARlab. If the notebook won't open or opens very slowly, remove its output by running the following command from a terminal: jupyter nbconvert --clear-output --inplace my_notebook.ipynb If the notebook is open and you can't save it, select Restart Kernel and Clear All Outputs from the Kernel menu and try saving it again.","title":"My notebook won't open, opens slowly, or won't save"},{"location":"user-guides/troubleshooting_guide/#i-tried-to-create-a-new-notebook-and-recieved-the-error-forbidden","text":"This can happen when you have logged out of OpenSARlab and then try to create a new notebook from an OpenSARlab browser tab that was left open. You must log back in before you can create, open, or run a notebook.","title":"I tried to create a new notebook and recieved the error Forbidden"},{"location":"user-guides/troubleshooting_guide/#i-am-receiving-a-no-space-left-on-device-error","text":"OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage . If you receive a storage space warning while logged into OpenSARlab, it is highly recommended to free up your space immediately by deleting unnecessary files. If your server shuts down without any available space, it will not have enough space on your volume to restart again and you will be locked out of your account. If you do get locked out from your account, contact an OpenSARlab administrator for help. They will assign enough extra storage to your server so that you can login and delete unnecessary files. If you do not have any files that you can delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request more storage space. Limits will only be increased if there is a demonstrable need.","title":"I am receiving a No space left on device error."},{"location":"user-guides/troubleshooting_guide/#my-server-wont-start-and-i-cannot-access-opensarlab","text":"This issue is typically due to an unexpected behavior of the nbgitpuller . Click the Event log arrow beneath the server startup progress bar to view the details of any nbgitpuller conflicts. Click the Event log arrow beneath the server startup progress bar. If the problem is: Related to nbgitpuller , you will find details regarding to which file(s) are causing the conflict in the event log. In such cases, note the names and locations of the offending file(s) and logout of OpenSARlab. Not related to the nbgitpuller , contact an OpenSARlab Administrator . Click the logout button located on top right corner of the screen After logging out, the startup screen will reload. Select the General SAR processing (without git puller) server option and click the Start button. Select the General SAR processing (without git puller) server option and click the Start button The server should now load and you will have access to your account. Go to where the conflicting file(s) are located. There are three options for dealing with each of the offending file(s): Delete the file(s) if there are no changes from the original ones that you wish to save. Rename the file(s) if there are changes you wish to save. If you wish to try again using nbgitpuller, update the file's timestamp. You can do so by opening the terminal and run touch /your_path_1/.../your_path_n/file_name . Once you are done with one of the above operations, logout of OpenSARlab. Click the logout button located at the top right corner of the screen Log back in and select General SAR processing server option. Select the General SAR processing option and click Start Upon completing above tasks, you should notice that: The nbgitpuller runs successfully. Server starts up properly. You are receiving updates from the ASF notebook library .","title":"My server won't start and I cannot access OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#the-edits-i-made-to-an-asf-notebook-have-disappeared-since-the-last-time-i-used-opensarlab","text":"When your OpenSARlab server starts up, nbgitpuller will run and pull in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and ASF, both changes will be saved. The ASF version will retain its original name while the user's version will have a timestamp appended to its name. Example file format: ASF Edit: sample_notebook.ipynb User Edit: sample_notebook_20210616165846.ipynb If you feel like your notebook is missing, it is likely in its original location with a recent timestamp appended to its name.","title":"The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#one-of-my-notebooks-looks-like-it-has-a-mix-of-code-from-various-versions-of-the-notebook","text":"We have seen this happen occasionally and it is due to a issues with nbgitpuller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"One of my notebooks looks like it has a mix of code from various versions of the notebook."},{"location":"user-guides/troubleshooting_guide/#i-know-there-was-an-update-made-to-an-asf-notebook-but-i-still-have-the-old-version","text":"We have seen this happen occasionally and it is due to nbgitpuller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"I know there was an update made to an ASF notebook but I still have the old version."},{"location":"user-guides/troubleshooting_guide/#i-am-having-trouble-setting-up-a-web-server-and-developing-my-web-app-in-opensarlab","text":"This cannot be done in OpenSARlab. You will need to do this elsewhere.","title":"I am having trouble setting up a web server and developing my web app in OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#a-notebook-wont-load-a-new-browser-tab-opens-and-shows-the-jupyterhub-header-but-no-notebook-appears","text":"This is due to slow loading time caused by a large notebook. If you run a notebook and close it without clearing all output from the code cells, the file size will increase. While the notebook may eventually load, you will need to reload your browser window if it times out. Example: A 40KB notebook can grow to over 60MB if you don't clear its output.","title":"A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears."},{"location":"user-guides/troubleshooting_guide/#i-tried-to-run-a-notebook-that-downloads-products-from-hyp3-and-i-get-an-error","text":"HyP3v1 (HyP3 beta) has been retired and replaced with an updated HyP3 API and SDK . Notebooks using the old version of HyP3 will be removed from the ASF Jupyter Notebook library in GitHub on September 30th 2021. While old notebooks will be removed from GitHub, they will not be deleted from user storage on your OpenSARlab account. If you wish, you may delete them yourself to avoid confusion. Once you have switched to using the new version of HyP3, you should start using HyP3 notebooks that include \"v2\" in their filenames. Example : Stop using Prepare_Data_Stack_HyP3.ipynb and start using Prepare_Data_Stack_HyP3_v2.ipynb","title":"I tried to run a notebook that downloads products from HyP3 and I get an error"},{"location":"user-guides/troubleshooting_guide/#my-issue-is-not-on-this-list","text":"Please contact an OpenSARlab administrator for help.","title":"My issue is not on this list"}]}